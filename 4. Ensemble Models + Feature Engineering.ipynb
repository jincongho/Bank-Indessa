{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"input/train_indessa.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = data.fillna(\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data['last_week_pay'] = data['last_week_pay'].str.extract('(\\d+)', expand=False)\n",
    "data = data.fillna(\"0\")\n",
    "data['last_week_pay'] = data['last_week_pay'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data['term'] = data['term'].str.extract('(\\d+)', expand=False).astype(int)\n",
    "data = data.fillna(\"0\")\n",
    "data['term'] = data['term'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Extract a new feature from term and last week pay\n",
    "\n",
    "data.insert(0, 'payment_completion', (data['last_week_pay']/(data['term']/12*52+1))*100)\n",
    "data['payment_completion'] = data['payment_completion'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Extract a new feature from funded_amnt_inv / loan_amnt\n",
    "\n",
    "data.insert(0, 'funded_ratio', (data['funded_amnt_inv']/data['loan_amnt'])*100)\n",
    "data['funded_ratio'] = data['funded_ratio'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Drop irrelevant features and text features\n",
    "drop_cols = ['member_id', 'batch_enrolled', 'desc', 'title', 'emp_title']\n",
    "data.drop(drop_cols, 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded:  grade\n",
      "Encoded:  sub_grade\n",
      "Encoded:  emp_length\n",
      "Encoded:  home_ownership\n",
      "Encoded:  verification_status\n",
      "Encoded:  pymnt_plan\n",
      "Encoded:  purpose\n",
      "Encoded:  initial_list_status\n",
      "Encoded:  application_type\n",
      "Encoded:  verification_status_joint\n",
      "Encoded:  zip_code\n",
      "Encoded:  addr_state\n"
     ]
    }
   ],
   "source": [
    "# Encode Label for Classifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cat_cols = ['grade', 'sub_grade', 'emp_length', 'home_ownership', 'verification_status', \n",
    "            'pymnt_plan', 'purpose', 'initial_list_status', 'application_type', \n",
    "            'verification_status_joint', 'zip_code', 'addr_state']\n",
    "le = {}\n",
    "\n",
    "for col in cat_cols:\n",
    "    le[col] = LabelEncoder()\n",
    "    data[col] = le[col].fit_transform(data[col])\n",
    "    le[col].classes_ = np.append(le[col].classes_, 'other')\n",
    "    \n",
    "    print('Encoded: ', col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Split Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "split = int(len(data)*0.75)\n",
    "\n",
    "# four most important features, performance score = 0.8685\n",
    "major = ['tot_cur_bal', 'last_week_pay', 'total_rev_hi_lim', 'int_rate']\n",
    "\n",
    "# not so important features, performance score = 0.7634\n",
    "minor = ['tot_cur_bal','zip_code', 'addr_state', 'revol_util', 'revol_bal', 'sub_grade', 'annual_inc', 'total_rec_int']\n",
    "\n",
    "data = data.dropna()\n",
    "X_train = data[data['loan_status'] >= 0].iloc[:split,:-1]\n",
    "Y_train = data[data['loan_status'] >= 0].iloc[:split,-1:]\n",
    "X_test = data[data['loan_status'] >= 0].iloc[split:,:-1]\n",
    "Y_test = data[data['loan_status'] >= 0].iloc[split:,-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Ensembled Model #1: Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/ipykernel/__main__.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100building tree 2 of 100building tree 3 of 100building tree 5 of 100building tree 6 of 100building tree 7 of 100building tree 9 of 100building tree 11 of 100building tree 12 of 100building tree 13 of 100building tree 14 of 100building tree 4 of 100building tree 15 of 100building tree 8 of 100building tree 18 of 100building tree 19 of 100building tree 21 of 100building tree 22 of 100building tree 10 of 100building tree 17 of 100building tree 24 of 100building tree 25 of 100building tree 20 of 100building tree 27 of 100building tree 28 of 100building tree 23 of 100building tree 29 of 100building tree 30 of 100building tree 16 of 100building tree 26 of 100building tree 32 of 100building tree 33 of 100building tree 34 of 100building tree 31 of 100building tree 35 of 100building tree 36 of 100\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  50 out of 100 | elapsed:    6.7s remaining:    6.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n",
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  71 out of 100 | elapsed:    8.5s remaining:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done  92 out of 100 | elapsed:    9.6s remaining:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   10.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=100, n_jobs=-1, oob_score=False,\n",
       "            random_state=None, verbose=5, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, verbose=5, n_jobs=-1)\n",
    "rf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=36)]: Done  50 out of 100 | elapsed:    0.3s remaining:    0.3s\n",
      "[Parallel(n_jobs=36)]: Done  71 out of 100 | elapsed:    0.4s remaining:    0.2s\n",
      "[Parallel(n_jobs=36)]: Done  92 out of 100 | elapsed:    0.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=36)]: Done 100 out of 100 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.85933872748991413"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Ensembled Model #2: KNeighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "knc_train_scores, knc_valid_scores = validation_curve(KNeighborsClassifier(), X_train, Y_train.values.reshape(-1), \n",
    "                                             'n_neighbors', np.arange(5,50,5), n_jobs=-1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "knc_train_scores_mean = np.mean(knc_train_scores, axis=1)\n",
    "knc_train_scores_std = np.std(knc_train_scores, axis=1)\n",
    "knc_valid_scores_mean = np.mean(knc_valid_scores, axis=1)\n",
    "knc_valid_scores_std = np.std(knc_valid_scores, axis=1)\n",
    "knc_param_range = np.arange(5,50,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.title(\"Validation Curve with KNeighbors\")\n",
    "plt.xlabel(\"n_neighbors\")\n",
    "plt.ylabel(\"Score\")\n",
    "lw=2\n",
    "plt.plot(knc_param_range, knc_train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.plot(knc_param_range, knc_valid_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/ipykernel/__main__.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=-1, n_neighbors=20, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knc = KNeighborsClassifier(n_neighbors=20, n_jobs=-1)\n",
    "knc.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80946156099979716"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knc.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Ensembled Model #3: SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/utils/validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 413570.88, NNZs: 41, Bias: 1881.693677, T: 399321, Avg. loss: 19927667685.546288\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 323846.31, NNZs: 41, Bias: 2015.243554, T: 798642, Avg. loss: 11171258276.007132\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 282477.20, NNZs: 41, Bias: 2090.461013, T: 1197963, Avg. loss: 7921208927.643699\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 254291.27, NNZs: 41, Bias: 2133.530384, T: 1597284, Avg. loss: 6199356689.409877\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 235023.27, NNZs: 41, Bias: 2162.040643, T: 1996605, Avg. loss: 5113431449.436664\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 219378.13, NNZs: 41, Bias: 2179.214138, T: 2395926, Avg. loss: 4366939947.848230\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 207938.77, NNZs: 41, Bias: 2197.842419, T: 2795247, Avg. loss: 3820780398.853961\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 197808.71, NNZs: 41, Bias: 2208.078782, T: 3194568, Avg. loss: 3400034551.294104\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 190812.87, NNZs: 41, Bias: 2221.491539, T: 3593889, Avg. loss: 3068653746.454822\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 184091.04, NNZs: 41, Bias: 2230.530007, T: 3993210, Avg. loss: 2798242126.174591\n",
      "Total training time: 0.85 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 177600.36, NNZs: 41, Bias: 2237.566518, T: 4392531, Avg. loss: 2573431897.715768\n",
      "Total training time: 0.94 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 172239.40, NNZs: 41, Bias: 2243.426569, T: 4791852, Avg. loss: 2383756579.952745\n",
      "Total training time: 1.02 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 167740.04, NNZs: 41, Bias: 2248.282250, T: 5191173, Avg. loss: 2221143046.875364\n",
      "Total training time: 1.11 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 163429.80, NNZs: 41, Bias: 2251.579558, T: 5590494, Avg. loss: 2080236001.245252\n",
      "Total training time: 1.20 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 159769.64, NNZs: 41, Bias: 2254.109083, T: 5989815, Avg. loss: 1957001602.739542\n",
      "Total training time: 1.28 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 156772.16, NNZs: 41, Bias: 2258.772783, T: 6389136, Avg. loss: 1849011713.428278\n",
      "Total training time: 1.37 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 153374.44, NNZs: 41, Bias: 2262.216132, T: 6788457, Avg. loss: 1752782716.072788\n",
      "Total training time: 1.45 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 150225.82, NNZs: 41, Bias: 2262.657293, T: 7187778, Avg. loss: 1666137886.616057\n",
      "Total training time: 1.54 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 147315.51, NNZs: 41, Bias: 2264.715960, T: 7587099, Avg. loss: 1588197379.423441\n",
      "Total training time: 1.62 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 144563.97, NNZs: 41, Bias: 2265.075037, T: 7986420, Avg. loss: 1517663852.172258\n",
      "Total training time: 1.71 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 141882.96, NNZs: 41, Bias: 2266.264687, T: 8385741, Avg. loss: 1453615932.462582\n",
      "Total training time: 1.79 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 139399.18, NNZs: 41, Bias: 2266.149599, T: 8785062, Avg. loss: 1394670502.857424\n",
      "Total training time: 1.88 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 137361.47, NNZs: 41, Bias: 2267.377672, T: 9184383, Avg. loss: 1340846223.190828\n",
      "Total training time: 1.96 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 135058.56, NNZs: 41, Bias: 2266.977891, T: 9583704, Avg. loss: 1291079394.382431\n",
      "Total training time: 2.05 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 133096.43, NNZs: 41, Bias: 2266.885587, T: 9983025, Avg. loss: 1245082081.268094\n",
      "Total training time: 2.13 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 131213.46, NNZs: 41, Bias: 2267.059823, T: 10382346, Avg. loss: 1202406856.635680\n",
      "Total training time: 2.22 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 129202.58, NNZs: 41, Bias: 2265.233677, T: 10781667, Avg. loss: 1162437185.274725\n",
      "Total training time: 2.30 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 127468.66, NNZs: 41, Bias: 2264.750232, T: 11180988, Avg. loss: 1125383162.002451\n",
      "Total training time: 2.39 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 125778.52, NNZs: 41, Bias: 2264.031493, T: 11580309, Avg. loss: 1090751043.976578\n",
      "Total training time: 2.47 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 124231.67, NNZs: 41, Bias: 2263.921077, T: 11979630, Avg. loss: 1058225491.033140\n",
      "Total training time: 2.56 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 122612.38, NNZs: 41, Bias: 2263.012897, T: 12378951, Avg. loss: 1027719561.267356\n",
      "Total training time: 2.65 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 121157.88, NNZs: 41, Bias: 2262.795567, T: 12778272, Avg. loss: 999152700.571714\n",
      "Total training time: 2.73 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 119637.24, NNZs: 41, Bias: 2261.997394, T: 13177593, Avg. loss: 972091173.158496\n",
      "Total training time: 2.82 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 118338.66, NNZs: 41, Bias: 2261.818095, T: 13576914, Avg. loss: 946416264.482125\n",
      "Total training time: 2.90 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 116995.22, NNZs: 41, Bias: 2261.434217, T: 13976235, Avg. loss: 922223252.126569\n",
      "Total training time: 2.99 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 115701.25, NNZs: 41, Bias: 2260.327411, T: 14375556, Avg. loss: 899248885.350862\n",
      "Total training time: 3.08 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 114410.19, NNZs: 41, Bias: 2259.389834, T: 14774877, Avg. loss: 877478235.466574\n",
      "Total training time: 3.16 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 113198.16, NNZs: 41, Bias: 2258.241728, T: 15174198, Avg. loss: 856770042.008517\n",
      "Total training time: 3.25 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 111829.19, NNZs: 41, Bias: 2256.398358, T: 15573519, Avg. loss: 837015674.912189\n",
      "Total training time: 3.33 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 110807.25, NNZs: 41, Bias: 2256.284611, T: 15972840, Avg. loss: 818251082.805057\n",
      "Total training time: 3.42 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 109678.03, NNZs: 41, Bias: 2255.729556, T: 16372161, Avg. loss: 800371961.559532\n",
      "Total training time: 3.51 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 108570.92, NNZs: 41, Bias: 2254.823394, T: 16771482, Avg. loss: 783250631.018422\n",
      "Total training time: 3.59 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 107438.11, NNZs: 41, Bias: 2253.289146, T: 17170803, Avg. loss: 766919002.269249\n",
      "Total training time: 3.68 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 106343.93, NNZs: 41, Bias: 2251.955062, T: 17570124, Avg. loss: 751211197.817555\n",
      "Total training time: 3.76 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 105331.65, NNZs: 41, Bias: 2250.603706, T: 17969445, Avg. loss: 736221039.580800\n",
      "Total training time: 3.85 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 104352.96, NNZs: 41, Bias: 2250.024337, T: 18368766, Avg. loss: 721821596.050166\n",
      "Total training time: 3.93 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 103511.74, NNZs: 41, Bias: 2249.913017, T: 18768087, Avg. loss: 708081306.964051\n",
      "Total training time: 4.02 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 102559.67, NNZs: 41, Bias: 2249.310607, T: 19167408, Avg. loss: 694843266.172198\n",
      "Total training time: 4.10 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 101635.88, NNZs: 41, Bias: 2248.112241, T: 19566729, Avg. loss: 682097812.601088\n",
      "Total training time: 4.19 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 100753.46, NNZs: 41, Bias: 2247.266494, T: 19966050, Avg. loss: 669820120.650509\n",
      "Total training time: 4.28 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 99936.28, NNZs: 41, Bias: 2246.359610, T: 20365371, Avg. loss: 658032183.115153\n",
      "Total training time: 4.36 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 99139.71, NNZs: 41, Bias: 2245.641030, T: 20764692, Avg. loss: 646686668.880275\n",
      "Total training time: 4.45 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 98294.05, NNZs: 41, Bias: 2244.372556, T: 21164013, Avg. loss: 635687398.749760\n",
      "Total training time: 4.53 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 97448.80, NNZs: 41, Bias: 2243.263599, T: 21563334, Avg. loss: 625073718.290474\n",
      "Total training time: 4.62 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 96718.38, NNZs: 41, Bias: 2242.677652, T: 21962655, Avg. loss: 614843023.903062\n",
      "Total training time: 4.71 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 96008.46, NNZs: 41, Bias: 2242.238624, T: 22361976, Avg. loss: 604979423.330743\n",
      "Total training time: 4.80 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 95244.72, NNZs: 41, Bias: 2241.361298, T: 22761297, Avg. loss: 595381354.193292\n",
      "Total training time: 4.88 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 94543.06, NNZs: 41, Bias: 2240.558859, T: 23160618, Avg. loss: 586141034.333674\n",
      "Total training time: 4.97 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 93820.74, NNZs: 41, Bias: 2239.541715, T: 23559939, Avg. loss: 577194384.665188\n",
      "Total training time: 5.05 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 93134.31, NNZs: 41, Bias: 2238.439457, T: 23959260, Avg. loss: 568517332.846525\n",
      "Total training time: 5.13 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 92464.02, NNZs: 41, Bias: 2238.099588, T: 24358581, Avg. loss: 560161389.079179\n",
      "Total training time: 5.22 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 91732.03, NNZs: 41, Bias: 2236.710086, T: 24757902, Avg. loss: 551988266.673096\n",
      "Total training time: 5.31 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 91088.07, NNZs: 41, Bias: 2236.103201, T: 25157223, Avg. loss: 544077148.709893\n",
      "Total training time: 5.39 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 90420.38, NNZs: 41, Bias: 2235.342514, T: 25556544, Avg. loss: 536383305.031321\n",
      "Total training time: 5.48 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 89833.00, NNZs: 41, Bias: 2234.926836, T: 25955865, Avg. loss: 528956274.790371\n",
      "Total training time: 5.56 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 89215.96, NNZs: 41, Bias: 2234.090902, T: 26355186, Avg. loss: 521726293.968399\n",
      "Total training time: 5.65 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 88590.63, NNZs: 41, Bias: 2233.258686, T: 26754507, Avg. loss: 514680794.200121\n",
      "Total training time: 5.73 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 87988.96, NNZs: 41, Bias: 2232.859892, T: 27153828, Avg. loss: 507854682.442755\n",
      "Total training time: 5.82 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 88058.51, NNZs: 41, Bias: 2231.769979, T: 27553149, Avg. loss: 501206551.812623\n",
      "Total training time: 5.90 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 86828.17, NNZs: 41, Bias: 2231.210227, T: 27952470, Avg. loss: 494757671.763458\n",
      "Total training time: 5.99 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 86262.71, NNZs: 41, Bias: 2230.634382, T: 28351791, Avg. loss: 488453307.636433\n",
      "Total training time: 6.08 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 85689.11, NNZs: 41, Bias: 2229.936499, T: 28751112, Avg. loss: 482328208.852422\n",
      "Total training time: 6.16 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 85132.73, NNZs: 41, Bias: 2228.844275, T: 29150433, Avg. loss: 476357915.129573\n",
      "Total training time: 6.25 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 84584.70, NNZs: 41, Bias: 2228.306507, T: 29549754, Avg. loss: 470546149.860804\n",
      "Total training time: 6.33 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 84042.83, NNZs: 41, Bias: 2227.517537, T: 29949075, Avg. loss: 464861885.813438\n",
      "Total training time: 6.42 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 83531.91, NNZs: 41, Bias: 2226.966510, T: 30348396, Avg. loss: 459330938.667933\n",
      "Total training time: 6.51 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 83040.78, NNZs: 41, Bias: 2226.518738, T: 30747717, Avg. loss: 453948793.642135\n",
      "Total training time: 6.59 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 82543.18, NNZs: 41, Bias: 2225.987142, T: 31147038, Avg. loss: 448698225.958707\n",
      "Total training time: 6.68 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 82071.36, NNZs: 41, Bias: 2225.343075, T: 31546359, Avg. loss: 443571576.710967\n",
      "Total training time: 6.76 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 81595.00, NNZs: 41, Bias: 2224.984346, T: 31945680, Avg. loss: 438568801.641754\n",
      "Total training time: 6.85 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 81140.43, NNZs: 41, Bias: 2224.957567, T: 32345001, Avg. loss: 433681572.533032\n",
      "Total training time: 6.93 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 80675.75, NNZs: 41, Bias: 2224.341180, T: 32744322, Avg. loss: 428901419.422431\n",
      "Total training time: 7.02 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 80213.05, NNZs: 41, Bias: 2223.686284, T: 33143643, Avg. loss: 424214013.844820\n",
      "Total training time: 7.10 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 79727.02, NNZs: 41, Bias: 2223.498676, T: 33542964, Avg. loss: 419639248.630820\n",
      "Total training time: 7.19 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 79288.13, NNZs: 41, Bias: 2222.876751, T: 33942285, Avg. loss: 415176004.137606\n",
      "Total training time: 7.27 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 78873.48, NNZs: 41, Bias: 2222.430799, T: 34341606, Avg. loss: 410819737.665416\n",
      "Total training time: 7.36 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 78413.74, NNZs: 41, Bias: 2221.768416, T: 34740927, Avg. loss: 406546360.539443\n",
      "Total training time: 7.45 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 78016.21, NNZs: 41, Bias: 2221.311441, T: 35140248, Avg. loss: 402369489.998087\n",
      "Total training time: 7.53 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 77610.83, NNZs: 41, Bias: 2220.828821, T: 35539569, Avg. loss: 398279430.951671\n",
      "Total training time: 7.62 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 77210.49, NNZs: 41, Bias: 2220.702358, T: 35938890, Avg. loss: 394281132.564867\n",
      "Total training time: 7.70 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 76794.78, NNZs: 41, Bias: 2220.269818, T: 36338211, Avg. loss: 390352835.048230\n",
      "Total training time: 7.79 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 76380.97, NNZs: 41, Bias: 2219.826795, T: 36737532, Avg. loss: 386520530.808461\n",
      "Total training time: 7.87 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 76004.45, NNZs: 41, Bias: 2219.309353, T: 37136853, Avg. loss: 382765402.256687\n",
      "Total training time: 7.96 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 75599.07, NNZs: 41, Bias: 2218.706496, T: 37536174, Avg. loss: 379092453.412967\n",
      "Total training time: 8.04 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 75221.32, NNZs: 41, Bias: 2218.541670, T: 37935495, Avg. loss: 375492011.020854\n",
      "Total training time: 8.13 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 74821.80, NNZs: 41, Bias: 2218.150797, T: 38334816, Avg. loss: 371956213.649777\n",
      "Total training time: 8.21 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 74421.35, NNZs: 41, Bias: 2217.631557, T: 38734137, Avg. loss: 368476281.166662\n",
      "Total training time: 8.30 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 74055.34, NNZs: 41, Bias: 2217.225922, T: 39133458, Avg. loss: 365073614.175034\n",
      "Total training time: 8.38 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 73692.95, NNZs: 41, Bias: 2216.704183, T: 39532779, Avg. loss: 361731374.787861\n",
      "Total training time: 8.47 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 73358.94, NNZs: 41, Bias: 2216.637142, T: 39932100, Avg. loss: 358459511.122578\n",
      "Total training time: 8.55 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 73008.29, NNZs: 41, Bias: 2216.473447, T: 40331421, Avg. loss: 355248232.626237\n",
      "Total training time: 8.64 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 72656.84, NNZs: 41, Bias: 2216.438003, T: 40730742, Avg. loss: 352094172.740709\n",
      "Total training time: 8.72 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 72304.16, NNZs: 41, Bias: 2215.903072, T: 41130063, Avg. loss: 348994888.591162\n",
      "Total training time: 8.81 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 71959.24, NNZs: 41, Bias: 2215.666772, T: 41529384, Avg. loss: 345957035.642559\n",
      "Total training time: 8.90 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 71624.66, NNZs: 41, Bias: 2215.699818, T: 41928705, Avg. loss: 342968877.002842\n",
      "Total training time: 8.98 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 71298.70, NNZs: 41, Bias: 2215.200668, T: 42328026, Avg. loss: 340033378.424275\n",
      "Total training time: 9.07 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 70958.30, NNZs: 41, Bias: 2214.997447, T: 42727347, Avg. loss: 337153209.230694\n",
      "Total training time: 9.15 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 70609.64, NNZs: 41, Bias: 2214.332369, T: 43126668, Avg. loss: 334321727.250522\n",
      "Total training time: 9.24 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 70298.45, NNZs: 41, Bias: 2214.198643, T: 43525989, Avg. loss: 331540609.762793\n",
      "Total training time: 9.32 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 69962.85, NNZs: 41, Bias: 2213.921361, T: 43925310, Avg. loss: 328806278.706104\n",
      "Total training time: 9.41 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 69666.90, NNZs: 41, Bias: 2213.991183, T: 44324631, Avg. loss: 326125180.903278\n",
      "Total training time: 9.49 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 69347.43, NNZs: 41, Bias: 2213.813724, T: 44723952, Avg. loss: 323483011.556844\n",
      "Total training time: 9.58 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 69038.76, NNZs: 41, Bias: 2213.551372, T: 45123273, Avg. loss: 320892215.920759\n",
      "Total training time: 9.66 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 68720.30, NNZs: 41, Bias: 2213.459756, T: 45522594, Avg. loss: 318341573.082458\n",
      "Total training time: 9.75 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 68427.72, NNZs: 41, Bias: 2213.242372, T: 45921915, Avg. loss: 315835669.681002\n",
      "Total training time: 9.83 seconds.\n",
      "-- Epoch 116\n",
      "Norm: 68133.29, NNZs: 41, Bias: 2212.955487, T: 46321236, Avg. loss: 313366302.524119\n",
      "Total training time: 9.92 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 67828.72, NNZs: 41, Bias: 2212.753154, T: 46720557, Avg. loss: 310935617.992416\n",
      "Total training time: 10.00 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 67557.20, NNZs: 41, Bias: 2212.623685, T: 47119878, Avg. loss: 308548334.680751\n",
      "Total training time: 10.09 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 67267.78, NNZs: 41, Bias: 2212.741839, T: 47519199, Avg. loss: 306195355.619641\n",
      "Total training time: 10.17 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 66973.48, NNZs: 41, Bias: 2212.389939, T: 47918520, Avg. loss: 303880976.619092\n",
      "Total training time: 10.26 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 66678.36, NNZs: 41, Bias: 2212.150298, T: 48317841, Avg. loss: 301600905.116778\n",
      "Total training time: 10.34 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 66394.77, NNZs: 41, Bias: 2212.034666, T: 48717162, Avg. loss: 299353750.328996\n",
      "Total training time: 10.43 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 66137.25, NNZs: 41, Bias: 2211.898675, T: 49116483, Avg. loss: 297147385.669539\n",
      "Total training time: 10.51 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 65870.25, NNZs: 41, Bias: 2211.894096, T: 49515804, Avg. loss: 294977278.510289\n",
      "Total training time: 10.60 seconds.\n",
      "-- Epoch 125\n",
      "Norm: 65587.94, NNZs: 41, Bias: 2211.871427, T: 49915125, Avg. loss: 292833252.716149\n",
      "Total training time: 10.68 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 65330.02, NNZs: 41, Bias: 2211.811916, T: 50314446, Avg. loss: 290729895.578298\n",
      "Total training time: 10.77 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 65066.70, NNZs: 41, Bias: 2211.641354, T: 50713767, Avg. loss: 288649068.823083\n",
      "Total training time: 10.85 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 64807.36, NNZs: 41, Bias: 2211.379254, T: 51113088, Avg. loss: 286601753.283750\n",
      "Total training time: 10.94 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 64547.89, NNZs: 41, Bias: 2211.001941, T: 51512409, Avg. loss: 284583030.365018\n",
      "Total training time: 11.02 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 64285.19, NNZs: 41, Bias: 2210.753248, T: 51911730, Avg. loss: 282592297.405371\n",
      "Total training time: 11.11 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 64030.97, NNZs: 41, Bias: 2210.602117, T: 52311051, Avg. loss: 280633920.347408\n",
      "Total training time: 11.19 seconds.\n",
      "-- Epoch 132\n",
      "Norm: 63783.71, NNZs: 41, Bias: 2210.727711, T: 52710372, Avg. loss: 278705543.581711\n",
      "Total training time: 11.28 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 63548.29, NNZs: 41, Bias: 2210.472386, T: 53109693, Avg. loss: 276803936.481478\n",
      "Total training time: 11.37 seconds.\n",
      "-- Epoch 134\n",
      "Norm: 63302.04, NNZs: 41, Bias: 2210.316383, T: 53509014, Avg. loss: 274924266.172376\n",
      "Total training time: 11.45 seconds.\n",
      "-- Epoch 135\n",
      "Norm: 63041.58, NNZs: 41, Bias: 2210.164397, T: 53908335, Avg. loss: 273070250.325993\n",
      "Total training time: 11.54 seconds.\n",
      "-- Epoch 136\n",
      "Norm: 62798.83, NNZs: 41, Bias: 2210.015066, T: 54307656, Avg. loss: 271242156.946360\n",
      "Total training time: 11.62 seconds.\n",
      "-- Epoch 137\n",
      "Norm: 62556.71, NNZs: 41, Bias: 2209.723608, T: 54706977, Avg. loss: 269438874.865050\n",
      "Total training time: 11.71 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 62318.10, NNZs: 41, Bias: 2209.529889, T: 55106298, Avg. loss: 267663375.241573\n",
      "Total training time: 11.79 seconds.\n",
      "-- Epoch 139\n",
      "Norm: 62093.45, NNZs: 41, Bias: 2209.567501, T: 55505619, Avg. loss: 265911565.639297\n",
      "Total training time: 11.87 seconds.\n",
      "-- Epoch 140\n",
      "Norm: 61852.73, NNZs: 41, Bias: 2209.406629, T: 55904940, Avg. loss: 264184361.347328\n",
      "Total training time: 11.96 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 61621.87, NNZs: 41, Bias: 2209.240050, T: 56304261, Avg. loss: 262479359.373744\n",
      "Total training time: 12.04 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 61379.26, NNZs: 41, Bias: 2209.155243, T: 56703582, Avg. loss: 260796679.289790\n",
      "Total training time: 12.13 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 61176.52, NNZs: 41, Bias: 2209.156008, T: 57102903, Avg. loss: 259140799.862461\n",
      "Total training time: 12.21 seconds.\n",
      "-- Epoch 144\n",
      "Norm: 60943.24, NNZs: 41, Bias: 2209.089860, T: 57502224, Avg. loss: 257506791.912021\n",
      "Total training time: 12.30 seconds.\n",
      "-- Epoch 145\n",
      "Norm: 60716.28, NNZs: 41, Bias: 2208.966819, T: 57901545, Avg. loss: 255893927.115510\n",
      "Total training time: 12.38 seconds.\n",
      "-- Epoch 146\n",
      "Norm: 60502.77, NNZs: 41, Bias: 2208.757624, T: 58300866, Avg. loss: 254298867.581248\n",
      "Total training time: 12.47 seconds.\n",
      "-- Epoch 147\n",
      "Norm: 60290.53, NNZs: 41, Bias: 2208.842497, T: 58700187, Avg. loss: 252731481.782201\n",
      "Total training time: 12.55 seconds.\n",
      "-- Epoch 148\n",
      "Norm: 60085.77, NNZs: 41, Bias: 2208.762670, T: 59099508, Avg. loss: 251180901.596476\n",
      "Total training time: 12.64 seconds.\n",
      "-- Epoch 149\n",
      "Norm: 59871.89, NNZs: 41, Bias: 2208.743685, T: 59498829, Avg. loss: 249649117.082721\n",
      "Total training time: 12.72 seconds.\n",
      "-- Epoch 150\n",
      "Norm: 59662.24, NNZs: 41, Bias: 2208.860805, T: 59898150, Avg. loss: 248135385.159172\n",
      "Total training time: 12.81 seconds.\n",
      "-- Epoch 151\n",
      "Norm: 59435.38, NNZs: 41, Bias: 2208.734768, T: 60297471, Avg. loss: 246642645.739596\n",
      "Total training time: 12.89 seconds.\n",
      "-- Epoch 152\n",
      "Norm: 59234.89, NNZs: 41, Bias: 2208.675047, T: 60696792, Avg. loss: 245170679.520978\n",
      "Total training time: 12.98 seconds.\n",
      "-- Epoch 153\n",
      "Norm: 59038.23, NNZs: 41, Bias: 2208.666608, T: 61096113, Avg. loss: 243717437.316801\n",
      "Total training time: 13.06 seconds.\n",
      "-- Epoch 154\n",
      "Norm: 58840.36, NNZs: 41, Bias: 2208.722449, T: 61495434, Avg. loss: 242280404.573598\n",
      "Total training time: 13.15 seconds.\n",
      "-- Epoch 155\n",
      "Norm: 58640.98, NNZs: 41, Bias: 2208.831657, T: 61894755, Avg. loss: 240861363.128176\n",
      "Total training time: 13.23 seconds.\n",
      "-- Epoch 156\n",
      "Norm: 58436.37, NNZs: 41, Bias: 2208.708458, T: 62294076, Avg. loss: 239452750.512745\n",
      "Total training time: 13.32 seconds.\n",
      "-- Epoch 157\n",
      "Norm: 58242.86, NNZs: 41, Bias: 2208.766840, T: 62693397, Avg. loss: 238067339.276623\n",
      "Total training time: 13.40 seconds.\n",
      "-- Epoch 158\n",
      "Norm: 58046.03, NNZs: 41, Bias: 2208.588398, T: 63092718, Avg. loss: 236694030.625872\n",
      "Total training time: 13.49 seconds.\n",
      "-- Epoch 159\n",
      "Norm: 57853.27, NNZs: 41, Bias: 2208.457778, T: 63492039, Avg. loss: 235339147.116656\n",
      "Total training time: 13.57 seconds.\n",
      "-- Epoch 160\n",
      "Norm: 57657.81, NNZs: 41, Bias: 2208.270574, T: 63891360, Avg. loss: 233999158.416706\n",
      "Total training time: 13.66 seconds.\n",
      "-- Epoch 161\n",
      "Norm: 57475.13, NNZs: 41, Bias: 2208.427907, T: 64290681, Avg. loss: 232676663.736347\n",
      "Total training time: 13.74 seconds.\n",
      "-- Epoch 162\n",
      "Norm: 57270.16, NNZs: 41, Bias: 2208.114245, T: 64690002, Avg. loss: 231369968.681303\n",
      "Total training time: 13.82 seconds.\n",
      "-- Epoch 163\n",
      "Norm: 57073.36, NNZs: 41, Bias: 2208.090240, T: 65089323, Avg. loss: 230075827.092996\n",
      "Total training time: 13.91 seconds.\n",
      "-- Epoch 164\n",
      "Norm: 56897.39, NNZs: 41, Bias: 2208.367568, T: 65488644, Avg. loss: 228800377.180366\n",
      "Total training time: 13.99 seconds.\n",
      "-- Epoch 165\n",
      "Norm: 56721.02, NNZs: 41, Bias: 2208.376638, T: 65887965, Avg. loss: 227538363.770428\n",
      "Total training time: 14.08 seconds.\n",
      "-- Epoch 166\n",
      "Norm: 56540.71, NNZs: 41, Bias: 2208.332959, T: 66287286, Avg. loss: 226292815.683719\n",
      "Total training time: 14.16 seconds.\n",
      "-- Epoch 167\n",
      "Norm: 56363.08, NNZs: 41, Bias: 2208.324462, T: 66686607, Avg. loss: 225058237.934453\n",
      "Total training time: 14.25 seconds.\n",
      "-- Epoch 168\n",
      "Norm: 56199.28, NNZs: 41, Bias: 2208.533557, T: 67085928, Avg. loss: 223840597.172288\n",
      "Total training time: 14.33 seconds.\n",
      "-- Epoch 169\n",
      "Norm: 56021.49, NNZs: 41, Bias: 2208.608632, T: 67485249, Avg. loss: 222637855.315760\n",
      "Total training time: 14.42 seconds.\n",
      "-- Epoch 170\n",
      "Norm: 55840.03, NNZs: 41, Bias: 2208.767128, T: 67884570, Avg. loss: 221446211.149577\n",
      "Total training time: 14.51 seconds.\n",
      "-- Epoch 171\n",
      "Norm: 55661.33, NNZs: 41, Bias: 2208.647868, T: 68283891, Avg. loss: 220265837.844836\n",
      "Total training time: 14.59 seconds.\n",
      "-- Epoch 172\n",
      "Norm: 55480.39, NNZs: 41, Bias: 2208.726196, T: 68683212, Avg. loss: 219098669.477637\n",
      "Total training time: 14.68 seconds.\n",
      "-- Epoch 173\n",
      "Norm: 55309.72, NNZs: 41, Bias: 2208.626213, T: 69082533, Avg. loss: 217946650.011182\n",
      "Total training time: 14.76 seconds.\n",
      "-- Epoch 174\n",
      "Norm: 55149.01, NNZs: 41, Bias: 2208.787435, T: 69481854, Avg. loss: 216808655.186060\n",
      "Total training time: 14.85 seconds.\n",
      "-- Epoch 175\n",
      "Norm: 54966.65, NNZs: 41, Bias: 2208.572031, T: 69881175, Avg. loss: 215676255.717959\n",
      "Total training time: 14.93 seconds.\n",
      "-- Epoch 176\n",
      "Norm: 54788.04, NNZs: 41, Bias: 2208.590388, T: 70280496, Avg. loss: 214558755.203874\n",
      "Total training time: 15.02 seconds.\n",
      "-- Epoch 177\n",
      "Norm: 54632.21, NNZs: 41, Bias: 2208.709058, T: 70679817, Avg. loss: 213459124.925740\n",
      "Total training time: 15.10 seconds.\n",
      "-- Epoch 178\n",
      "Norm: 54473.31, NNZs: 41, Bias: 2208.715180, T: 71079138, Avg. loss: 212365857.005344\n",
      "Total training time: 15.19 seconds.\n",
      "-- Epoch 179\n",
      "Norm: 54312.93, NNZs: 41, Bias: 2208.817506, T: 71478459, Avg. loss: 211287753.408539\n",
      "Total training time: 15.27 seconds.\n",
      "-- Epoch 180\n",
      "Norm: 54135.39, NNZs: 41, Bias: 2208.680677, T: 71877780, Avg. loss: 210217335.890981\n",
      "Total training time: 15.36 seconds.\n",
      "-- Epoch 181\n",
      "Norm: 53975.56, NNZs: 41, Bias: 2208.793406, T: 72277101, Avg. loss: 209159792.689078\n",
      "Total training time: 15.44 seconds.\n",
      "-- Epoch 182\n",
      "Norm: 53811.12, NNZs: 41, Bias: 2208.831561, T: 72676422, Avg. loss: 208114322.132544\n",
      "Total training time: 15.53 seconds.\n",
      "-- Epoch 183\n",
      "Norm: 53654.29, NNZs: 41, Bias: 2208.842263, T: 73075743, Avg. loss: 207078855.551650\n",
      "Total training time: 15.61 seconds.\n",
      "-- Epoch 184\n",
      "Norm: 53496.61, NNZs: 41, Bias: 2208.896074, T: 73475064, Avg. loss: 206054004.447399\n",
      "Total training time: 15.70 seconds.\n",
      "-- Epoch 185\n",
      "Norm: 53339.85, NNZs: 41, Bias: 2208.881308, T: 73874385, Avg. loss: 205038159.065200\n",
      "Total training time: 15.78 seconds.\n",
      "-- Epoch 186\n",
      "Norm: 53176.76, NNZs: 41, Bias: 2208.879078, T: 74273706, Avg. loss: 204033047.356730\n",
      "Total training time: 15.86 seconds.\n",
      "-- Epoch 187\n",
      "Norm: 53011.14, NNZs: 41, Bias: 2208.902729, T: 74673027, Avg. loss: 203036978.491070\n",
      "Total training time: 15.95 seconds.\n",
      "-- Epoch 188\n",
      "Norm: 52857.00, NNZs: 41, Bias: 2208.938253, T: 75072348, Avg. loss: 202053169.789783\n",
      "Total training time: 16.03 seconds.\n",
      "-- Epoch 189\n",
      "Norm: 52703.44, NNZs: 41, Bias: 2209.012929, T: 75471669, Avg. loss: 201079211.437458\n",
      "Total training time: 16.12 seconds.\n",
      "-- Epoch 190\n",
      "Norm: 52550.80, NNZs: 41, Bias: 2209.144021, T: 75870990, Avg. loss: 200114039.082099\n",
      "Total training time: 16.20 seconds.\n",
      "-- Epoch 191\n",
      "Norm: 52396.95, NNZs: 41, Bias: 2209.150748, T: 76270311, Avg. loss: 199158743.578803\n",
      "Total training time: 16.29 seconds.\n",
      "-- Epoch 192\n",
      "Norm: 52244.14, NNZs: 41, Bias: 2209.094069, T: 76669632, Avg. loss: 198211125.767031\n",
      "Total training time: 16.37 seconds.\n",
      "-- Epoch 193\n",
      "Norm: 52098.33, NNZs: 41, Bias: 2209.129019, T: 77068953, Avg. loss: 197276875.759133\n",
      "Total training time: 16.46 seconds.\n",
      "-- Epoch 194\n",
      "Norm: 51952.19, NNZs: 41, Bias: 2209.231251, T: 77468274, Avg. loss: 196349170.003174\n",
      "Total training time: 16.54 seconds.\n",
      "-- Epoch 195\n",
      "Norm: 51811.10, NNZs: 41, Bias: 2209.331203, T: 77867595, Avg. loss: 195431233.128898\n",
      "Total training time: 16.63 seconds.\n",
      "-- Epoch 196\n",
      "Norm: 51666.42, NNZs: 41, Bias: 2209.281794, T: 78266916, Avg. loss: 194522198.850622\n",
      "Total training time: 16.71 seconds.\n",
      "-- Epoch 197\n",
      "Norm: 51513.91, NNZs: 41, Bias: 2209.265881, T: 78666237, Avg. loss: 193619986.645782\n",
      "Total training time: 16.80 seconds.\n",
      "-- Epoch 198\n",
      "Norm: 51368.88, NNZs: 41, Bias: 2209.454463, T: 79065558, Avg. loss: 192730672.999219\n",
      "Total training time: 16.88 seconds.\n",
      "-- Epoch 199\n",
      "Norm: 51221.80, NNZs: 41, Bias: 2209.558015, T: 79464879, Avg. loss: 191848375.448721\n",
      "Total training time: 16.97 seconds.\n",
      "-- Epoch 200\n",
      "Norm: 51079.35, NNZs: 41, Bias: 2209.559977, T: 79864200, Avg. loss: 190974047.383634\n",
      "Total training time: 17.05 seconds.\n",
      "-- Epoch 201\n",
      "Norm: 50940.95, NNZs: 41, Bias: 2209.617091, T: 80263521, Avg. loss: 190107867.628001\n",
      "Total training time: 17.14 seconds.\n",
      "-- Epoch 202\n",
      "Norm: 50797.41, NNZs: 41, Bias: 2209.657385, T: 80662842, Avg. loss: 189250648.656682\n",
      "Total training time: 17.22 seconds.\n",
      "-- Epoch 203\n",
      "Norm: 50659.69, NNZs: 41, Bias: 2209.749440, T: 81062163, Avg. loss: 188399874.338974\n",
      "Total training time: 17.30 seconds.\n",
      "-- Epoch 204\n",
      "Norm: 50522.70, NNZs: 41, Bias: 2209.790042, T: 81461484, Avg. loss: 187556560.183399\n",
      "Total training time: 17.39 seconds.\n",
      "-- Epoch 205\n",
      "Norm: 50387.27, NNZs: 41, Bias: 2209.848883, T: 81860805, Avg. loss: 186722302.411619\n",
      "Total training time: 17.47 seconds.\n",
      "-- Epoch 206\n",
      "Norm: 50252.58, NNZs: 41, Bias: 2209.842284, T: 82260126, Avg. loss: 185895748.199127\n",
      "Total training time: 17.56 seconds.\n",
      "-- Epoch 207\n",
      "Norm: 50126.93, NNZs: 41, Bias: 2209.968336, T: 82659447, Avg. loss: 185080034.887177\n",
      "Total training time: 17.64 seconds.\n",
      "-- Epoch 208\n",
      "Norm: 49994.20, NNZs: 41, Bias: 2210.021348, T: 83058768, Avg. loss: 184270630.922871\n",
      "Total training time: 17.73 seconds.\n",
      "-- Epoch 209\n",
      "Norm: 49861.81, NNZs: 41, Bias: 2210.066793, T: 83458089, Avg. loss: 183466723.368844\n",
      "Total training time: 17.81 seconds.\n",
      "-- Epoch 210\n",
      "Norm: 49722.30, NNZs: 41, Bias: 2210.067878, T: 83857410, Avg. loss: 182669055.539018\n",
      "Total training time: 17.89 seconds.\n",
      "-- Epoch 211\n",
      "Norm: 49591.92, NNZs: 41, Bias: 2209.972479, T: 84256731, Avg. loss: 181879698.255709\n",
      "Total training time: 17.98 seconds.\n",
      "-- Epoch 212\n",
      "Norm: 49467.49, NNZs: 41, Bias: 2210.184245, T: 84656052, Avg. loss: 181098097.628455\n",
      "Total training time: 18.06 seconds.\n",
      "-- Epoch 213\n",
      "Norm: 49342.51, NNZs: 41, Bias: 2210.331439, T: 85055373, Avg. loss: 180324063.415996\n",
      "Total training time: 18.15 seconds.\n",
      "-- Epoch 214\n",
      "Norm: 49208.65, NNZs: 41, Bias: 2210.348523, T: 85454694, Avg. loss: 179555167.508978\n",
      "Total training time: 18.23 seconds.\n",
      "-- Epoch 215\n",
      "Norm: 49081.08, NNZs: 41, Bias: 2210.430980, T: 85854015, Avg. loss: 178794024.269353\n",
      "Total training time: 18.32 seconds.\n",
      "-- Epoch 216\n",
      "Norm: 48955.95, NNZs: 41, Bias: 2210.511842, T: 86253336, Avg. loss: 178038818.324170\n",
      "Total training time: 18.40 seconds.\n",
      "-- Epoch 217\n",
      "Norm: 48835.81, NNZs: 41, Bias: 2210.567383, T: 86652657, Avg. loss: 177290749.892283\n",
      "Total training time: 18.49 seconds.\n",
      "-- Epoch 218\n",
      "Norm: 48712.74, NNZs: 41, Bias: 2210.578854, T: 87051978, Avg. loss: 176546846.799908\n",
      "Total training time: 18.57 seconds.\n",
      "-- Epoch 219\n",
      "Norm: 48591.17, NNZs: 41, Bias: 2210.721611, T: 87451299, Avg. loss: 175811987.286282\n",
      "Total training time: 18.65 seconds.\n",
      "-- Epoch 220\n",
      "Norm: 48471.26, NNZs: 41, Bias: 2210.832857, T: 87850620, Avg. loss: 175083561.227221\n",
      "Total training time: 18.74 seconds.\n",
      "-- Epoch 221\n",
      "Norm: 48351.12, NNZs: 41, Bias: 2210.929136, T: 88249941, Avg. loss: 174359251.445403\n",
      "Total training time: 18.82 seconds.\n",
      "-- Epoch 222\n",
      "Norm: 48229.18, NNZs: 41, Bias: 2210.963830, T: 88649262, Avg. loss: 173642732.044540\n",
      "Total training time: 18.91 seconds.\n",
      "-- Epoch 223\n",
      "Norm: 48120.30, NNZs: 41, Bias: 2211.128317, T: 89048583, Avg. loss: 172934013.055788\n",
      "Total training time: 18.99 seconds.\n",
      "-- Epoch 224\n",
      "Norm: 48004.18, NNZs: 41, Bias: 2211.173879, T: 89447904, Avg. loss: 172230314.387173\n",
      "Total training time: 19.08 seconds.\n",
      "-- Epoch 225\n",
      "Norm: 47890.29, NNZs: 41, Bias: 2211.257856, T: 89847225, Avg. loss: 171532608.430790\n",
      "Total training time: 19.16 seconds.\n",
      "-- Epoch 226\n",
      "Norm: 47765.61, NNZs: 41, Bias: 2211.372584, T: 90246546, Avg. loss: 170839729.273340\n",
      "Total training time: 19.25 seconds.\n",
      "-- Epoch 227\n",
      "Norm: 47656.98, NNZs: 41, Bias: 2211.603859, T: 90645867, Avg. loss: 170154919.655228\n",
      "Total training time: 19.33 seconds.\n",
      "-- Epoch 228\n",
      "Norm: 47541.96, NNZs: 41, Bias: 2211.795235, T: 91045188, Avg. loss: 169475362.787308\n",
      "Total training time: 19.42 seconds.\n",
      "-- Epoch 229\n",
      "Norm: 47429.55, NNZs: 41, Bias: 2212.002237, T: 91444509, Avg. loss: 168801379.060706\n",
      "Total training time: 19.50 seconds.\n",
      "-- Epoch 230\n",
      "Norm: 47313.18, NNZs: 41, Bias: 2212.041564, T: 91843830, Avg. loss: 168131923.726291\n",
      "Total training time: 19.59 seconds.\n",
      "-- Epoch 231\n",
      "Norm: 47205.83, NNZs: 41, Bias: 2212.150914, T: 92243151, Avg. loss: 167468752.863884\n",
      "Total training time: 19.67 seconds.\n",
      "-- Epoch 232\n",
      "Norm: 47090.00, NNZs: 41, Bias: 2212.259705, T: 92642472, Avg. loss: 166809231.645627\n",
      "Total training time: 19.75 seconds.\n",
      "-- Epoch 233\n",
      "Norm: 46975.73, NNZs: 41, Bias: 2212.256634, T: 93041793, Avg. loss: 166155759.947598\n",
      "Total training time: 19.84 seconds.\n",
      "-- Epoch 234\n",
      "Norm: 46873.09, NNZs: 41, Bias: 2212.492511, T: 93441114, Avg. loss: 165508429.098225\n",
      "Total training time: 19.92 seconds.\n",
      "-- Epoch 235\n",
      "Norm: 46759.27, NNZs: 41, Bias: 2212.436393, T: 93840435, Avg. loss: 164866396.261901\n",
      "Total training time: 20.01 seconds.\n",
      "-- Epoch 236\n",
      "Norm: 46645.26, NNZs: 41, Bias: 2212.513378, T: 94239756, Avg. loss: 164229090.004327\n",
      "Total training time: 20.09 seconds.\n",
      "-- Epoch 237\n",
      "Norm: 46534.86, NNZs: 41, Bias: 2212.574435, T: 94639077, Avg. loss: 163596178.601962\n",
      "Total training time: 20.18 seconds.\n",
      "-- Epoch 238\n",
      "Norm: 46434.97, NNZs: 41, Bias: 2212.852775, T: 95038398, Avg. loss: 162968876.470610\n",
      "Total training time: 20.26 seconds.\n",
      "-- Epoch 239\n",
      "Norm: 46334.87, NNZs: 41, Bias: 2213.010819, T: 95437719, Avg. loss: 162347800.779001\n",
      "Total training time: 20.35 seconds.\n",
      "-- Epoch 240\n",
      "Norm: 46226.78, NNZs: 41, Bias: 2213.106525, T: 95837040, Avg. loss: 161731201.843586\n",
      "Total training time: 20.43 seconds.\n",
      "-- Epoch 241\n",
      "Norm: 46121.43, NNZs: 41, Bias: 2213.277133, T: 96236361, Avg. loss: 161118549.252456\n",
      "Total training time: 20.52 seconds.\n",
      "-- Epoch 242\n",
      "Norm: 46007.22, NNZs: 41, Bias: 2213.256306, T: 96635682, Avg. loss: 160510094.245293\n",
      "Total training time: 20.60 seconds.\n",
      "-- Epoch 243\n",
      "Norm: 45905.67, NNZs: 41, Bias: 2213.360319, T: 97035003, Avg. loss: 159906177.521505\n",
      "Total training time: 20.69 seconds.\n",
      "-- Epoch 244\n",
      "Norm: 45799.87, NNZs: 41, Bias: 2213.586510, T: 97434324, Avg. loss: 159309107.363431\n",
      "Total training time: 20.77 seconds.\n",
      "-- Epoch 245\n",
      "Norm: 45693.34, NNZs: 41, Bias: 2213.670750, T: 97833645, Avg. loss: 158715702.654129\n",
      "Total training time: 20.85 seconds.\n",
      "-- Epoch 246\n",
      "Norm: 45590.60, NNZs: 41, Bias: 2213.835493, T: 98232966, Avg. loss: 158127990.124514\n",
      "Total training time: 20.94 seconds.\n",
      "-- Epoch 247\n",
      "Norm: 45486.76, NNZs: 41, Bias: 2213.815955, T: 98632287, Avg. loss: 157543052.172527\n",
      "Total training time: 21.02 seconds.\n",
      "-- Epoch 248\n",
      "Norm: 45377.75, NNZs: 41, Bias: 2213.898408, T: 99031608, Avg. loss: 156964511.698300\n",
      "Total training time: 21.11 seconds.\n",
      "-- Epoch 249\n",
      "Norm: 45276.75, NNZs: 41, Bias: 2214.015019, T: 99430929, Avg. loss: 156389199.949156\n",
      "Total training time: 21.19 seconds.\n",
      "-- Epoch 250\n",
      "Norm: 45176.30, NNZs: 41, Bias: 2214.134317, T: 99830250, Avg. loss: 155817745.764341\n",
      "Total training time: 21.28 seconds.\n",
      "-- Epoch 251\n",
      "Norm: 45068.31, NNZs: 41, Bias: 2214.242026, T: 100229571, Avg. loss: 155249978.228224\n",
      "Total training time: 21.36 seconds.\n",
      "-- Epoch 252\n",
      "Norm: 44969.61, NNZs: 41, Bias: 2214.286335, T: 100628892, Avg. loss: 154688182.842531\n",
      "Total training time: 21.45 seconds.\n",
      "-- Epoch 253\n",
      "Norm: 44867.71, NNZs: 41, Bias: 2214.438372, T: 101028213, Avg. loss: 154129634.921798\n",
      "Total training time: 21.53 seconds.\n",
      "-- Epoch 254\n",
      "Norm: 44760.59, NNZs: 41, Bias: 2214.491739, T: 101427534, Avg. loss: 153574212.037881\n",
      "Total training time: 21.61 seconds.\n",
      "-- Epoch 255\n",
      "Norm: 44662.65, NNZs: 41, Bias: 2214.591407, T: 101826855, Avg. loss: 153023390.600322\n",
      "Total training time: 21.70 seconds.\n",
      "-- Epoch 256\n",
      "Norm: 44569.28, NNZs: 41, Bias: 2214.808266, T: 102226176, Avg. loss: 152477649.455004\n",
      "Total training time: 21.78 seconds.\n",
      "-- Epoch 257\n",
      "Norm: 44470.35, NNZs: 41, Bias: 2214.868139, T: 102625497, Avg. loss: 151934817.602154\n",
      "Total training time: 21.87 seconds.\n",
      "-- Epoch 258\n",
      "Norm: 44371.79, NNZs: 41, Bias: 2214.975301, T: 103024818, Avg. loss: 151396770.502156\n",
      "Total training time: 21.95 seconds.\n",
      "-- Epoch 259\n",
      "Norm: 44276.08, NNZs: 41, Bias: 2215.162414, T: 103424139, Avg. loss: 150862741.573783\n",
      "Total training time: 22.04 seconds.\n",
      "-- Epoch 260\n",
      "Norm: 44188.95, NNZs: 41, Bias: 2215.284218, T: 103823460, Avg. loss: 150333716.544788\n",
      "Total training time: 22.12 seconds.\n",
      "-- Epoch 261\n",
      "Norm: 44095.59, NNZs: 41, Bias: 2215.370497, T: 104222781, Avg. loss: 149807941.378124\n",
      "Total training time: 22.20 seconds.\n",
      "-- Epoch 262\n",
      "Norm: 43999.50, NNZs: 41, Bias: 2215.472884, T: 104622102, Avg. loss: 149286452.165043\n",
      "Total training time: 22.29 seconds.\n",
      "-- Epoch 263\n",
      "Norm: 43905.34, NNZs: 41, Bias: 2215.553114, T: 105021423, Avg. loss: 148767427.039482\n",
      "Total training time: 22.37 seconds.\n",
      "-- Epoch 264\n",
      "Norm: 43816.85, NNZs: 41, Bias: 2215.797249, T: 105420744, Avg. loss: 148252386.204362\n",
      "Total training time: 22.46 seconds.\n",
      "-- Epoch 265\n",
      "Norm: 43719.98, NNZs: 41, Bias: 2215.890001, T: 105820065, Avg. loss: 147741555.791727\n",
      "Total training time: 22.54 seconds.\n",
      "-- Epoch 266\n",
      "Norm: 43628.85, NNZs: 41, Bias: 2215.976271, T: 106219386, Avg. loss: 147233335.513128\n",
      "Total training time: 22.63 seconds.\n",
      "-- Epoch 267\n",
      "Norm: 43539.82, NNZs: 41, Bias: 2216.080527, T: 106618707, Avg. loss: 146729442.510823\n",
      "Total training time: 22.71 seconds.\n",
      "-- Epoch 268\n",
      "Norm: 43454.09, NNZs: 41, Bias: 2216.278135, T: 107018028, Avg. loss: 146229561.739217\n",
      "Total training time: 22.80 seconds.\n",
      "-- Epoch 269\n",
      "Norm: 43359.44, NNZs: 41, Bias: 2216.356067, T: 107417349, Avg. loss: 145732207.062785\n",
      "Total training time: 22.88 seconds.\n",
      "-- Epoch 270\n",
      "Norm: 43272.55, NNZs: 41, Bias: 2216.549660, T: 107816670, Avg. loss: 145238840.187848\n",
      "Total training time: 22.97 seconds.\n",
      "-- Epoch 271\n",
      "Norm: 43187.02, NNZs: 41, Bias: 2216.599321, T: 108215991, Avg. loss: 144749251.958057\n",
      "Total training time: 23.05 seconds.\n",
      "-- Epoch 272\n",
      "Norm: 43106.40, NNZs: 41, Bias: 2216.700768, T: 108615312, Avg. loss: 144264284.416388\n",
      "Total training time: 23.13 seconds.\n",
      "-- Epoch 273\n",
      "Norm: 43023.26, NNZs: 41, Bias: 2216.893221, T: 109014633, Avg. loss: 143782467.698089\n",
      "Total training time: 23.22 seconds.\n",
      "-- Epoch 274\n",
      "Norm: 42937.39, NNZs: 41, Bias: 2216.995472, T: 109413954, Avg. loss: 143302977.809909\n",
      "Total training time: 23.30 seconds.\n",
      "-- Epoch 275\n",
      "Norm: 42850.36, NNZs: 41, Bias: 2217.166651, T: 109813275, Avg. loss: 142826963.129371\n",
      "Total training time: 23.39 seconds.\n",
      "-- Epoch 276\n",
      "Norm: 42766.17, NNZs: 41, Bias: 2217.229146, T: 110212596, Avg. loss: 142353197.600738\n",
      "Total training time: 23.47 seconds.\n",
      "-- Epoch 277\n",
      "Norm: 42679.39, NNZs: 41, Bias: 2217.320833, T: 110611917, Avg. loss: 141883286.590934\n",
      "Total training time: 23.55 seconds.\n",
      "-- Epoch 278\n",
      "Norm: 42592.08, NNZs: 41, Bias: 2217.465600, T: 111011238, Avg. loss: 141416103.087223\n",
      "Total training time: 23.64 seconds.\n",
      "-- Epoch 279\n",
      "Norm: 42513.57, NNZs: 41, Bias: 2217.648220, T: 111410559, Avg. loss: 140953591.813612\n",
      "Total training time: 23.72 seconds.\n",
      "-- Epoch 280\n",
      "Norm: 42425.18, NNZs: 41, Bias: 2217.691831, T: 111809880, Avg. loss: 140493891.096132\n",
      "Total training time: 23.81 seconds.\n",
      "-- Epoch 281\n",
      "Norm: 42349.52, NNZs: 41, Bias: 2217.853942, T: 112209201, Avg. loss: 140038176.941783\n",
      "Total training time: 23.89 seconds.\n",
      "-- Epoch 282\n",
      "Norm: 42257.60, NNZs: 41, Bias: 2217.969511, T: 112608522, Avg. loss: 139583939.641757\n",
      "Total training time: 23.98 seconds.\n",
      "-- Epoch 283\n",
      "Norm: 42175.09, NNZs: 41, Bias: 2218.081102, T: 113007843, Avg. loss: 139131532.509991\n",
      "Total training time: 24.06 seconds.\n",
      "-- Epoch 284\n",
      "Norm: 42092.44, NNZs: 41, Bias: 2218.251373, T: 113407164, Avg. loss: 138684345.234846\n",
      "Total training time: 24.15 seconds.\n",
      "-- Epoch 285\n",
      "Norm: 42010.24, NNZs: 41, Bias: 2218.298961, T: 113806485, Avg. loss: 138239633.761546\n",
      "Total training time: 24.23 seconds.\n",
      "-- Epoch 286\n",
      "Norm: 41931.96, NNZs: 41, Bias: 2218.342825, T: 114205806, Avg. loss: 137797567.035611\n",
      "Total training time: 24.31 seconds.\n",
      "-- Epoch 287\n",
      "Norm: 41843.99, NNZs: 41, Bias: 2218.328886, T: 114605127, Avg. loss: 137357421.329014\n",
      "Total training time: 24.40 seconds.\n",
      "-- Epoch 288\n",
      "Norm: 41762.83, NNZs: 41, Bias: 2218.486442, T: 115004448, Avg. loss: 136921528.672483\n",
      "Total training time: 24.48 seconds.\n",
      "-- Epoch 289\n",
      "Norm: 41673.54, NNZs: 41, Bias: 2218.543719, T: 115403769, Avg. loss: 136487360.857738\n",
      "Total training time: 24.57 seconds.\n",
      "-- Epoch 290\n",
      "Norm: 41589.14, NNZs: 41, Bias: 2218.595809, T: 115803090, Avg. loss: 136056233.839266\n",
      "Total training time: 24.65 seconds.\n",
      "-- Epoch 291\n",
      "Norm: 41504.19, NNZs: 41, Bias: 2218.723831, T: 116202411, Avg. loss: 135628031.579663\n",
      "Total training time: 24.73 seconds.\n",
      "-- Epoch 292\n",
      "Norm: 41424.86, NNZs: 41, Bias: 2218.925487, T: 116601732, Avg. loss: 135203376.102015\n",
      "Total training time: 24.82 seconds.\n",
      "-- Epoch 293\n",
      "Norm: 41342.47, NNZs: 41, Bias: 2218.997014, T: 117001053, Avg. loss: 134781806.765910\n",
      "Total training time: 24.90 seconds.\n",
      "-- Epoch 294\n",
      "Norm: 41265.03, NNZs: 41, Bias: 2219.042001, T: 117400374, Avg. loss: 134362612.948085\n",
      "Total training time: 24.99 seconds.\n",
      "-- Epoch 295\n",
      "Norm: 41184.07, NNZs: 41, Bias: 2219.118307, T: 117799695, Avg. loss: 133945144.403377\n",
      "Total training time: 25.07 seconds.\n",
      "-- Epoch 296\n",
      "Norm: 41103.79, NNZs: 41, Bias: 2219.200414, T: 118199016, Avg. loss: 133531674.177678\n",
      "Total training time: 25.15 seconds.\n",
      "-- Epoch 297\n",
      "Norm: 41029.83, NNZs: 41, Bias: 2219.403385, T: 118598337, Avg. loss: 133121010.036342\n",
      "Total training time: 25.24 seconds.\n",
      "-- Epoch 298\n",
      "Norm: 40950.33, NNZs: 41, Bias: 2219.518257, T: 118997658, Avg. loss: 132711240.350265\n",
      "Total training time: 25.32 seconds.\n",
      "-- Epoch 299\n",
      "Norm: 40874.29, NNZs: 41, Bias: 2219.545252, T: 119396979, Avg. loss: 132305803.769706\n",
      "Total training time: 25.41 seconds.\n",
      "-- Epoch 300\n",
      "Norm: 40792.60, NNZs: 41, Bias: 2219.569219, T: 119796300, Avg. loss: 131901554.893423\n",
      "Total training time: 25.49 seconds.\n",
      "-- Epoch 301\n",
      "Norm: 40721.31, NNZs: 41, Bias: 2219.714214, T: 120195621, Avg. loss: 131500834.526330\n",
      "Total training time: 25.58 seconds.\n",
      "-- Epoch 302\n",
      "Norm: 40646.87, NNZs: 41, Bias: 2219.740703, T: 120594942, Avg. loss: 131102884.856488\n",
      "Total training time: 25.66 seconds.\n",
      "-- Epoch 303\n",
      "Norm: 40571.06, NNZs: 41, Bias: 2219.784429, T: 120994263, Avg. loss: 130707046.571532\n",
      "Total training time: 25.75 seconds.\n",
      "-- Epoch 304\n",
      "Norm: 40503.47, NNZs: 41, Bias: 2219.936668, T: 121393584, Avg. loss: 130315573.179949\n",
      "Total training time: 25.84 seconds.\n",
      "-- Epoch 305\n",
      "Norm: 40428.51, NNZs: 41, Bias: 2220.011013, T: 121792905, Avg. loss: 129923887.013736\n",
      "Total training time: 25.93 seconds.\n",
      "-- Epoch 306\n",
      "Norm: 40355.16, NNZs: 41, Bias: 2220.145470, T: 122192226, Avg. loss: 129536513.293777\n",
      "Total training time: 26.01 seconds.\n",
      "-- Epoch 307\n",
      "Norm: 40288.80, NNZs: 41, Bias: 2220.341595, T: 122591547, Avg. loss: 129151181.568692\n",
      "Total training time: 26.10 seconds.\n",
      "-- Epoch 308\n",
      "Norm: 40218.92, NNZs: 41, Bias: 2220.448178, T: 122990868, Avg. loss: 128768204.131986\n",
      "Total training time: 26.18 seconds.\n",
      "-- Epoch 309\n",
      "Norm: 40143.12, NNZs: 41, Bias: 2220.534915, T: 123390189, Avg. loss: 128386879.770001\n",
      "Total training time: 26.27 seconds.\n",
      "-- Epoch 310\n",
      "Norm: 40074.10, NNZs: 41, Bias: 2220.714783, T: 123789510, Avg. loss: 128008425.441536\n",
      "Total training time: 26.36 seconds.\n",
      "-- Epoch 311\n",
      "Norm: 40001.27, NNZs: 41, Bias: 2220.814398, T: 124188831, Avg. loss: 127632097.037195\n",
      "Total training time: 26.44 seconds.\n",
      "-- Epoch 312\n",
      "Norm: 39931.99, NNZs: 41, Bias: 2220.799585, T: 124588152, Avg. loss: 127257370.104814\n",
      "Total training time: 26.53 seconds.\n",
      "-- Epoch 313\n",
      "Norm: 39856.41, NNZs: 41, Bias: 2220.874561, T: 124987473, Avg. loss: 126884558.732584\n",
      "Total training time: 26.61 seconds.\n",
      "-- Epoch 314\n",
      "Norm: 39789.80, NNZs: 41, Bias: 2221.067590, T: 125386794, Avg. loss: 126515749.365098\n",
      "Total training time: 26.70 seconds.\n",
      "-- Epoch 315\n",
      "Norm: 39722.32, NNZs: 41, Bias: 2221.240873, T: 125786115, Avg. loss: 126148854.301322\n",
      "Total training time: 26.79 seconds.\n",
      "-- Epoch 316\n",
      "Norm: 39654.17, NNZs: 41, Bias: 2221.380813, T: 126185436, Avg. loss: 125784310.819811\n",
      "Total training time: 26.87 seconds.\n",
      "-- Epoch 317\n",
      "Norm: 39577.62, NNZs: 41, Bias: 2221.333719, T: 126584757, Avg. loss: 125419709.522850\n",
      "Total training time: 26.96 seconds.\n",
      "-- Epoch 318\n",
      "Norm: 39509.73, NNZs: 41, Bias: 2221.473194, T: 126984078, Avg. loss: 125059048.988447\n",
      "Total training time: 27.05 seconds.\n",
      "-- Epoch 319\n",
      "Norm: 39435.90, NNZs: 41, Bias: 2221.547069, T: 127383399, Avg. loss: 124700448.263974\n",
      "Total training time: 27.13 seconds.\n",
      "-- Epoch 320\n",
      "Norm: 39370.34, NNZs: 41, Bias: 2221.758326, T: 127782720, Avg. loss: 124344870.961338\n",
      "Total training time: 27.22 seconds.\n",
      "-- Epoch 321\n",
      "Norm: 39298.87, NNZs: 41, Bias: 2221.858605, T: 128182041, Avg. loss: 123990427.410345\n",
      "Total training time: 27.31 seconds.\n",
      "-- Epoch 322\n",
      "Norm: 39234.41, NNZs: 41, Bias: 2222.074473, T: 128581362, Avg. loss: 123638292.900348\n",
      "Total training time: 27.39 seconds.\n",
      "-- Epoch 323\n",
      "Norm: 39168.63, NNZs: 41, Bias: 2222.143321, T: 128980683, Avg. loss: 123288243.165854\n",
      "Total training time: 27.48 seconds.\n",
      "-- Epoch 324\n",
      "Norm: 39111.68, NNZs: 41, Bias: 2222.317905, T: 129380004, Avg. loss: 122941527.762292\n",
      "Total training time: 27.57 seconds.\n",
      "-- Epoch 325\n",
      "Norm: 39043.21, NNZs: 41, Bias: 2222.458312, T: 129779325, Avg. loss: 122595841.165724\n",
      "Total training time: 27.66 seconds.\n",
      "-- Epoch 326\n",
      "Norm: 38975.58, NNZs: 41, Bias: 2222.562940, T: 130178646, Avg. loss: 122251347.112187\n",
      "Total training time: 27.74 seconds.\n",
      "-- Epoch 327\n",
      "Norm: 38913.96, NNZs: 41, Bias: 2222.658273, T: 130577967, Avg. loss: 121909420.631884\n",
      "Total training time: 27.83 seconds.\n",
      "-- Epoch 328\n",
      "Norm: 38844.21, NNZs: 41, Bias: 2222.749609, T: 130977288, Avg. loss: 121568803.837798\n",
      "Total training time: 27.91 seconds.\n",
      "-- Epoch 329\n",
      "Norm: 38778.19, NNZs: 41, Bias: 2222.817049, T: 131376609, Avg. loss: 121230666.949341\n",
      "Total training time: 28.00 seconds.\n",
      "-- Epoch 330\n",
      "Norm: 38711.68, NNZs: 41, Bias: 2222.920656, T: 131775930, Avg. loss: 120894065.426650\n",
      "Total training time: 28.08 seconds.\n",
      "-- Epoch 331\n",
      "Norm: 38648.52, NNZs: 41, Bias: 2223.060719, T: 132175251, Avg. loss: 120560622.221299\n",
      "Total training time: 28.16 seconds.\n",
      "-- Epoch 332\n",
      "Norm: 38590.17, NNZs: 41, Bias: 2223.249952, T: 132574572, Avg. loss: 120228066.627669\n",
      "Total training time: 28.25 seconds.\n",
      "-- Epoch 333\n",
      "Norm: 38521.53, NNZs: 41, Bias: 2223.325914, T: 132973893, Avg. loss: 119896879.966258\n",
      "Total training time: 28.33 seconds.\n",
      "-- Epoch 334\n",
      "Norm: 38455.57, NNZs: 41, Bias: 2223.448315, T: 133373214, Avg. loss: 119568949.465328\n",
      "Total training time: 28.42 seconds.\n",
      "-- Epoch 335\n",
      "Norm: 38398.71, NNZs: 41, Bias: 2223.529816, T: 133772535, Avg. loss: 119242976.835201\n",
      "Total training time: 28.50 seconds.\n",
      "-- Epoch 336\n",
      "Norm: 38335.62, NNZs: 41, Bias: 2223.664834, T: 134171856, Avg. loss: 118918882.728584\n",
      "Total training time: 28.58 seconds.\n",
      "-- Epoch 337\n",
      "Norm: 38271.88, NNZs: 41, Bias: 2223.823818, T: 134571177, Avg. loss: 118596315.014538\n",
      "Total training time: 28.67 seconds.\n",
      "-- Epoch 338\n",
      "Norm: 38208.16, NNZs: 41, Bias: 2223.952038, T: 134970498, Avg. loss: 118275691.141471\n",
      "Total training time: 28.75 seconds.\n",
      "-- Epoch 339\n",
      "Norm: 38142.98, NNZs: 41, Bias: 2224.087893, T: 135369819, Avg. loss: 117955801.018143\n",
      "Total training time: 28.83 seconds.\n",
      "-- Epoch 340\n",
      "Norm: 38082.21, NNZs: 41, Bias: 2224.213529, T: 135769140, Avg. loss: 117638517.928770\n",
      "Total training time: 28.92 seconds.\n",
      "-- Epoch 341\n",
      "Norm: 38023.90, NNZs: 41, Bias: 2224.389962, T: 136168461, Avg. loss: 117324216.136501\n",
      "Total training time: 29.00 seconds.\n",
      "-- Epoch 342\n",
      "Norm: 37961.68, NNZs: 41, Bias: 2224.445623, T: 136567782, Avg. loss: 117010559.251291\n",
      "Total training time: 29.09 seconds.\n",
      "-- Epoch 343\n",
      "Norm: 37905.00, NNZs: 41, Bias: 2224.594425, T: 136967103, Avg. loss: 116698821.091359\n",
      "Total training time: 29.17 seconds.\n",
      "-- Epoch 344\n",
      "Norm: 37839.46, NNZs: 41, Bias: 2224.698575, T: 137366424, Avg. loss: 116388710.936526\n",
      "Total training time: 29.26 seconds.\n",
      "-- Epoch 345\n",
      "Norm: 37778.04, NNZs: 41, Bias: 2224.778016, T: 137765745, Avg. loss: 116080010.822674\n",
      "Total training time: 29.34 seconds.\n",
      "-- Epoch 346\n",
      "Norm: 37721.81, NNZs: 41, Bias: 2224.843231, T: 138165066, Avg. loss: 115773090.823284\n",
      "Total training time: 29.42 seconds.\n",
      "-- Epoch 347\n",
      "Norm: 37662.76, NNZs: 41, Bias: 2225.014665, T: 138564387, Avg. loss: 115467465.719045\n",
      "Total training time: 29.51 seconds.\n",
      "-- Epoch 348\n",
      "Norm: 37604.43, NNZs: 41, Bias: 2225.098196, T: 138963708, Avg. loss: 115164424.043169\n",
      "Total training time: 29.59 seconds.\n",
      "-- Epoch 349\n",
      "Norm: 37535.24, NNZs: 41, Bias: 2225.184128, T: 139363029, Avg. loss: 114861322.657119\n",
      "Total training time: 29.68 seconds.\n",
      "-- Epoch 350\n",
      "Norm: 37473.54, NNZs: 41, Bias: 2225.260327, T: 139762350, Avg. loss: 114561157.469937\n",
      "Total training time: 29.76 seconds.\n",
      "-- Epoch 351\n",
      "Norm: 37411.52, NNZs: 41, Bias: 2225.331380, T: 140161671, Avg. loss: 114261883.453444\n",
      "Total training time: 29.84 seconds.\n",
      "-- Epoch 352\n",
      "Norm: 37351.77, NNZs: 41, Bias: 2225.460408, T: 140560992, Avg. loss: 113963867.725884\n",
      "Total training time: 29.93 seconds.\n",
      "-- Epoch 353\n",
      "Norm: 37297.49, NNZs: 41, Bias: 2225.610788, T: 140960313, Avg. loss: 113668163.908628\n",
      "Total training time: 30.01 seconds.\n",
      "-- Epoch 354\n",
      "Norm: 37235.71, NNZs: 41, Bias: 2225.661826, T: 141359634, Avg. loss: 113374026.260203\n",
      "Total training time: 30.10 seconds.\n",
      "-- Epoch 355\n",
      "Norm: 37178.24, NNZs: 41, Bias: 2225.824704, T: 141758955, Avg. loss: 113082450.294798\n",
      "Total training time: 30.18 seconds.\n",
      "-- Epoch 356\n",
      "Norm: 37123.30, NNZs: 41, Bias: 2225.946725, T: 142158276, Avg. loss: 112791861.937952\n",
      "Total training time: 30.26 seconds.\n",
      "-- Epoch 357\n",
      "Norm: 37067.56, NNZs: 41, Bias: 2226.063233, T: 142557597, Avg. loss: 112502719.120723\n",
      "Total training time: 30.35 seconds.\n",
      "-- Epoch 358\n",
      "Norm: 37012.06, NNZs: 41, Bias: 2226.196609, T: 142956918, Avg. loss: 112214540.515220\n",
      "Total training time: 30.43 seconds.\n",
      "-- Epoch 359\n",
      "Norm: 36954.69, NNZs: 41, Bias: 2226.261688, T: 143356239, Avg. loss: 111928160.441613\n",
      "Total training time: 30.51 seconds.\n",
      "-- Epoch 360\n",
      "Norm: 36896.38, NNZs: 41, Bias: 2226.334168, T: 143755560, Avg. loss: 111643581.560483\n",
      "Total training time: 30.60 seconds.\n",
      "-- Epoch 361\n",
      "Norm: 36841.74, NNZs: 41, Bias: 2226.508480, T: 144154881, Avg. loss: 111360730.666115\n",
      "Total training time: 30.68 seconds.\n",
      "-- Epoch 362\n",
      "Norm: 36787.62, NNZs: 41, Bias: 2226.662971, T: 144554202, Avg. loss: 111080006.925456\n",
      "Total training time: 30.77 seconds.\n",
      "-- Epoch 363\n",
      "Norm: 36730.43, NNZs: 41, Bias: 2226.744429, T: 144953523, Avg. loss: 110799229.601416\n",
      "Total training time: 30.85 seconds.\n",
      "-- Epoch 364\n",
      "Norm: 36673.22, NNZs: 41, Bias: 2226.748068, T: 145352844, Avg. loss: 110520443.662713\n",
      "Total training time: 30.94 seconds.\n",
      "-- Epoch 365\n",
      "Norm: 36616.05, NNZs: 41, Bias: 2226.877308, T: 145752165, Avg. loss: 110242450.573965\n",
      "Total training time: 31.02 seconds.\n",
      "-- Epoch 366\n",
      "Norm: 36560.84, NNZs: 41, Bias: 2226.980483, T: 146151486, Avg. loss: 109966795.456011\n",
      "Total training time: 31.10 seconds.\n",
      "-- Epoch 367\n",
      "Norm: 36505.94, NNZs: 41, Bias: 2227.030558, T: 146550807, Avg. loss: 109692196.087828\n",
      "Total training time: 31.19 seconds.\n",
      "-- Epoch 368\n",
      "Norm: 36452.33, NNZs: 41, Bias: 2227.171541, T: 146950128, Avg. loss: 109419603.609941\n",
      "Total training time: 31.28 seconds.\n",
      "-- Epoch 369\n",
      "Norm: 36397.89, NNZs: 41, Bias: 2227.307473, T: 147349449, Avg. loss: 109148349.998558\n",
      "Total training time: 31.36 seconds.\n",
      "-- Epoch 370\n",
      "Norm: 36339.50, NNZs: 41, Bias: 2227.415903, T: 147748770, Avg. loss: 108878319.445170\n",
      "Total training time: 31.45 seconds.\n",
      "-- Epoch 371\n",
      "Norm: 36287.57, NNZs: 41, Bias: 2227.490256, T: 148148091, Avg. loss: 108609742.748347\n",
      "Total training time: 31.53 seconds.\n",
      "-- Epoch 372\n",
      "Norm: 36234.13, NNZs: 41, Bias: 2227.613784, T: 148547412, Avg. loss: 108342363.246849\n",
      "Total training time: 31.61 seconds.\n",
      "-- Epoch 373\n",
      "Norm: 36176.74, NNZs: 41, Bias: 2227.610601, T: 148946733, Avg. loss: 108075628.228828\n",
      "Total training time: 31.70 seconds.\n",
      "-- Epoch 374\n",
      "Norm: 36124.94, NNZs: 41, Bias: 2227.797317, T: 149346054, Avg. loss: 107811756.467850\n",
      "Total training time: 31.78 seconds.\n",
      "-- Epoch 375\n",
      "Norm: 36089.88, NNZs: 41, Bias: 2227.880476, T: 149745375, Avg. loss: 107548340.527186\n",
      "Total training time: 31.87 seconds.\n",
      "-- Epoch 376\n",
      "Norm: 36018.15, NNZs: 41, Bias: 2227.974943, T: 150144696, Avg. loss: 107286478.961690\n",
      "Total training time: 31.95 seconds.\n",
      "-- Epoch 377\n",
      "Norm: 35959.93, NNZs: 41, Bias: 2228.056037, T: 150544017, Avg. loss: 107025556.687739\n",
      "Total training time: 32.04 seconds.\n",
      "-- Epoch 378\n",
      "Norm: 35908.42, NNZs: 41, Bias: 2228.243338, T: 150943338, Avg. loss: 106766908.755165\n",
      "Total training time: 32.12 seconds.\n",
      "-- Epoch 379\n",
      "Norm: 35858.86, NNZs: 41, Bias: 2228.338567, T: 151342659, Avg. loss: 106509463.709957\n",
      "Total training time: 32.20 seconds.\n",
      "-- Epoch 380\n",
      "Norm: 35808.51, NNZs: 41, Bias: 2228.439173, T: 151741980, Avg. loss: 106253256.145341\n",
      "Total training time: 32.29 seconds.\n",
      "-- Epoch 381\n",
      "Norm: 35757.99, NNZs: 41, Bias: 2228.634313, T: 152141301, Avg. loss: 105998018.173349\n",
      "Total training time: 32.37 seconds.\n",
      "-- Epoch 382\n",
      "Norm: 35707.79, NNZs: 41, Bias: 2228.793155, T: 152540622, Avg. loss: 105744574.788373\n",
      "Total training time: 32.46 seconds.\n",
      "-- Epoch 383\n",
      "Norm: 35657.61, NNZs: 41, Bias: 2228.865377, T: 152939943, Avg. loss: 105491882.475349\n",
      "Total training time: 32.54 seconds.\n",
      "-- Epoch 384\n",
      "Norm: 35600.33, NNZs: 41, Bias: 2228.873466, T: 153339264, Avg. loss: 105240067.322171\n",
      "Total training time: 32.63 seconds.\n",
      "-- Epoch 385\n",
      "Norm: 35552.41, NNZs: 41, Bias: 2229.082093, T: 153738585, Avg. loss: 104989894.595862\n",
      "Total training time: 32.71 seconds.\n",
      "-- Epoch 386\n",
      "Norm: 35502.81, NNZs: 41, Bias: 2229.181015, T: 154137906, Avg. loss: 104740364.354297\n",
      "Total training time: 32.79 seconds.\n",
      "-- Epoch 387\n",
      "Norm: 35451.79, NNZs: 41, Bias: 2229.195801, T: 154537227, Avg. loss: 104492434.693573\n",
      "Total training time: 32.88 seconds.\n",
      "-- Epoch 388\n",
      "Norm: 35404.40, NNZs: 41, Bias: 2229.343665, T: 154936548, Avg. loss: 104246061.602450\n",
      "Total training time: 32.96 seconds.\n",
      "-- Epoch 389\n",
      "Norm: 35354.87, NNZs: 41, Bias: 2229.415326, T: 155335869, Avg. loss: 104000871.206808\n",
      "Total training time: 33.05 seconds.\n",
      "-- Epoch 390\n",
      "Norm: 35303.70, NNZs: 41, Bias: 2229.493518, T: 155735190, Avg. loss: 103756563.826285\n",
      "Total training time: 33.13 seconds.\n",
      "-- Epoch 391\n",
      "Norm: 35256.50, NNZs: 41, Bias: 2229.582064, T: 156134511, Avg. loss: 103513508.693854\n",
      "Total training time: 33.22 seconds.\n",
      "-- Epoch 392\n",
      "Norm: 35205.98, NNZs: 41, Bias: 2229.678807, T: 156533832, Avg. loss: 103271515.978389\n",
      "Total training time: 33.30 seconds.\n",
      "-- Epoch 393\n",
      "Norm: 35156.87, NNZs: 41, Bias: 2229.770108, T: 156933153, Avg. loss: 103031140.398352\n",
      "Total training time: 33.38 seconds.\n",
      "-- Epoch 394\n",
      "Norm: 35108.33, NNZs: 41, Bias: 2229.830682, T: 157332474, Avg. loss: 102791148.532490\n",
      "Total training time: 33.47 seconds.\n",
      "-- Epoch 395\n",
      "Norm: 35064.63, NNZs: 41, Bias: 2229.997965, T: 157731795, Avg. loss: 102553375.886307\n",
      "Total training time: 33.55 seconds.\n",
      "-- Epoch 396\n",
      "Norm: 35017.32, NNZs: 41, Bias: 2230.076794, T: 158131116, Avg. loss: 102315887.465600\n",
      "Total training time: 33.64 seconds.\n",
      "-- Epoch 397\n",
      "Norm: 34968.65, NNZs: 41, Bias: 2230.194784, T: 158530437, Avg. loss: 102079830.464374\n",
      "Total training time: 33.72 seconds.\n",
      "-- Epoch 398\n",
      "Norm: 34922.48, NNZs: 41, Bias: 2230.374015, T: 158929758, Avg. loss: 101845483.184467\n",
      "Total training time: 33.81 seconds.\n",
      "-- Epoch 399\n",
      "Norm: 34874.06, NNZs: 41, Bias: 2230.389879, T: 159329079, Avg. loss: 101610748.895220\n",
      "Total training time: 33.89 seconds.\n",
      "-- Epoch 400\n",
      "Norm: 34828.68, NNZs: 41, Bias: 2230.501715, T: 159728400, Avg. loss: 101378420.516831\n",
      "Total training time: 33.98 seconds.\n",
      "-- Epoch 401\n",
      "Norm: 34781.72, NNZs: 41, Bias: 2230.576519, T: 160127721, Avg. loss: 101147189.309410\n",
      "Total training time: 34.06 seconds.\n",
      "-- Epoch 402\n",
      "Norm: 34748.68, NNZs: 41, Bias: 2230.570997, T: 160527042, Avg. loss: 100916421.940859\n",
      "Total training time: 34.15 seconds.\n",
      "-- Epoch 403\n",
      "Norm: 34684.68, NNZs: 41, Bias: 2230.627789, T: 160926363, Avg. loss: 100686688.277070\n",
      "Total training time: 34.23 seconds.\n",
      "-- Epoch 404\n",
      "Norm: 34636.57, NNZs: 41, Bias: 2230.695526, T: 161325684, Avg. loss: 100458465.276964\n",
      "Total training time: 34.32 seconds.\n",
      "-- Epoch 405\n",
      "Norm: 34589.29, NNZs: 41, Bias: 2230.811356, T: 161725005, Avg. loss: 100230903.914199\n",
      "Total training time: 34.40 seconds.\n",
      "-- Epoch 406\n",
      "Norm: 34542.53, NNZs: 41, Bias: 2230.898055, T: 162124326, Avg. loss: 100004639.625692\n",
      "Total training time: 34.48 seconds.\n",
      "-- Epoch 407\n",
      "Norm: 34498.13, NNZs: 41, Bias: 2230.992470, T: 162523647, Avg. loss: 99779386.592569\n",
      "Total training time: 34.57 seconds.\n",
      "-- Epoch 408\n",
      "Norm: 34453.47, NNZs: 41, Bias: 2231.131429, T: 162922968, Avg. loss: 99555097.510153\n",
      "Total training time: 34.65 seconds.\n",
      "-- Epoch 409\n",
      "Norm: 34406.97, NNZs: 41, Bias: 2231.219910, T: 163322289, Avg. loss: 99331803.540294\n",
      "Total training time: 34.74 seconds.\n",
      "-- Epoch 410\n",
      "Norm: 34360.07, NNZs: 41, Bias: 2231.286044, T: 163721610, Avg. loss: 99109450.980531\n",
      "Total training time: 34.82 seconds.\n",
      "-- Epoch 411\n",
      "Norm: 34313.64, NNZs: 41, Bias: 2231.393398, T: 164120931, Avg. loss: 98888401.517714\n",
      "Total training time: 34.91 seconds.\n",
      "-- Epoch 412\n",
      "Norm: 34268.08, NNZs: 41, Bias: 2231.467930, T: 164520252, Avg. loss: 98667709.592228\n",
      "Total training time: 34.99 seconds.\n",
      "-- Epoch 413\n",
      "Norm: 34225.59, NNZs: 41, Bias: 2231.582045, T: 164919573, Avg. loss: 98448943.059651\n",
      "Total training time: 35.07 seconds.\n",
      "-- Epoch 414\n",
      "Norm: 34181.25, NNZs: 41, Bias: 2231.687878, T: 165318894, Avg. loss: 98230979.689484\n",
      "Total training time: 35.16 seconds.\n",
      "-- Epoch 415\n",
      "Norm: 34136.86, NNZs: 41, Bias: 2231.816903, T: 165718215, Avg. loss: 98014102.785050\n",
      "Total training time: 35.24 seconds.\n",
      "-- Epoch 416\n",
      "Norm: 34094.58, NNZs: 41, Bias: 2231.936958, T: 166117536, Avg. loss: 97798644.230186\n",
      "Total training time: 35.33 seconds.\n",
      "-- Epoch 417\n",
      "Norm: 34049.11, NNZs: 41, Bias: 2232.069032, T: 166516857, Avg. loss: 97583963.592034\n",
      "Total training time: 35.41 seconds.\n",
      "-- Epoch 418\n",
      "Norm: 34002.43, NNZs: 41, Bias: 2232.116068, T: 166916178, Avg. loss: 97369634.731438\n",
      "Total training time: 35.50 seconds.\n",
      "-- Epoch 419\n",
      "Norm: 33957.47, NNZs: 41, Bias: 2232.157459, T: 167315499, Avg. loss: 97156415.615134\n",
      "Total training time: 35.58 seconds.\n",
      "-- Epoch 420\n",
      "Norm: 33913.76, NNZs: 41, Bias: 2232.278970, T: 167714820, Avg. loss: 96943843.831098\n",
      "Total training time: 35.67 seconds.\n",
      "-- Epoch 421\n",
      "Norm: 33873.52, NNZs: 41, Bias: 2232.353056, T: 168114141, Avg. loss: 96733182.842305\n",
      "Total training time: 35.75 seconds.\n",
      "-- Epoch 422\n",
      "Norm: 33829.75, NNZs: 41, Bias: 2232.422213, T: 168513462, Avg. loss: 96523415.143098\n",
      "Total training time: 35.84 seconds.\n",
      "-- Epoch 423\n",
      "Norm: 33784.97, NNZs: 41, Bias: 2232.529115, T: 168912783, Avg. loss: 96314233.474607\n",
      "Total training time: 35.92 seconds.\n",
      "-- Epoch 424\n",
      "Norm: 33739.64, NNZs: 41, Bias: 2232.619661, T: 169312104, Avg. loss: 96105609.121159\n",
      "Total training time: 36.00 seconds.\n",
      "-- Epoch 425\n",
      "Norm: 33696.73, NNZs: 41, Bias: 2232.729855, T: 169711425, Avg. loss: 95898254.332791\n",
      "Total training time: 36.09 seconds.\n",
      "-- Epoch 426\n",
      "Norm: 33655.08, NNZs: 41, Bias: 2232.831297, T: 170110746, Avg. loss: 95692123.350223\n",
      "Total training time: 36.17 seconds.\n",
      "-- Epoch 427\n",
      "Norm: 33612.99, NNZs: 41, Bias: 2232.977636, T: 170510067, Avg. loss: 95486545.437219\n",
      "Total training time: 36.26 seconds.\n",
      "-- Epoch 428\n",
      "Norm: 33570.00, NNZs: 41, Bias: 2233.033419, T: 170909388, Avg. loss: 95281728.461625\n",
      "Total training time: 36.34 seconds.\n",
      "-- Epoch 429\n",
      "Norm: 33529.20, NNZs: 41, Bias: 2233.157102, T: 171308709, Avg. loss: 95078579.234792\n",
      "Total training time: 36.43 seconds.\n",
      "-- Epoch 430\n",
      "Norm: 33491.93, NNZs: 41, Bias: 2233.239633, T: 171708030, Avg. loss: 94876304.356839\n",
      "Total training time: 36.51 seconds.\n",
      "-- Epoch 431\n",
      "Norm: 33452.12, NNZs: 41, Bias: 2233.284292, T: 172107351, Avg. loss: 94674257.908009\n",
      "Total training time: 36.59 seconds.\n",
      "-- Epoch 432\n",
      "Norm: 33409.50, NNZs: 41, Bias: 2233.296126, T: 172506672, Avg. loss: 94472974.811332\n",
      "Total training time: 36.68 seconds.\n",
      "-- Epoch 433\n",
      "Norm: 33368.05, NNZs: 41, Bias: 2233.426553, T: 172905993, Avg. loss: 94272814.877075\n",
      "Total training time: 36.77 seconds.\n",
      "-- Epoch 434\n",
      "Norm: 33328.79, NNZs: 41, Bias: 2233.542759, T: 173305314, Avg. loss: 94073405.942575\n",
      "Total training time: 36.85 seconds.\n",
      "-- Epoch 435\n",
      "Norm: 33290.90, NNZs: 41, Bias: 2233.620204, T: 173704635, Avg. loss: 93875452.666418\n",
      "Total training time: 36.94 seconds.\n",
      "-- Epoch 436\n",
      "Norm: 33251.40, NNZs: 41, Bias: 2233.712704, T: 174103956, Avg. loss: 93678006.109484\n",
      "Total training time: 37.02 seconds.\n",
      "-- Epoch 437\n",
      "Norm: 33210.38, NNZs: 41, Bias: 2233.816671, T: 174503277, Avg. loss: 93481189.027535\n",
      "Total training time: 37.11 seconds.\n",
      "-- Epoch 438\n",
      "Norm: 33169.34, NNZs: 41, Bias: 2233.867923, T: 174902598, Avg. loss: 93285298.448198\n",
      "Total training time: 37.19 seconds.\n",
      "-- Epoch 439\n",
      "Norm: 33130.77, NNZs: 41, Bias: 2233.948770, T: 175301919, Avg. loss: 93090736.716605\n",
      "Total training time: 37.28 seconds.\n",
      "-- Epoch 440\n",
      "Norm: 33091.90, NNZs: 41, Bias: 2234.067275, T: 175701240, Avg. loss: 92897086.668857\n",
      "Total training time: 37.36 seconds.\n",
      "-- Epoch 441\n",
      "Norm: 33053.18, NNZs: 41, Bias: 2234.144304, T: 176100561, Avg. loss: 92703976.514132\n",
      "Total training time: 37.44 seconds.\n",
      "-- Epoch 442\n",
      "Norm: 33015.01, NNZs: 41, Bias: 2234.263215, T: 176499882, Avg. loss: 92511683.721211\n",
      "Total training time: 37.53 seconds.\n",
      "-- Epoch 443\n",
      "Norm: 32977.17, NNZs: 41, Bias: 2234.378661, T: 176899203, Avg. loss: 92320541.315326\n",
      "Total training time: 37.61 seconds.\n",
      "-- Epoch 444\n",
      "Norm: 32934.94, NNZs: 41, Bias: 2234.440473, T: 177298524, Avg. loss: 92129762.241635\n",
      "Total training time: 37.70 seconds.\n",
      "-- Epoch 445\n",
      "Norm: 32896.09, NNZs: 41, Bias: 2234.475194, T: 177697845, Avg. loss: 91940203.685767\n",
      "Total training time: 37.78 seconds.\n",
      "-- Epoch 446\n",
      "Norm: 32856.17, NNZs: 41, Bias: 2234.529819, T: 178097166, Avg. loss: 91750854.585075\n",
      "Total training time: 37.87 seconds.\n",
      "-- Epoch 447\n",
      "Norm: 32815.96, NNZs: 41, Bias: 2234.671879, T: 178496487, Avg. loss: 91562972.632462\n",
      "Total training time: 37.95 seconds.\n",
      "-- Epoch 448\n",
      "Norm: 32779.51, NNZs: 41, Bias: 2234.782263, T: 178895808, Avg. loss: 91375581.859406\n",
      "Total training time: 38.04 seconds.\n",
      "-- Epoch 449\n",
      "Norm: 32738.31, NNZs: 41, Bias: 2234.827128, T: 179295129, Avg. loss: 91188632.770446\n",
      "Total training time: 38.12 seconds.\n",
      "-- Epoch 450\n",
      "Norm: 32699.09, NNZs: 41, Bias: 2234.908058, T: 179694450, Avg. loss: 91002927.628270\n",
      "Total training time: 38.21 seconds.\n",
      "-- Epoch 451\n",
      "Norm: 32658.77, NNZs: 41, Bias: 2234.967268, T: 180093771, Avg. loss: 90817999.802088\n",
      "Total training time: 38.30 seconds.\n",
      "-- Epoch 452\n",
      "Norm: 32619.70, NNZs: 41, Bias: 2235.063774, T: 180493092, Avg. loss: 90633783.319095\n",
      "Total training time: 38.39 seconds.\n",
      "-- Epoch 453\n",
      "Norm: 32585.24, NNZs: 41, Bias: 2235.229603, T: 180892413, Avg. loss: 90450569.304800\n",
      "Total training time: 38.48 seconds.\n",
      "-- Epoch 454\n",
      "Norm: 32550.00, NNZs: 41, Bias: 2235.382505, T: 181291734, Avg. loss: 90268199.699994\n",
      "Total training time: 38.56 seconds.\n",
      "-- Epoch 455\n",
      "Norm: 32511.80, NNZs: 41, Bias: 2235.468939, T: 181691055, Avg. loss: 90086227.326702\n",
      "Total training time: 38.64 seconds.\n",
      "-- Epoch 456\n",
      "Norm: 32474.29, NNZs: 41, Bias: 2235.594562, T: 182090376, Avg. loss: 89905338.789615\n",
      "Total training time: 38.73 seconds.\n",
      "-- Epoch 457\n",
      "Norm: 32434.69, NNZs: 41, Bias: 2235.723558, T: 182489697, Avg. loss: 89725018.419580\n",
      "Total training time: 38.81 seconds.\n",
      "-- Epoch 458\n",
      "Norm: 32396.32, NNZs: 41, Bias: 2235.778301, T: 182889018, Avg. loss: 89545207.045552\n",
      "Total training time: 38.89 seconds.\n",
      "-- Epoch 459\n",
      "Norm: 32357.88, NNZs: 41, Bias: 2235.796859, T: 183288339, Avg. loss: 89365620.360458\n",
      "Total training time: 38.97 seconds.\n",
      "-- Epoch 460\n",
      "Norm: 32322.68, NNZs: 41, Bias: 2235.918789, T: 183687660, Avg. loss: 89188043.038473\n",
      "Total training time: 39.06 seconds.\n",
      "-- Epoch 461\n",
      "Norm: 32286.06, NNZs: 41, Bias: 2236.011021, T: 184086981, Avg. loss: 89010420.996818\n",
      "Total training time: 39.14 seconds.\n",
      "-- Epoch 462\n",
      "Norm: 32248.74, NNZs: 41, Bias: 2236.094184, T: 184486302, Avg. loss: 88833867.140116\n",
      "Total training time: 39.22 seconds.\n",
      "-- Epoch 463\n",
      "Norm: 32212.06, NNZs: 41, Bias: 2236.161298, T: 184885623, Avg. loss: 88657437.695810\n",
      "Total training time: 39.30 seconds.\n",
      "-- Epoch 464\n",
      "Norm: 32175.92, NNZs: 41, Bias: 2236.259218, T: 185284944, Avg. loss: 88481951.106497\n",
      "Total training time: 39.39 seconds.\n",
      "-- Epoch 465\n",
      "Norm: 32141.41, NNZs: 41, Bias: 2236.377906, T: 185684265, Avg. loss: 88307666.779707\n",
      "Total training time: 39.47 seconds.\n",
      "-- Epoch 466\n",
      "Norm: 32105.51, NNZs: 41, Bias: 2236.465672, T: 186083586, Avg. loss: 88133822.921514\n",
      "Total training time: 39.55 seconds.\n",
      "-- Epoch 467\n",
      "Norm: 32070.74, NNZs: 41, Bias: 2236.572236, T: 186482907, Avg. loss: 87960710.725269\n",
      "Total training time: 39.63 seconds.\n",
      "-- Epoch 468\n",
      "Norm: 32034.02, NNZs: 41, Bias: 2236.675727, T: 186882228, Avg. loss: 87788404.064904\n",
      "Total training time: 39.72 seconds.\n",
      "-- Epoch 469\n",
      "Norm: 31996.05, NNZs: 41, Bias: 2236.735625, T: 187281549, Avg. loss: 87617188.916972\n",
      "Total training time: 39.80 seconds.\n",
      "-- Epoch 470\n",
      "Norm: 31957.87, NNZs: 41, Bias: 2236.772111, T: 187680870, Avg. loss: 87445754.529285\n",
      "Total training time: 39.88 seconds.\n",
      "-- Epoch 471\n",
      "Norm: 31925.00, NNZs: 41, Bias: 2236.899218, T: 188080191, Avg. loss: 87275958.669667\n",
      "Total training time: 39.96 seconds.\n",
      "-- Epoch 472\n",
      "Norm: 31889.86, NNZs: 41, Bias: 2236.990356, T: 188479512, Avg. loss: 87106556.881105\n",
      "Total training time: 40.04 seconds.\n",
      "-- Epoch 473\n",
      "Norm: 31849.39, NNZs: 41, Bias: 2236.984895, T: 188878833, Avg. loss: 86937053.900706\n",
      "Total training time: 40.13 seconds.\n",
      "-- Epoch 474\n",
      "Norm: 31813.79, NNZs: 41, Bias: 2237.065742, T: 189278154, Avg. loss: 86768785.163536\n",
      "Total training time: 40.21 seconds.\n",
      "-- Epoch 475\n",
      "Norm: 31781.72, NNZs: 41, Bias: 2237.212442, T: 189677475, Avg. loss: 86601212.812366\n",
      "Total training time: 40.29 seconds.\n",
      "-- Epoch 476\n",
      "Norm: 31746.69, NNZs: 41, Bias: 2237.258601, T: 190076796, Avg. loss: 86434706.714142\n",
      "Total training time: 40.37 seconds.\n",
      "-- Epoch 477\n",
      "Norm: 31711.85, NNZs: 41, Bias: 2237.323742, T: 190476117, Avg. loss: 86268698.871542\n",
      "Total training time: 40.45 seconds.\n",
      "-- Epoch 478\n",
      "Norm: 31672.94, NNZs: 41, Bias: 2237.348306, T: 190875438, Avg. loss: 86102877.889784\n",
      "Total training time: 40.54 seconds.\n",
      "-- Epoch 479\n",
      "Norm: 31637.64, NNZs: 41, Bias: 2237.415321, T: 191274759, Avg. loss: 85938002.657158\n",
      "Total training time: 40.62 seconds.\n",
      "-- Epoch 480\n",
      "Norm: 31607.20, NNZs: 41, Bias: 2237.578600, T: 191674080, Avg. loss: 85774216.020748\n",
      "Total training time: 40.70 seconds.\n",
      "-- Epoch 481\n",
      "Norm: 31568.66, NNZs: 41, Bias: 2237.565273, T: 192073401, Avg. loss: 85610169.483117\n",
      "Total training time: 40.78 seconds.\n",
      "-- Epoch 482\n",
      "Norm: 31533.88, NNZs: 41, Bias: 2237.605597, T: 192472722, Avg. loss: 85447220.901447\n",
      "Total training time: 40.87 seconds.\n",
      "-- Epoch 483\n",
      "Norm: 31500.62, NNZs: 41, Bias: 2237.713574, T: 192872043, Avg. loss: 85284886.097586\n",
      "Total training time: 40.95 seconds.\n",
      "-- Epoch 484\n",
      "Norm: 31465.73, NNZs: 41, Bias: 2237.831032, T: 193271364, Avg. loss: 85123100.321275\n",
      "Total training time: 41.03 seconds.\n",
      "-- Epoch 485\n",
      "Norm: 31430.40, NNZs: 41, Bias: 2237.937181, T: 193670685, Avg. loss: 84962252.001658\n",
      "Total training time: 41.11 seconds.\n",
      "-- Epoch 486\n",
      "Norm: 31398.92, NNZs: 41, Bias: 2238.000748, T: 194070006, Avg. loss: 84801932.688281\n",
      "Total training time: 41.19 seconds.\n",
      "-- Epoch 487\n",
      "Norm: 31365.19, NNZs: 41, Bias: 2238.100190, T: 194469327, Avg. loss: 84642166.445680\n",
      "Total training time: 41.28 seconds.\n",
      "-- Epoch 488\n",
      "Norm: 31332.12, NNZs: 41, Bias: 2238.164128, T: 194868648, Avg. loss: 84482994.811520\n",
      "Total training time: 41.36 seconds.\n",
      "-- Epoch 489\n",
      "Norm: 31296.62, NNZs: 41, Bias: 2238.277748, T: 195267969, Avg. loss: 84324731.635853\n",
      "Total training time: 41.44 seconds.\n",
      "-- Epoch 490\n",
      "Norm: 31262.13, NNZs: 41, Bias: 2238.358610, T: 195667290, Avg. loss: 84166813.226395\n",
      "Total training time: 41.52 seconds.\n",
      "-- Epoch 491\n",
      "Norm: 31229.76, NNZs: 41, Bias: 2238.460700, T: 196066611, Avg. loss: 84009982.902866\n",
      "Total training time: 41.60 seconds.\n",
      "-- Epoch 492\n",
      "Norm: 31196.36, NNZs: 41, Bias: 2238.517213, T: 196465932, Avg. loss: 83853117.853862\n",
      "Total training time: 41.69 seconds.\n",
      "-- Epoch 493\n",
      "Norm: 31160.65, NNZs: 41, Bias: 2238.607148, T: 196865253, Avg. loss: 83696889.128367\n",
      "Total training time: 41.77 seconds.\n",
      "-- Epoch 494\n",
      "Norm: 31126.58, NNZs: 41, Bias: 2238.664573, T: 197264574, Avg. loss: 83541086.649669\n",
      "Total training time: 41.85 seconds.\n",
      "-- Epoch 495\n",
      "Norm: 31095.89, NNZs: 41, Bias: 2238.772438, T: 197663895, Avg. loss: 83386424.432440\n",
      "Total training time: 41.93 seconds.\n",
      "-- Epoch 496\n",
      "Norm: 31063.68, NNZs: 41, Bias: 2238.869299, T: 198063216, Avg. loss: 83232061.027272\n",
      "Total training time: 42.01 seconds.\n",
      "-- Epoch 497\n",
      "Norm: 31029.45, NNZs: 41, Bias: 2238.932040, T: 198462537, Avg. loss: 83078087.602932\n",
      "Total training time: 42.09 seconds.\n",
      "-- Epoch 498\n",
      "Norm: 30995.41, NNZs: 41, Bias: 2238.986370, T: 198861858, Avg. loss: 82924860.775490\n",
      "Total training time: 42.18 seconds.\n",
      "-- Epoch 499\n",
      "Norm: 30963.26, NNZs: 41, Bias: 2239.058690, T: 199261179, Avg. loss: 82772353.107940\n",
      "Total training time: 42.26 seconds.\n",
      "-- Epoch 500\n",
      "Norm: 30931.51, NNZs: 41, Bias: 2239.139670, T: 199660500, Avg. loss: 82620197.398861\n",
      "Total training time: 42.34 seconds.\n",
      "-- Epoch 501\n",
      "Norm: 30898.97, NNZs: 41, Bias: 2239.212075, T: 200059821, Avg. loss: 82468600.081969\n",
      "Total training time: 42.43 seconds.\n",
      "-- Epoch 502\n",
      "Norm: 30865.59, NNZs: 41, Bias: 2239.330328, T: 200459142, Avg. loss: 82317648.591081\n",
      "Total training time: 42.51 seconds.\n",
      "-- Epoch 503\n",
      "Norm: 30835.36, NNZs: 41, Bias: 2239.450578, T: 200858463, Avg. loss: 82167740.605931\n",
      "Total training time: 42.60 seconds.\n",
      "-- Epoch 504\n",
      "Norm: 30805.57, NNZs: 41, Bias: 2239.557356, T: 201257784, Avg. loss: 82018125.421052\n",
      "Total training time: 42.68 seconds.\n",
      "-- Epoch 505\n",
      "Norm: 30777.16, NNZs: 41, Bias: 2239.654242, T: 201657105, Avg. loss: 81869167.638374\n",
      "Total training time: 42.76 seconds.\n",
      "-- Epoch 506\n",
      "Norm: 30744.82, NNZs: 41, Bias: 2239.742806, T: 202056426, Avg. loss: 81720531.069713\n",
      "Total training time: 42.85 seconds.\n",
      "-- Epoch 507\n",
      "Norm: 30713.72, NNZs: 41, Bias: 2239.818978, T: 202455747, Avg. loss: 81572541.991642\n",
      "Total training time: 42.93 seconds.\n",
      "-- Epoch 508\n",
      "Norm: 30686.07, NNZs: 41, Bias: 2239.956334, T: 202855068, Avg. loss: 81425482.945571\n",
      "Total training time: 43.01 seconds.\n",
      "-- Epoch 509\n",
      "Norm: 30653.60, NNZs: 41, Bias: 2240.029044, T: 203254389, Avg. loss: 81278504.029359\n",
      "Total training time: 43.09 seconds.\n",
      "-- Epoch 510\n",
      "Norm: 30622.11, NNZs: 41, Bias: 2240.129079, T: 203653710, Avg. loss: 81132091.837978\n",
      "Total training time: 43.17 seconds.\n",
      "-- Epoch 511\n",
      "Norm: 30590.14, NNZs: 41, Bias: 2240.194436, T: 204053031, Avg. loss: 80986140.817173\n",
      "Total training time: 43.26 seconds.\n",
      "-- Epoch 512\n",
      "Norm: 30558.50, NNZs: 41, Bias: 2240.267122, T: 204452352, Avg. loss: 80841094.927712\n",
      "Total training time: 43.34 seconds.\n",
      "-- Epoch 513\n",
      "Norm: 30527.90, NNZs: 41, Bias: 2240.353099, T: 204851673, Avg. loss: 80696339.284993\n",
      "Total training time: 43.42 seconds.\n",
      "-- Epoch 514\n",
      "Norm: 30493.77, NNZs: 41, Bias: 2240.405228, T: 205250994, Avg. loss: 80552015.340704\n",
      "Total training time: 43.51 seconds.\n",
      "-- Epoch 515\n",
      "Norm: 30461.86, NNZs: 41, Bias: 2240.474778, T: 205650315, Avg. loss: 80408188.020738\n",
      "Total training time: 43.59 seconds.\n",
      "-- Epoch 516\n",
      "Norm: 30427.86, NNZs: 41, Bias: 2240.516560, T: 206049636, Avg. loss: 80264813.608245\n",
      "Total training time: 43.67 seconds.\n",
      "-- Epoch 517\n",
      "Norm: 30398.22, NNZs: 41, Bias: 2240.582505, T: 206448957, Avg. loss: 80122432.014998\n",
      "Total training time: 43.76 seconds.\n",
      "-- Epoch 518\n",
      "Norm: 30367.94, NNZs: 41, Bias: 2240.683229, T: 206848278, Avg. loss: 79980399.984330\n",
      "Total training time: 43.84 seconds.\n",
      "-- Epoch 519\n",
      "Norm: 30341.22, NNZs: 41, Bias: 2240.827186, T: 207247599, Avg. loss: 79839102.539347\n",
      "Total training time: 43.92 seconds.\n",
      "-- Epoch 520\n",
      "Norm: 30311.17, NNZs: 41, Bias: 2240.909335, T: 207646920, Avg. loss: 79698208.332155\n",
      "Total training time: 44.01 seconds.\n",
      "-- Epoch 521\n",
      "Norm: 30278.83, NNZs: 41, Bias: 2240.966073, T: 208046241, Avg. loss: 79557389.325168\n",
      "Total training time: 44.09 seconds.\n",
      "-- Epoch 522\n",
      "Norm: 30248.21, NNZs: 41, Bias: 2241.020435, T: 208445562, Avg. loss: 79417537.748467\n",
      "Total training time: 44.17 seconds.\n",
      "-- Epoch 523\n",
      "Norm: 30217.92, NNZs: 41, Bias: 2241.154071, T: 208844883, Avg. loss: 79278173.140014\n",
      "Total training time: 44.25 seconds.\n",
      "-- Epoch 524\n",
      "Norm: 30187.77, NNZs: 41, Bias: 2241.214156, T: 209244204, Avg. loss: 79139142.278617\n",
      "Total training time: 44.33 seconds.\n",
      "-- Epoch 525\n",
      "Norm: 30157.67, NNZs: 41, Bias: 2241.311899, T: 209643525, Avg. loss: 79000604.114725\n",
      "Total training time: 44.42 seconds.\n",
      "-- Epoch 526\n",
      "Norm: 30126.07, NNZs: 41, Bias: 2241.405283, T: 210042846, Avg. loss: 78862401.285453\n",
      "Total training time: 44.50 seconds.\n",
      "-- Epoch 527\n",
      "Norm: 30097.04, NNZs: 41, Bias: 2241.492232, T: 210442167, Avg. loss: 78724921.344696\n",
      "Total training time: 44.58 seconds.\n",
      "-- Epoch 528\n",
      "Norm: 30068.73, NNZs: 41, Bias: 2241.545958, T: 210841488, Avg. loss: 78588201.352473\n",
      "Total training time: 44.66 seconds.\n",
      "-- Epoch 529\n",
      "Norm: 30038.17, NNZs: 41, Bias: 2241.630682, T: 211240809, Avg. loss: 78451653.839516\n",
      "Total training time: 44.75 seconds.\n",
      "-- Epoch 530\n",
      "Norm: 30009.01, NNZs: 41, Bias: 2241.709025, T: 211640130, Avg. loss: 78315736.624645\n",
      "Total training time: 44.83 seconds.\n",
      "-- Epoch 531\n",
      "Norm: 29978.51, NNZs: 41, Bias: 2241.828007, T: 212039451, Avg. loss: 78180415.883142\n",
      "Total training time: 44.91 seconds.\n",
      "-- Epoch 532\n",
      "Norm: 29950.01, NNZs: 41, Bias: 2241.913094, T: 212438772, Avg. loss: 78045487.273190\n",
      "Total training time: 44.99 seconds.\n",
      "-- Epoch 533\n",
      "Norm: 29920.26, NNZs: 41, Bias: 2242.001521, T: 212838093, Avg. loss: 77911184.604538\n",
      "Total training time: 45.07 seconds.\n",
      "-- Epoch 534\n",
      "Norm: 29887.72, NNZs: 41, Bias: 2241.998530, T: 213237414, Avg. loss: 77776962.475554\n",
      "Total training time: 45.16 seconds.\n",
      "-- Epoch 535\n",
      "Norm: 29860.38, NNZs: 41, Bias: 2242.106688, T: 213636735, Avg. loss: 77643449.608768\n",
      "Total training time: 45.24 seconds.\n",
      "-- Epoch 536\n",
      "Norm: 29832.36, NNZs: 41, Bias: 2242.178898, T: 214036056, Avg. loss: 77510383.113470\n",
      "Total training time: 45.32 seconds.\n",
      "-- Epoch 537\n",
      "Norm: 29803.09, NNZs: 41, Bias: 2242.213959, T: 214435377, Avg. loss: 77377766.842553\n",
      "Total training time: 45.40 seconds.\n",
      "-- Epoch 538\n",
      "Norm: 29775.46, NNZs: 41, Bias: 2242.270444, T: 214834698, Avg. loss: 77245703.966931\n",
      "Total training time: 45.48 seconds.\n",
      "-- Epoch 539\n",
      "Norm: 29747.28, NNZs: 41, Bias: 2242.411407, T: 215234019, Avg. loss: 77114211.169832\n",
      "Total training time: 45.57 seconds.\n",
      "-- Epoch 540\n",
      "Norm: 29719.53, NNZs: 41, Bias: 2242.520020, T: 215633340, Avg. loss: 76983028.627714\n",
      "Total training time: 45.65 seconds.\n",
      "-- Epoch 541\n",
      "Norm: 29691.10, NNZs: 41, Bias: 2242.583771, T: 216032661, Avg. loss: 76852295.395335\n",
      "Total training time: 45.73 seconds.\n",
      "-- Epoch 542\n",
      "Norm: 29663.22, NNZs: 41, Bias: 2242.672385, T: 216431982, Avg. loss: 76722096.715521\n",
      "Total training time: 45.81 seconds.\n",
      "-- Epoch 543\n",
      "Norm: 29635.89, NNZs: 41, Bias: 2242.750505, T: 216831303, Avg. loss: 76592279.452048\n",
      "Total training time: 45.89 seconds.\n",
      "-- Epoch 544\n",
      "Norm: 29606.93, NNZs: 41, Bias: 2242.861551, T: 217230624, Avg. loss: 76463197.590208\n",
      "Total training time: 45.97 seconds.\n",
      "-- Epoch 545\n",
      "Norm: 29580.64, NNZs: 41, Bias: 2242.905469, T: 217629945, Avg. loss: 76334373.100653\n",
      "Total training time: 46.06 seconds.\n",
      "-- Epoch 546\n",
      "Norm: 29551.95, NNZs: 41, Bias: 2242.983873, T: 218029266, Avg. loss: 76206168.926683\n",
      "Total training time: 46.14 seconds.\n",
      "-- Epoch 547\n",
      "Norm: 29523.76, NNZs: 41, Bias: 2243.036692, T: 218428587, Avg. loss: 76078248.198370\n",
      "Total training time: 46.22 seconds.\n",
      "-- Epoch 548\n",
      "Norm: 29495.92, NNZs: 41, Bias: 2243.110221, T: 218827908, Avg. loss: 75950842.402994\n",
      "Total training time: 46.30 seconds.\n",
      "-- Epoch 549\n",
      "Norm: 29468.52, NNZs: 41, Bias: 2243.207810, T: 219227229, Avg. loss: 75823805.998628\n",
      "Total training time: 46.38 seconds.\n",
      "-- Epoch 550\n",
      "Norm: 29440.91, NNZs: 41, Bias: 2243.239728, T: 219626550, Avg. loss: 75697121.151308\n",
      "Total training time: 46.47 seconds.\n",
      "-- Epoch 551\n",
      "Norm: 29412.58, NNZs: 41, Bias: 2243.362367, T: 220025871, Avg. loss: 75570847.385876\n",
      "Total training time: 46.55 seconds.\n",
      "-- Epoch 552\n",
      "Norm: 29386.32, NNZs: 41, Bias: 2243.410138, T: 220425192, Avg. loss: 75444869.231890\n",
      "Total training time: 46.63 seconds.\n",
      "-- Epoch 553\n",
      "Norm: 29357.99, NNZs: 41, Bias: 2243.476100, T: 220824513, Avg. loss: 75319358.483030\n",
      "Total training time: 46.71 seconds.\n",
      "-- Epoch 554\n",
      "Norm: 29331.12, NNZs: 41, Bias: 2243.530921, T: 221223834, Avg. loss: 75194117.277234\n",
      "Total training time: 46.80 seconds.\n",
      "-- Epoch 555\n",
      "Norm: 29303.65, NNZs: 41, Bias: 2243.616885, T: 221623155, Avg. loss: 75069500.096017\n",
      "Total training time: 46.88 seconds.\n",
      "-- Epoch 556\n",
      "Norm: 29274.72, NNZs: 41, Bias: 2243.643196, T: 222022476, Avg. loss: 74945133.151399\n",
      "Total training time: 46.96 seconds.\n",
      "-- Epoch 557\n",
      "Norm: 29245.26, NNZs: 41, Bias: 2243.697213, T: 222421797, Avg. loss: 74820993.575770\n",
      "Total training time: 47.04 seconds.\n",
      "-- Epoch 558\n",
      "Norm: 29220.09, NNZs: 41, Bias: 2243.791339, T: 222821118, Avg. loss: 74697735.421375\n",
      "Total training time: 47.12 seconds.\n",
      "-- Epoch 559\n",
      "Norm: 29191.91, NNZs: 41, Bias: 2243.848785, T: 223220439, Avg. loss: 74574873.277885\n",
      "Total training time: 47.21 seconds.\n",
      "-- Epoch 560\n",
      "Norm: 29164.34, NNZs: 41, Bias: 2243.901781, T: 223619760, Avg. loss: 74452721.166381\n",
      "Total training time: 47.29 seconds.\n",
      "-- Epoch 561\n",
      "Norm: 29135.05, NNZs: 41, Bias: 2243.971529, T: 224019081, Avg. loss: 74330528.637476\n",
      "Total training time: 47.37 seconds.\n",
      "-- Epoch 562\n",
      "Norm: 29107.73, NNZs: 41, Bias: 2244.006354, T: 224418402, Avg. loss: 74208827.363431\n",
      "Total training time: 47.45 seconds.\n",
      "-- Epoch 563\n",
      "Norm: 29081.22, NNZs: 41, Bias: 2244.069772, T: 224817723, Avg. loss: 74087732.858123\n",
      "Total training time: 47.53 seconds.\n",
      "-- Epoch 564\n",
      "Norm: 29054.97, NNZs: 41, Bias: 2244.190983, T: 225217044, Avg. loss: 73967094.665975\n",
      "Total training time: 47.61 seconds.\n",
      "-- Epoch 565\n",
      "Norm: 29029.05, NNZs: 41, Bias: 2244.278131, T: 225616365, Avg. loss: 73846880.496277\n",
      "Total training time: 47.70 seconds.\n",
      "-- Epoch 566\n",
      "Norm: 29000.00, NNZs: 41, Bias: 2244.337802, T: 226015686, Avg. loss: 73726566.466878\n",
      "Total training time: 47.78 seconds.\n",
      "-- Epoch 567\n",
      "Norm: 28970.44, NNZs: 41, Bias: 2244.365210, T: 226415007, Avg. loss: 73607083.064092\n",
      "Total training time: 47.86 seconds.\n",
      "-- Epoch 568\n",
      "Norm: 28943.31, NNZs: 41, Bias: 2244.423467, T: 226814328, Avg. loss: 73488034.706730\n",
      "Total training time: 47.94 seconds.\n",
      "-- Epoch 569\n",
      "Norm: 28916.66, NNZs: 41, Bias: 2244.501698, T: 227213649, Avg. loss: 73369344.859368\n",
      "Total training time: 48.02 seconds.\n",
      "-- Epoch 570\n",
      "Norm: 28889.75, NNZs: 41, Bias: 2244.559586, T: 227612970, Avg. loss: 73250895.830449\n",
      "Total training time: 48.11 seconds.\n",
      "-- Epoch 571\n",
      "Norm: 28865.18, NNZs: 41, Bias: 2244.612092, T: 228012291, Avg. loss: 73133189.555317\n",
      "Total training time: 48.19 seconds.\n",
      "-- Epoch 572\n",
      "Norm: 28838.46, NNZs: 41, Bias: 2244.701347, T: 228411612, Avg. loss: 73015675.572752\n",
      "Total training time: 48.27 seconds.\n",
      "-- Epoch 573\n",
      "Norm: 28813.21, NNZs: 41, Bias: 2244.799326, T: 228810933, Avg. loss: 72898530.167208\n",
      "Total training time: 48.35 seconds.\n",
      "-- Epoch 574\n",
      "Norm: 28790.40, NNZs: 41, Bias: 2244.938714, T: 229210254, Avg. loss: 72782257.473034\n",
      "Total training time: 48.43 seconds.\n",
      "-- Epoch 575\n",
      "Norm: 28763.93, NNZs: 41, Bias: 2244.998163, T: 229609575, Avg. loss: 72665843.299860\n",
      "Total training time: 48.52 seconds.\n",
      "-- Epoch 576\n",
      "Norm: 28736.53, NNZs: 41, Bias: 2245.017651, T: 230008896, Avg. loss: 72549704.832223\n",
      "Total training time: 48.60 seconds.\n",
      "-- Epoch 577\n",
      "Norm: 28708.76, NNZs: 41, Bias: 2245.021486, T: 230408217, Avg. loss: 72434118.782410\n",
      "Total training time: 48.68 seconds.\n",
      "-- Epoch 578\n",
      "Norm: 28683.53, NNZs: 41, Bias: 2245.103935, T: 230807538, Avg. loss: 72318874.882136\n",
      "Total training time: 48.76 seconds.\n",
      "-- Epoch 579\n",
      "Norm: 28659.38, NNZs: 41, Bias: 2245.188622, T: 231206859, Avg. loss: 72204202.475451\n",
      "Total training time: 48.85 seconds.\n",
      "-- Epoch 580\n",
      "Norm: 28633.23, NNZs: 41, Bias: 2245.251891, T: 231606180, Avg. loss: 72089913.521387\n",
      "Total training time: 48.93 seconds.\n",
      "-- Epoch 581\n",
      "Norm: 28605.48, NNZs: 41, Bias: 2245.301976, T: 232005501, Avg. loss: 71975546.916274\n",
      "Total training time: 49.01 seconds.\n",
      "-- Epoch 582\n",
      "Norm: 28578.89, NNZs: 41, Bias: 2245.362644, T: 232404822, Avg. loss: 71861623.660107\n",
      "Total training time: 49.09 seconds.\n",
      "-- Epoch 583\n",
      "Norm: 28554.62, NNZs: 41, Bias: 2245.500211, T: 232804143, Avg. loss: 71748657.408202\n",
      "Total training time: 49.18 seconds.\n",
      "-- Epoch 584\n",
      "Norm: 28531.03, NNZs: 41, Bias: 2245.585030, T: 233203464, Avg. loss: 71635487.668339\n",
      "Total training time: 49.26 seconds.\n",
      "-- Epoch 585\n",
      "Norm: 28507.53, NNZs: 41, Bias: 2245.653254, T: 233602785, Avg. loss: 71523029.308684\n",
      "Total training time: 49.34 seconds.\n",
      "-- Epoch 586\n",
      "Norm: 28482.55, NNZs: 41, Bias: 2245.710624, T: 234002106, Avg. loss: 71410920.817138\n",
      "Total training time: 49.42 seconds.\n",
      "-- Epoch 587\n",
      "Norm: 28458.38, NNZs: 41, Bias: 2245.819589, T: 234401427, Avg. loss: 71299359.692016\n",
      "Total training time: 49.50 seconds.\n",
      "-- Epoch 588\n",
      "Norm: 28434.73, NNZs: 41, Bias: 2245.873120, T: 234800748, Avg. loss: 71188092.910657\n",
      "Total training time: 49.58 seconds.\n",
      "-- Epoch 589\n",
      "Norm: 28409.51, NNZs: 41, Bias: 2245.943934, T: 235200069, Avg. loss: 71077299.530752\n",
      "Total training time: 49.67 seconds.\n",
      "-- Epoch 590\n",
      "Norm: 28384.55, NNZs: 41, Bias: 2246.030787, T: 235599390, Avg. loss: 70966537.591296\n",
      "Total training time: 49.75 seconds.\n",
      "-- Epoch 591\n",
      "Norm: 28359.94, NNZs: 41, Bias: 2246.076924, T: 235998711, Avg. loss: 70856198.060294\n",
      "Total training time: 49.83 seconds.\n",
      "-- Epoch 592\n",
      "Norm: 28333.98, NNZs: 41, Bias: 2246.097917, T: 236398032, Avg. loss: 70745917.071074\n",
      "Total training time: 49.91 seconds.\n",
      "-- Epoch 593\n",
      "Norm: 28310.81, NNZs: 41, Bias: 2246.186681, T: 236797353, Avg. loss: 70636471.591090\n",
      "Total training time: 49.99 seconds.\n",
      "-- Epoch 594\n",
      "Norm: 28286.70, NNZs: 41, Bias: 2246.291516, T: 237196674, Avg. loss: 70527208.115938\n",
      "Total training time: 50.08 seconds.\n",
      "-- Epoch 595\n",
      "Norm: 28262.35, NNZs: 41, Bias: 2246.326929, T: 237595995, Avg. loss: 70418176.567903\n",
      "Total training time: 50.16 seconds.\n",
      "-- Epoch 596\n",
      "Norm: 28239.50, NNZs: 41, Bias: 2246.429038, T: 237995316, Avg. loss: 70309634.004121\n",
      "Total training time: 50.24 seconds.\n",
      "-- Epoch 597\n",
      "Norm: 28215.83, NNZs: 41, Bias: 2246.456733, T: 238394637, Avg. loss: 70201285.997203\n",
      "Total training time: 50.32 seconds.\n",
      "-- Epoch 598\n",
      "Norm: 28192.22, NNZs: 41, Bias: 2246.509859, T: 238793958, Avg. loss: 70093309.041827\n",
      "Total training time: 50.41 seconds.\n",
      "-- Epoch 599\n",
      "Norm: 28169.80, NNZs: 41, Bias: 2246.595229, T: 239193279, Avg. loss: 69985958.631484\n",
      "Total training time: 50.49 seconds.\n",
      "-- Epoch 600\n",
      "Norm: 28146.86, NNZs: 41, Bias: 2246.666747, T: 239592600, Avg. loss: 69878826.002856\n",
      "Total training time: 50.57 seconds.\n",
      "-- Epoch 601\n",
      "Norm: 28122.87, NNZs: 41, Bias: 2246.735482, T: 239991921, Avg. loss: 69771860.518407\n",
      "Total training time: 50.65 seconds.\n",
      "-- Epoch 602\n",
      "Norm: 28100.91, NNZs: 41, Bias: 2246.797764, T: 240391242, Avg. loss: 69665301.970285\n",
      "Total training time: 50.73 seconds.\n",
      "-- Epoch 603\n",
      "Norm: 28076.25, NNZs: 41, Bias: 2246.821896, T: 240790563, Avg. loss: 69559078.986533\n",
      "Total training time: 50.82 seconds.\n",
      "-- Epoch 604\n",
      "Norm: 28052.25, NNZs: 41, Bias: 2246.847451, T: 241189884, Avg. loss: 69453328.216323\n",
      "Total training time: 50.90 seconds.\n",
      "-- Epoch 605\n",
      "Norm: 28028.60, NNZs: 41, Bias: 2246.909742, T: 241589205, Avg. loss: 69348000.658862\n",
      "Total training time: 50.98 seconds.\n",
      "-- Epoch 606\n",
      "Norm: 28004.24, NNZs: 41, Bias: 2246.963807, T: 241988526, Avg. loss: 69242755.648260\n",
      "Total training time: 51.06 seconds.\n",
      "-- Epoch 607\n",
      "Norm: 27980.50, NNZs: 41, Bias: 2246.998851, T: 242387847, Avg. loss: 69137627.535933\n",
      "Total training time: 51.14 seconds.\n",
      "-- Epoch 608\n",
      "Norm: 27956.72, NNZs: 41, Bias: 2247.061027, T: 242787168, Avg. loss: 69032895.493408\n",
      "Total training time: 51.23 seconds.\n",
      "-- Epoch 609\n",
      "Norm: 27932.72, NNZs: 41, Bias: 2247.110734, T: 243186489, Avg. loss: 68928644.638353\n",
      "Total training time: 51.31 seconds.\n",
      "-- Epoch 610\n",
      "Norm: 27909.60, NNZs: 41, Bias: 2247.195886, T: 243585810, Avg. loss: 68824696.816673\n",
      "Total training time: 51.39 seconds.\n",
      "-- Epoch 611\n",
      "Norm: 27888.41, NNZs: 41, Bias: 2247.261714, T: 243985131, Avg. loss: 68721272.537181\n",
      "Total training time: 51.47 seconds.\n",
      "-- Epoch 612\n",
      "Norm: 27864.32, NNZs: 41, Bias: 2247.322335, T: 244384452, Avg. loss: 68618122.130386\n",
      "Total training time: 51.55 seconds.\n",
      "-- Epoch 613\n",
      "Norm: 27841.65, NNZs: 41, Bias: 2247.418786, T: 244783773, Avg. loss: 68515307.969834\n",
      "Total training time: 51.64 seconds.\n",
      "-- Epoch 614\n",
      "Norm: 27822.11, NNZs: 41, Bias: 2247.535986, T: 245183094, Avg. loss: 68412964.486454\n",
      "Total training time: 51.72 seconds.\n",
      "-- Epoch 615\n",
      "Norm: 27797.19, NNZs: 41, Bias: 2247.554897, T: 245582415, Avg. loss: 68310598.386490\n",
      "Total training time: 51.80 seconds.\n",
      "-- Epoch 616\n",
      "Norm: 27773.04, NNZs: 41, Bias: 2247.580583, T: 245981736, Avg. loss: 68208484.567755\n",
      "Total training time: 51.88 seconds.\n",
      "-- Epoch 617\n",
      "Norm: 27751.33, NNZs: 41, Bias: 2247.672566, T: 246381057, Avg. loss: 68106945.769938\n",
      "Total training time: 51.96 seconds.\n",
      "-- Epoch 618\n",
      "Norm: 27727.64, NNZs: 41, Bias: 2247.705975, T: 246780378, Avg. loss: 68005481.850027\n",
      "Total training time: 52.04 seconds.\n",
      "-- Epoch 619\n",
      "Norm: 27704.89, NNZs: 41, Bias: 2247.736236, T: 247179699, Avg. loss: 67904574.588847\n",
      "Total training time: 52.13 seconds.\n",
      "-- Epoch 620\n",
      "Norm: 27680.96, NNZs: 41, Bias: 2247.802364, T: 247579020, Avg. loss: 67803722.764787\n",
      "Total training time: 52.21 seconds.\n",
      "-- Epoch 621\n",
      "Norm: 27658.35, NNZs: 41, Bias: 2247.846446, T: 247978341, Avg. loss: 67703132.443666\n",
      "Total training time: 52.29 seconds.\n",
      "-- Epoch 622\n",
      "Norm: 27634.38, NNZs: 41, Bias: 2247.865312, T: 248377662, Avg. loss: 67603071.664630\n",
      "Total training time: 52.37 seconds.\n",
      "-- Epoch 623\n",
      "Norm: 27613.08, NNZs: 41, Bias: 2247.949439, T: 248776983, Avg. loss: 67503405.092323\n",
      "Total training time: 52.45 seconds.\n",
      "-- Epoch 624\n",
      "Norm: 27592.58, NNZs: 41, Bias: 2248.031703, T: 249176304, Avg. loss: 67404158.579059\n",
      "Total training time: 52.54 seconds.\n",
      "-- Epoch 625\n",
      "Norm: 27569.51, NNZs: 41, Bias: 2248.102116, T: 249575625, Avg. loss: 67304855.993627\n",
      "Total training time: 52.62 seconds.\n",
      "-- Epoch 626\n",
      "Norm: 27544.52, NNZs: 41, Bias: 2248.117798, T: 249974946, Avg. loss: 67205571.675676\n",
      "Total training time: 52.70 seconds.\n",
      "-- Epoch 627\n",
      "Norm: 27523.14, NNZs: 41, Bias: 2248.218826, T: 250374267, Avg. loss: 67107018.480169\n",
      "Total training time: 52.78 seconds.\n",
      "-- Epoch 628\n",
      "Norm: 27500.79, NNZs: 41, Bias: 2248.300354, T: 250773588, Avg. loss: 67008796.776880\n",
      "Total training time: 52.86 seconds.\n",
      "-- Epoch 629\n",
      "Norm: 27477.73, NNZs: 41, Bias: 2248.332234, T: 251172909, Avg. loss: 66910652.086245\n",
      "Total training time: 52.95 seconds.\n",
      "-- Epoch 630\n",
      "Norm: 27455.38, NNZs: 41, Bias: 2248.422176, T: 251572230, Avg. loss: 66813103.075431\n",
      "Total training time: 53.03 seconds.\n",
      "-- Epoch 631\n",
      "Norm: 27431.49, NNZs: 41, Bias: 2248.467453, T: 251971551, Avg. loss: 66715580.776865\n",
      "Total training time: 53.11 seconds.\n",
      "-- Epoch 632\n",
      "Norm: 27408.45, NNZs: 41, Bias: 2248.540580, T: 252370872, Avg. loss: 66618391.797648\n",
      "Total training time: 53.19 seconds.\n",
      "-- Epoch 633\n",
      "Norm: 27386.90, NNZs: 41, Bias: 2248.552778, T: 252770193, Avg. loss: 66521669.703935\n",
      "Total training time: 53.27 seconds.\n",
      "-- Epoch 634\n",
      "Norm: 27364.87, NNZs: 41, Bias: 2248.572538, T: 253169514, Avg. loss: 66425113.720045\n",
      "Total training time: 53.36 seconds.\n",
      "-- Epoch 635\n",
      "Norm: 27342.14, NNZs: 41, Bias: 2248.618652, T: 253568835, Avg. loss: 66328959.212935\n",
      "Total training time: 53.44 seconds.\n",
      "-- Epoch 636\n",
      "Norm: 27319.99, NNZs: 41, Bias: 2248.679921, T: 253968156, Avg. loss: 66232901.514564\n",
      "Total training time: 53.52 seconds.\n",
      "-- Epoch 637\n",
      "Norm: 27298.75, NNZs: 41, Bias: 2248.721946, T: 254367477, Avg. loss: 66137140.188976\n",
      "Total training time: 53.60 seconds.\n",
      "-- Epoch 638\n",
      "Norm: 27278.03, NNZs: 41, Bias: 2248.786541, T: 254766798, Avg. loss: 66041941.740460\n",
      "Total training time: 53.68 seconds.\n",
      "-- Epoch 639\n",
      "Norm: 27257.14, NNZs: 41, Bias: 2248.874536, T: 255166119, Avg. loss: 65946920.132954\n",
      "Total training time: 53.77 seconds.\n",
      "-- Epoch 640\n",
      "Norm: 27233.55, NNZs: 41, Bias: 2248.874688, T: 255565440, Avg. loss: 65851850.370606\n",
      "Total training time: 53.85 seconds.\n",
      "-- Epoch 641\n",
      "Norm: 27214.16, NNZs: 41, Bias: 2248.914438, T: 255964761, Avg. loss: 65757345.528587\n",
      "Total training time: 53.93 seconds.\n",
      "-- Epoch 642\n",
      "Norm: 27193.26, NNZs: 41, Bias: 2249.003764, T: 256364082, Avg. loss: 65663300.338490\n",
      "Total training time: 54.02 seconds.\n",
      "-- Epoch 643\n",
      "Norm: 27171.44, NNZs: 41, Bias: 2249.040093, T: 256763403, Avg. loss: 65569391.221919\n",
      "Total training time: 54.10 seconds.\n",
      "-- Epoch 644\n",
      "Norm: 27153.96, NNZs: 41, Bias: 2249.149055, T: 257162724, Avg. loss: 65476008.576193\n",
      "Total training time: 54.18 seconds.\n",
      "-- Epoch 645\n",
      "Norm: 27132.13, NNZs: 41, Bias: 2249.196781, T: 257562045, Avg. loss: 65382627.980218\n",
      "Total training time: 54.26 seconds.\n",
      "-- Epoch 646\n",
      "Norm: 27111.10, NNZs: 41, Bias: 2249.278122, T: 257961366, Avg. loss: 65289654.181252\n",
      "Total training time: 54.34 seconds.\n",
      "-- Epoch 647\n",
      "Norm: 27091.07, NNZs: 41, Bias: 2249.334354, T: 258360687, Avg. loss: 65196871.085519\n",
      "Total training time: 54.43 seconds.\n",
      "-- Epoch 648\n",
      "Norm: 27069.81, NNZs: 41, Bias: 2249.382695, T: 258760008, Avg. loss: 65104412.119791\n",
      "Total training time: 54.51 seconds.\n",
      "-- Epoch 649\n",
      "Norm: 27049.79, NNZs: 41, Bias: 2249.494214, T: 259159329, Avg. loss: 65012335.602495\n",
      "Total training time: 54.59 seconds.\n",
      "-- Epoch 650\n",
      "Norm: 27031.15, NNZs: 41, Bias: 2249.558478, T: 259558650, Avg. loss: 64920589.754135\n",
      "Total training time: 54.67 seconds.\n",
      "-- Epoch 651\n",
      "Norm: 27008.97, NNZs: 41, Bias: 2249.621141, T: 259957971, Avg. loss: 64828723.821665\n",
      "Total training time: 54.75 seconds.\n",
      "-- Epoch 652\n",
      "Norm: 26987.55, NNZs: 41, Bias: 2249.676205, T: 260357292, Avg. loss: 64737348.500169\n",
      "Total training time: 54.84 seconds.\n",
      "-- Epoch 653\n",
      "Norm: 26967.42, NNZs: 41, Bias: 2249.791935, T: 260756613, Avg. loss: 64646145.829562\n",
      "Total training time: 54.92 seconds.\n",
      "-- Epoch 654\n",
      "Norm: 26946.28, NNZs: 41, Bias: 2249.830399, T: 261155934, Avg. loss: 64555001.760457\n",
      "Total training time: 55.00 seconds.\n",
      "-- Epoch 655\n",
      "Norm: 26925.54, NNZs: 41, Bias: 2249.881804, T: 261555255, Avg. loss: 64464348.986948\n",
      "Total training time: 55.08 seconds.\n",
      "-- Epoch 656\n",
      "Norm: 26904.64, NNZs: 41, Bias: 2249.952410, T: 261954576, Avg. loss: 64373750.987452\n",
      "Total training time: 55.16 seconds.\n",
      "-- Epoch 657\n",
      "Norm: 26884.87, NNZs: 41, Bias: 2250.010707, T: 262353897, Avg. loss: 64283665.348731\n",
      "Total training time: 55.25 seconds.\n",
      "-- Epoch 658\n",
      "Norm: 26863.86, NNZs: 41, Bias: 2250.068594, T: 262753218, Avg. loss: 64193597.610270\n",
      "Total training time: 55.33 seconds.\n",
      "-- Epoch 659\n",
      "Norm: 26844.30, NNZs: 41, Bias: 2250.117585, T: 263152539, Avg. loss: 64104081.024575\n",
      "Total training time: 55.41 seconds.\n",
      "-- Epoch 660\n",
      "Norm: 26823.63, NNZs: 41, Bias: 2250.152203, T: 263551860, Avg. loss: 64014547.708382\n",
      "Total training time: 55.49 seconds.\n",
      "-- Epoch 661\n",
      "Norm: 26804.68, NNZs: 41, Bias: 2250.247447, T: 263951181, Avg. loss: 63925557.992047\n",
      "Total training time: 55.57 seconds.\n",
      "-- Epoch 662\n",
      "Norm: 26785.91, NNZs: 41, Bias: 2250.313726, T: 264350502, Avg. loss: 63836725.434348\n",
      "Total training time: 55.66 seconds.\n",
      "-- Epoch 663\n",
      "Norm: 26765.03, NNZs: 41, Bias: 2250.337441, T: 264749823, Avg. loss: 63748131.859201\n",
      "Total training time: 55.74 seconds.\n",
      "-- Epoch 664\n",
      "Norm: 26746.17, NNZs: 41, Bias: 2250.389000, T: 265149144, Avg. loss: 63659726.051747\n",
      "Total training time: 55.82 seconds.\n",
      "-- Epoch 665\n",
      "Norm: 26725.64, NNZs: 41, Bias: 2250.444103, T: 265548465, Avg. loss: 63571614.780281\n",
      "Total training time: 55.90 seconds.\n",
      "-- Epoch 666\n",
      "Norm: 26707.27, NNZs: 41, Bias: 2250.504190, T: 265947786, Avg. loss: 63483830.019902\n",
      "Total training time: 55.98 seconds.\n",
      "-- Epoch 667\n",
      "Norm: 26688.16, NNZs: 41, Bias: 2250.583411, T: 266347107, Avg. loss: 63396414.650108\n",
      "Total training time: 56.07 seconds.\n",
      "-- Epoch 668\n",
      "Norm: 26667.38, NNZs: 41, Bias: 2250.633395, T: 266746428, Avg. loss: 63309055.856823\n",
      "Total training time: 56.15 seconds.\n",
      "-- Epoch 669\n",
      "Norm: 26647.13, NNZs: 41, Bias: 2250.700385, T: 267145749, Avg. loss: 63221914.625208\n",
      "Total training time: 56.23 seconds.\n",
      "-- Epoch 670\n",
      "Norm: 26625.32, NNZs: 41, Bias: 2250.714858, T: 267545070, Avg. loss: 63134932.064169\n",
      "Total training time: 56.31 seconds.\n",
      "-- Epoch 671\n",
      "Norm: 26604.90, NNZs: 41, Bias: 2250.749366, T: 267944391, Avg. loss: 63048205.761465\n",
      "Total training time: 56.39 seconds.\n",
      "-- Epoch 672\n",
      "Norm: 26583.44, NNZs: 41, Bias: 2250.806910, T: 268343712, Avg. loss: 62961773.705566\n",
      "Total training time: 56.48 seconds.\n",
      "-- Epoch 673\n",
      "Norm: 26562.97, NNZs: 41, Bias: 2250.854751, T: 268743033, Avg. loss: 62875686.878665\n",
      "Total training time: 56.56 seconds.\n",
      "-- Epoch 674\n",
      "Norm: 26544.68, NNZs: 41, Bias: 2250.917664, T: 269142354, Avg. loss: 62789930.572108\n",
      "Total training time: 56.64 seconds.\n",
      "-- Epoch 675\n",
      "Norm: 26526.12, NNZs: 41, Bias: 2250.955093, T: 269541675, Avg. loss: 62704540.140028\n",
      "Total training time: 56.72 seconds.\n",
      "-- Epoch 676\n",
      "Norm: 26508.66, NNZs: 41, Bias: 2251.015843, T: 269940996, Avg. loss: 62619324.415440\n",
      "Total training time: 56.80 seconds.\n",
      "-- Epoch 677\n",
      "Norm: 26490.81, NNZs: 41, Bias: 2251.067214, T: 270340317, Avg. loss: 62534299.544118\n",
      "Total training time: 56.88 seconds.\n",
      "-- Epoch 678\n",
      "Norm: 26471.74, NNZs: 41, Bias: 2251.091325, T: 270739638, Avg. loss: 62449298.337368\n",
      "Total training time: 56.97 seconds.\n",
      "-- Epoch 679\n",
      "Norm: 26453.18, NNZs: 41, Bias: 2251.148596, T: 271138959, Avg. loss: 62364700.719297\n",
      "Total training time: 57.05 seconds.\n",
      "-- Epoch 680\n",
      "Norm: 26433.86, NNZs: 41, Bias: 2251.189160, T: 271538280, Avg. loss: 62280422.945133\n",
      "Total training time: 57.13 seconds.\n",
      "-- Epoch 681\n",
      "Norm: 26414.71, NNZs: 41, Bias: 2251.270992, T: 271937601, Avg. loss: 62196335.454986\n",
      "Total training time: 57.21 seconds.\n",
      "-- Epoch 682\n",
      "Norm: 26394.37, NNZs: 41, Bias: 2251.331579, T: 272336922, Avg. loss: 62112309.264989\n",
      "Total training time: 57.30 seconds.\n",
      "-- Epoch 683\n",
      "Norm: 26375.94, NNZs: 41, Bias: 2251.435926, T: 272736243, Avg. loss: 62028501.357835\n",
      "Total training time: 57.38 seconds.\n",
      "-- Epoch 684\n",
      "Norm: 26357.12, NNZs: 41, Bias: 2251.501439, T: 273135564, Avg. loss: 61945102.945039\n",
      "Total training time: 57.46 seconds.\n",
      "-- Epoch 685\n",
      "Norm: 26339.59, NNZs: 41, Bias: 2251.572109, T: 273534885, Avg. loss: 61862004.650153\n",
      "Total training time: 57.54 seconds.\n",
      "-- Epoch 686\n",
      "Norm: 26320.72, NNZs: 41, Bias: 2251.614332, T: 273934206, Avg. loss: 61778967.607767\n",
      "Total training time: 57.62 seconds.\n",
      "-- Epoch 687\n",
      "Norm: 26304.28, NNZs: 41, Bias: 2251.704949, T: 274333527, Avg. loss: 61696374.031249\n",
      "Total training time: 57.71 seconds.\n",
      "-- Epoch 688\n",
      "Norm: 26286.57, NNZs: 41, Bias: 2251.785516, T: 274732848, Avg. loss: 61613967.019983\n",
      "Total training time: 57.79 seconds.\n",
      "-- Epoch 689\n",
      "Norm: 26264.97, NNZs: 41, Bias: 2251.810832, T: 275132169, Avg. loss: 61531541.674216\n",
      "Total training time: 57.87 seconds.\n",
      "-- Epoch 690\n",
      "Norm: 26246.01, NNZs: 41, Bias: 2251.836389, T: 275531490, Avg. loss: 61449451.708965\n",
      "Total training time: 57.95 seconds.\n",
      "-- Epoch 691\n",
      "Norm: 26227.60, NNZs: 41, Bias: 2251.914145, T: 275930811, Avg. loss: 61367663.394210\n",
      "Total training time: 58.03 seconds.\n",
      "-- Epoch 692\n",
      "Norm: 26210.45, NNZs: 41, Bias: 2251.983811, T: 276330132, Avg. loss: 61286120.922915\n",
      "Total training time: 58.12 seconds.\n",
      "-- Epoch 693\n",
      "Norm: 26192.73, NNZs: 41, Bias: 2252.044127, T: 276729453, Avg. loss: 61204744.843314\n",
      "Total training time: 58.20 seconds.\n",
      "-- Epoch 694\n",
      "Norm: 26173.62, NNZs: 41, Bias: 2252.076651, T: 277128774, Avg. loss: 61123526.925483\n",
      "Total training time: 58.28 seconds.\n",
      "-- Epoch 695\n",
      "Norm: 26157.26, NNZs: 41, Bias: 2252.125828, T: 277528095, Avg. loss: 61042834.323494\n",
      "Total training time: 58.36 seconds.\n",
      "-- Epoch 696\n",
      "Norm: 26139.26, NNZs: 41, Bias: 2252.205757, T: 277927416, Avg. loss: 60962211.609950\n",
      "Total training time: 58.44 seconds.\n",
      "-- Epoch 697\n",
      "Norm: 26120.47, NNZs: 41, Bias: 2252.272172, T: 278326737, Avg. loss: 60881844.556171\n",
      "Total training time: 58.52 seconds.\n",
      "-- Epoch 698\n",
      "Norm: 26100.23, NNZs: 41, Bias: 2252.312509, T: 278726058, Avg. loss: 60801409.199776\n",
      "Total training time: 58.61 seconds.\n",
      "-- Epoch 699\n",
      "Norm: 26081.80, NNZs: 41, Bias: 2252.363118, T: 279125379, Avg. loss: 60721371.705073\n",
      "Total training time: 58.69 seconds.\n",
      "-- Epoch 700\n",
      "Norm: 26063.39, NNZs: 41, Bias: 2252.455901, T: 279524700, Avg. loss: 60641621.688820\n",
      "Total training time: 58.77 seconds.\n",
      "-- Epoch 701\n",
      "Norm: 26043.56, NNZs: 41, Bias: 2252.477466, T: 279924021, Avg. loss: 60561851.685822\n",
      "Total training time: 58.85 seconds.\n",
      "-- Epoch 702\n",
      "Norm: 26027.02, NNZs: 41, Bias: 2252.561581, T: 280323342, Avg. loss: 60482734.093715\n",
      "Total training time: 58.93 seconds.\n",
      "-- Epoch 703\n",
      "Norm: 26011.83, NNZs: 41, Bias: 2252.629448, T: 280722663, Avg. loss: 60403837.415944\n",
      "Total training time: 59.02 seconds.\n",
      "-- Epoch 704\n",
      "Norm: 25991.21, NNZs: 41, Bias: 2252.610812, T: 281121984, Avg. loss: 60324774.099748\n",
      "Total training time: 59.10 seconds.\n",
      "-- Epoch 705\n",
      "Norm: 25973.59, NNZs: 41, Bias: 2252.640455, T: 281521305, Avg. loss: 60246108.442996\n",
      "Total training time: 59.18 seconds.\n",
      "-- Epoch 706\n",
      "Norm: 25954.34, NNZs: 41, Bias: 2252.673268, T: 281920626, Avg. loss: 60167626.532317\n",
      "Total training time: 59.26 seconds.\n",
      "-- Epoch 707\n",
      "Norm: 25937.52, NNZs: 41, Bias: 2252.719350, T: 282319947, Avg. loss: 60089501.105505\n",
      "Total training time: 59.34 seconds.\n",
      "-- Epoch 708\n",
      "Norm: 25920.93, NNZs: 41, Bias: 2252.788696, T: 282719268, Avg. loss: 60011573.234048\n",
      "Total training time: 59.42 seconds.\n",
      "-- Epoch 709\n",
      "Norm: 25901.62, NNZs: 41, Bias: 2252.790658, T: 283118589, Avg. loss: 59933502.399339\n",
      "Total training time: 59.50 seconds.\n",
      "-- Epoch 710\n",
      "Norm: 25882.34, NNZs: 41, Bias: 2252.845032, T: 283517910, Avg. loss: 59855726.050976\n",
      "Total training time: 59.58 seconds.\n",
      "-- Epoch 711\n",
      "Norm: 25862.82, NNZs: 41, Bias: 2252.915371, T: 283917231, Avg. loss: 59778189.753561\n",
      "Total training time: 59.67 seconds.\n",
      "-- Epoch 712\n",
      "Norm: 25847.63, NNZs: 41, Bias: 2252.974783, T: 284316552, Avg. loss: 59700866.652594\n",
      "Total training time: 59.75 seconds.\n",
      "-- Epoch 713\n",
      "Norm: 25830.13, NNZs: 41, Bias: 2253.017095, T: 284715873, Avg. loss: 59623526.188909\n",
      "Total training time: 59.83 seconds.\n",
      "-- Epoch 714\n",
      "Norm: 25812.14, NNZs: 41, Bias: 2253.064660, T: 285115194, Avg. loss: 59546632.079695\n",
      "Total training time: 59.91 seconds.\n",
      "-- Epoch 715\n",
      "Norm: 25795.64, NNZs: 41, Bias: 2253.115700, T: 285514515, Avg. loss: 59470100.230846\n",
      "Total training time: 59.99 seconds.\n",
      "-- Epoch 716\n",
      "Norm: 25778.06, NNZs: 41, Bias: 2253.177311, T: 285913836, Avg. loss: 59393745.860905\n",
      "Total training time: 60.08 seconds.\n",
      "-- Epoch 717\n",
      "Norm: 25760.04, NNZs: 41, Bias: 2253.246941, T: 286313157, Avg. loss: 59317439.126975\n",
      "Total training time: 60.16 seconds.\n",
      "-- Epoch 718\n",
      "Norm: 25740.18, NNZs: 41, Bias: 2253.278797, T: 286712478, Avg. loss: 59241235.363744\n",
      "Total training time: 60.24 seconds.\n",
      "-- Epoch 719\n",
      "Norm: 25723.08, NNZs: 41, Bias: 2253.344577, T: 287111799, Avg. loss: 59165429.384846\n",
      "Total training time: 60.32 seconds.\n",
      "-- Epoch 720\n",
      "Norm: 25706.78, NNZs: 41, Bias: 2253.362820, T: 287511120, Avg. loss: 59089731.644675\n",
      "Total training time: 60.40 seconds.\n",
      "-- Epoch 721\n",
      "Norm: 25689.34, NNZs: 41, Bias: 2253.392452, T: 287910441, Avg. loss: 59014199.742146\n",
      "Total training time: 60.48 seconds.\n",
      "-- Epoch 722\n",
      "Norm: 25672.60, NNZs: 41, Bias: 2253.475091, T: 288309762, Avg. loss: 58938963.327677\n",
      "Total training time: 60.56 seconds.\n",
      "-- Epoch 723\n",
      "Norm: 25655.47, NNZs: 41, Bias: 2253.518900, T: 288709083, Avg. loss: 58863825.701095\n",
      "Total training time: 60.64 seconds.\n",
      "-- Epoch 724\n",
      "Norm: 25645.15, NNZs: 41, Bias: 2253.607081, T: 289108404, Avg. loss: 58788965.057604\n",
      "Total training time: 60.72 seconds.\n",
      "-- Epoch 725\n",
      "Norm: 25621.91, NNZs: 41, Bias: 2253.677042, T: 289507725, Avg. loss: 58714388.420376\n",
      "Total training time: 60.81 seconds.\n",
      "-- Epoch 726\n",
      "Norm: 25603.97, NNZs: 41, Bias: 2253.715847, T: 289907046, Avg. loss: 58639855.947957\n",
      "Total training time: 60.89 seconds.\n",
      "-- Epoch 727\n",
      "Norm: 25585.37, NNZs: 41, Bias: 2253.743019, T: 290306367, Avg. loss: 58565463.230677\n",
      "Total training time: 60.97 seconds.\n",
      "-- Epoch 728\n",
      "Norm: 25567.46, NNZs: 41, Bias: 2253.818614, T: 290705688, Avg. loss: 58491395.268977\n",
      "Total training time: 61.05 seconds.\n",
      "-- Epoch 729\n",
      "Norm: 25548.06, NNZs: 41, Bias: 2253.870874, T: 291105009, Avg. loss: 58417382.160470\n",
      "Total training time: 61.13 seconds.\n",
      "-- Epoch 730\n",
      "Norm: 25532.73, NNZs: 41, Bias: 2253.912882, T: 291504330, Avg. loss: 58343785.184958\n",
      "Total training time: 61.21 seconds.\n",
      "-- Epoch 731\n",
      "Norm: 25514.94, NNZs: 41, Bias: 2253.974045, T: 291903651, Avg. loss: 58270194.069649\n",
      "Total training time: 61.29 seconds.\n",
      "-- Epoch 732\n",
      "Norm: 25499.59, NNZs: 41, Bias: 2253.993070, T: 292302972, Avg. loss: 58196999.210424\n",
      "Total training time: 61.37 seconds.\n",
      "-- Epoch 733\n",
      "Norm: 25481.16, NNZs: 41, Bias: 2254.005119, T: 292702293, Avg. loss: 58123854.371709\n",
      "Total training time: 61.46 seconds.\n",
      "-- Epoch 734\n",
      "Norm: 25464.53, NNZs: 41, Bias: 2254.005513, T: 293101614, Avg. loss: 58050794.733961\n",
      "Total training time: 61.54 seconds.\n",
      "-- Epoch 735\n",
      "Norm: 25448.23, NNZs: 41, Bias: 2254.063179, T: 293500935, Avg. loss: 57978045.458963\n",
      "Total training time: 61.62 seconds.\n",
      "-- Epoch 736\n",
      "Norm: 25430.57, NNZs: 41, Bias: 2254.111669, T: 293900256, Avg. loss: 57905569.958075\n",
      "Total training time: 61.70 seconds.\n",
      "-- Epoch 737\n",
      "Norm: 25412.75, NNZs: 41, Bias: 2254.134377, T: 294299577, Avg. loss: 57833203.794002\n",
      "Total training time: 61.78 seconds.\n",
      "-- Epoch 738\n",
      "Norm: 25397.52, NNZs: 41, Bias: 2254.221042, T: 294698898, Avg. loss: 57761191.887318\n",
      "Total training time: 61.86 seconds.\n",
      "-- Epoch 739\n",
      "Norm: 25382.05, NNZs: 41, Bias: 2254.292526, T: 295098219, Avg. loss: 57689475.290510\n",
      "Total training time: 61.94 seconds.\n",
      "-- Epoch 740\n",
      "Norm: 25364.37, NNZs: 41, Bias: 2254.334498, T: 295497540, Avg. loss: 57617663.376268\n",
      "Total training time: 62.02 seconds.\n",
      "-- Epoch 741\n",
      "Norm: 25347.02, NNZs: 41, Bias: 2254.413365, T: 295896861, Avg. loss: 57546100.144239\n",
      "Total training time: 62.11 seconds.\n",
      "-- Epoch 742\n",
      "Norm: 25330.30, NNZs: 41, Bias: 2254.435122, T: 296296182, Avg. loss: 57474549.013927\n",
      "Total training time: 62.19 seconds.\n",
      "-- Epoch 743\n",
      "Norm: 25314.82, NNZs: 41, Bias: 2254.524614, T: 296695503, Avg. loss: 57403438.006162\n",
      "Total training time: 62.27 seconds.\n",
      "-- Epoch 744\n",
      "Norm: 25296.23, NNZs: 41, Bias: 2254.554134, T: 297094824, Avg. loss: 57332301.834192\n",
      "Total training time: 62.35 seconds.\n",
      "-- Epoch 745\n",
      "Norm: 25278.95, NNZs: 41, Bias: 2254.606049, T: 297494145, Avg. loss: 57261546.900498\n",
      "Total training time: 62.43 seconds.\n",
      "-- Epoch 746\n",
      "Norm: 25262.73, NNZs: 41, Bias: 2254.640596, T: 297893466, Avg. loss: 57190825.713133\n",
      "Total training time: 62.51 seconds.\n",
      "-- Epoch 747\n",
      "Norm: 25246.47, NNZs: 41, Bias: 2254.670796, T: 298292787, Avg. loss: 57120365.889149\n",
      "Total training time: 62.59 seconds.\n",
      "-- Epoch 748\n",
      "Norm: 25229.61, NNZs: 41, Bias: 2254.729624, T: 298692108, Avg. loss: 57049887.702785\n",
      "Total training time: 62.67 seconds.\n",
      "-- Epoch 749\n",
      "Norm: 25213.40, NNZs: 41, Bias: 2254.767346, T: 299091429, Avg. loss: 56979739.648977\n",
      "Total training time: 62.75 seconds.\n",
      "-- Epoch 750\n",
      "Norm: 25198.09, NNZs: 41, Bias: 2254.793293, T: 299490750, Avg. loss: 56909815.709064\n",
      "Total training time: 62.84 seconds.\n",
      "-- Epoch 751\n",
      "Norm: 25181.85, NNZs: 41, Bias: 2254.843099, T: 299890071, Avg. loss: 56840122.165024\n",
      "Total training time: 62.92 seconds.\n",
      "-- Epoch 752\n",
      "Norm: 25167.47, NNZs: 41, Bias: 2254.878843, T: 300289392, Avg. loss: 56770535.015533\n",
      "Total training time: 63.00 seconds.\n",
      "-- Epoch 753\n",
      "Norm: 25148.76, NNZs: 41, Bias: 2254.921179, T: 300688713, Avg. loss: 56700935.771878\n",
      "Total training time: 63.08 seconds.\n",
      "-- Epoch 754\n",
      "Norm: 25132.06, NNZs: 41, Bias: 2254.923705, T: 301088034, Avg. loss: 56631604.152818\n",
      "Total training time: 63.16 seconds.\n",
      "-- Epoch 755\n",
      "Norm: 25116.84, NNZs: 41, Bias: 2254.976661, T: 301487355, Avg. loss: 56562508.937021\n",
      "Total training time: 63.24 seconds.\n",
      "-- Epoch 756\n",
      "Norm: 25100.34, NNZs: 41, Bias: 2255.027719, T: 301886676, Avg. loss: 56493585.676326\n",
      "Total training time: 63.32 seconds.\n",
      "-- Epoch 757\n",
      "Norm: 25086.27, NNZs: 41, Bias: 2255.084001, T: 302285997, Avg. loss: 56424899.968511\n",
      "Total training time: 63.40 seconds.\n",
      "-- Epoch 758\n",
      "Norm: 25068.34, NNZs: 41, Bias: 2255.121161, T: 302685318, Avg. loss: 56356241.625979\n",
      "Total training time: 63.48 seconds.\n",
      "-- Epoch 759\n",
      "Norm: 25051.16, NNZs: 41, Bias: 2255.165728, T: 303084639, Avg. loss: 56287602.454648\n",
      "Total training time: 63.57 seconds.\n",
      "-- Epoch 760\n",
      "Norm: 25035.30, NNZs: 41, Bias: 2255.191989, T: 303483960, Avg. loss: 56219343.228835\n",
      "Total training time: 63.65 seconds.\n",
      "-- Epoch 761\n",
      "Norm: 25022.01, NNZs: 41, Bias: 2255.299329, T: 303883281, Avg. loss: 56151454.347283\n",
      "Total training time: 63.73 seconds.\n",
      "-- Epoch 762\n",
      "Norm: 25006.62, NNZs: 41, Bias: 2255.345098, T: 304282602, Avg. loss: 56083705.233716\n",
      "Total training time: 63.81 seconds.\n",
      "-- Epoch 763\n",
      "Norm: 24989.07, NNZs: 41, Bias: 2255.379639, T: 304681923, Avg. loss: 56016000.070497\n",
      "Total training time: 63.89 seconds.\n",
      "-- Epoch 764\n",
      "Norm: 24972.96, NNZs: 41, Bias: 2255.390925, T: 305081244, Avg. loss: 55948464.705193\n",
      "Total training time: 63.98 seconds.\n",
      "-- Epoch 765\n",
      "Norm: 24956.91, NNZs: 41, Bias: 2255.414378, T: 305480565, Avg. loss: 55881172.362076\n",
      "Total training time: 64.06 seconds.\n",
      "-- Epoch 766\n",
      "Norm: 24942.16, NNZs: 41, Bias: 2255.477569, T: 305879886, Avg. loss: 55813828.992778\n",
      "Total training time: 64.14 seconds.\n",
      "-- Epoch 767\n",
      "Norm: 24928.07, NNZs: 41, Bias: 2255.561202, T: 306279207, Avg. loss: 55746947.925236\n",
      "Total training time: 64.22 seconds.\n",
      "-- Epoch 768\n",
      "Norm: 24912.56, NNZs: 41, Bias: 2255.637271, T: 306678528, Avg. loss: 55680157.568592\n",
      "Total training time: 64.30 seconds.\n",
      "-- Epoch 769\n",
      "Norm: 24896.79, NNZs: 41, Bias: 2255.647702, T: 307077849, Avg. loss: 55613328.246625\n",
      "Total training time: 64.38 seconds.\n",
      "-- Epoch 770\n",
      "Norm: 24880.80, NNZs: 41, Bias: 2255.680383, T: 307477170, Avg. loss: 55546945.502306\n",
      "Total training time: 64.46 seconds.\n",
      "-- Epoch 771\n",
      "Norm: 24866.39, NNZs: 41, Bias: 2255.722365, T: 307876491, Avg. loss: 55480623.012970\n",
      "Total training time: 64.55 seconds.\n",
      "-- Epoch 772\n",
      "Norm: 24850.50, NNZs: 41, Bias: 2255.759375, T: 308275812, Avg. loss: 55414337.823718\n",
      "Total training time: 64.63 seconds.\n",
      "-- Epoch 773\n",
      "Norm: 24836.34, NNZs: 41, Bias: 2255.825121, T: 308675133, Avg. loss: 55348324.826194\n",
      "Total training time: 64.71 seconds.\n",
      "-- Epoch 774\n",
      "Norm: 24822.51, NNZs: 41, Bias: 2255.874695, T: 309074454, Avg. loss: 55282606.582611\n",
      "Total training time: 64.79 seconds.\n",
      "-- Epoch 775\n",
      "Norm: 24805.99, NNZs: 41, Bias: 2255.902891, T: 309473775, Avg. loss: 55216942.973410\n",
      "Total training time: 64.87 seconds.\n",
      "-- Epoch 776\n",
      "Norm: 24791.65, NNZs: 41, Bias: 2255.955856, T: 309873096, Avg. loss: 55151444.525615\n",
      "Total training time: 64.95 seconds.\n",
      "-- Epoch 777\n",
      "Norm: 24777.25, NNZs: 41, Bias: 2255.993275, T: 310272417, Avg. loss: 55086093.710367\n",
      "Total training time: 65.03 seconds.\n",
      "-- Epoch 778\n",
      "Norm: 24763.05, NNZs: 41, Bias: 2256.079864, T: 310671738, Avg. loss: 55021129.890592\n",
      "Total training time: 65.11 seconds.\n",
      "-- Epoch 779\n",
      "Norm: 24748.20, NNZs: 41, Bias: 2256.147922, T: 311071059, Avg. loss: 54956106.297429\n",
      "Total training time: 65.20 seconds.\n",
      "-- Epoch 780\n",
      "Norm: 24733.19, NNZs: 41, Bias: 2256.192004, T: 311470380, Avg. loss: 54891296.250019\n",
      "Total training time: 65.28 seconds.\n",
      "-- Epoch 781\n",
      "Norm: 24717.06, NNZs: 41, Bias: 2256.183892, T: 311869701, Avg. loss: 54826534.348295\n",
      "Total training time: 65.36 seconds.\n",
      "-- Epoch 782\n",
      "Norm: 24701.52, NNZs: 41, Bias: 2256.251706, T: 312269022, Avg. loss: 54761928.206078\n",
      "Total training time: 65.44 seconds.\n",
      "-- Epoch 783\n",
      "Norm: 24685.51, NNZs: 41, Bias: 2256.280774, T: 312668343, Avg. loss: 54697393.435442\n",
      "Total training time: 65.52 seconds.\n",
      "-- Epoch 784\n",
      "Norm: 24670.70, NNZs: 41, Bias: 2256.345452, T: 313067664, Avg. loss: 54633176.256815\n",
      "Total training time: 65.60 seconds.\n",
      "-- Epoch 785\n",
      "Norm: 24656.84, NNZs: 41, Bias: 2256.383861, T: 313466985, Avg. loss: 54569126.317968\n",
      "Total training time: 65.68 seconds.\n",
      "-- Epoch 786\n",
      "Norm: 24641.70, NNZs: 41, Bias: 2256.409891, T: 313866306, Avg. loss: 54505134.600254\n",
      "Total training time: 65.76 seconds.\n",
      "-- Epoch 787\n",
      "Norm: 24626.09, NNZs: 41, Bias: 2256.434913, T: 314265627, Avg. loss: 54441121.415938\n",
      "Total training time: 65.84 seconds.\n",
      "-- Epoch 788\n",
      "Norm: 24610.67, NNZs: 41, Bias: 2256.506529, T: 314664948, Avg. loss: 54377550.151696\n",
      "Total training time: 65.92 seconds.\n",
      "-- Epoch 789\n",
      "Norm: 24597.23, NNZs: 41, Bias: 2256.559496, T: 315064269, Avg. loss: 54314147.796012\n",
      "Total training time: 66.01 seconds.\n",
      "-- Epoch 790\n",
      "Norm: 24582.02, NNZs: 41, Bias: 2256.582225, T: 315463590, Avg. loss: 54250646.061450\n",
      "Total training time: 66.09 seconds.\n",
      "-- Epoch 791\n",
      "Norm: 24567.16, NNZs: 41, Bias: 2256.619878, T: 315862911, Avg. loss: 54187366.285780\n",
      "Total training time: 66.17 seconds.\n",
      "-- Epoch 792\n",
      "Norm: 24554.64, NNZs: 41, Bias: 2256.667974, T: 316262232, Avg. loss: 54124422.001996\n",
      "Total training time: 66.25 seconds.\n",
      "-- Epoch 793\n",
      "Norm: 24538.88, NNZs: 41, Bias: 2256.693133, T: 316661553, Avg. loss: 54061405.360799\n",
      "Total training time: 66.33 seconds.\n",
      "-- Epoch 794\n",
      "Norm: 24523.72, NNZs: 41, Bias: 2256.725449, T: 317060874, Avg. loss: 53998578.226458\n",
      "Total training time: 66.41 seconds.\n",
      "-- Epoch 795\n",
      "Norm: 24508.17, NNZs: 41, Bias: 2256.775769, T: 317460195, Avg. loss: 53935948.115792\n",
      "Total training time: 66.49 seconds.\n",
      "-- Epoch 796\n",
      "Norm: 24492.77, NNZs: 41, Bias: 2256.825268, T: 317859516, Avg. loss: 53873449.461268\n",
      "Total training time: 66.57 seconds.\n",
      "-- Epoch 797\n",
      "Norm: 24477.24, NNZs: 41, Bias: 2256.848535, T: 318258837, Avg. loss: 53811081.331225\n",
      "Total training time: 66.66 seconds.\n",
      "-- Epoch 798\n",
      "Norm: 24461.92, NNZs: 41, Bias: 2256.890244, T: 318658158, Avg. loss: 53748824.591731\n",
      "Total training time: 66.74 seconds.\n",
      "-- Epoch 799\n",
      "Norm: 24447.01, NNZs: 41, Bias: 2256.951346, T: 319057479, Avg. loss: 53686827.501838\n",
      "Total training time: 66.82 seconds.\n",
      "-- Epoch 800\n",
      "Norm: 24432.47, NNZs: 41, Bias: 2257.013349, T: 319456800, Avg. loss: 53625035.988517\n",
      "Total training time: 66.90 seconds.\n",
      "-- Epoch 801\n",
      "Norm: 24419.86, NNZs: 41, Bias: 2257.094308, T: 319856121, Avg. loss: 53563419.709452\n",
      "Total training time: 66.98 seconds.\n",
      "-- Epoch 802\n",
      "Norm: 24404.88, NNZs: 41, Bias: 2257.128555, T: 320255442, Avg. loss: 53501881.686746\n",
      "Total training time: 67.06 seconds.\n",
      "-- Epoch 803\n",
      "Norm: 24389.58, NNZs: 41, Bias: 2257.151148, T: 320654763, Avg. loss: 53440444.009117\n",
      "Total training time: 67.14 seconds.\n",
      "-- Epoch 804\n",
      "Norm: 24376.34, NNZs: 41, Bias: 2257.224971, T: 321054084, Avg. loss: 53379382.458572\n",
      "Total training time: 67.22 seconds.\n",
      "-- Epoch 805\n",
      "Norm: 24361.95, NNZs: 41, Bias: 2257.273259, T: 321453405, Avg. loss: 53318389.382374\n",
      "Total training time: 67.31 seconds.\n",
      "-- Epoch 806\n",
      "Norm: 24346.77, NNZs: 41, Bias: 2257.305470, T: 321852726, Avg. loss: 53257464.471089\n",
      "Total training time: 67.39 seconds.\n",
      "-- Epoch 807\n",
      "Norm: 24333.11, NNZs: 41, Bias: 2257.356891, T: 322252047, Avg. loss: 53196674.662070\n",
      "Total training time: 67.47 seconds.\n",
      "-- Epoch 808\n",
      "Norm: 24318.47, NNZs: 41, Bias: 2257.407014, T: 322651368, Avg. loss: 53136047.588310\n",
      "Total training time: 67.55 seconds.\n",
      "-- Epoch 809\n",
      "Norm: 24304.21, NNZs: 41, Bias: 2257.449291, T: 323050689, Avg. loss: 53075666.663161\n",
      "Total training time: 67.63 seconds.\n",
      "-- Epoch 810\n",
      "Norm: 24288.20, NNZs: 41, Bias: 2257.488859, T: 323450010, Avg. loss: 53015224.923754\n",
      "Total training time: 67.71 seconds.\n",
      "-- Epoch 811\n",
      "Norm: 24274.56, NNZs: 41, Bias: 2257.541126, T: 323849331, Avg. loss: 52954989.900797\n",
      "Total training time: 67.80 seconds.\n",
      "-- Epoch 812\n",
      "Norm: 24260.94, NNZs: 41, Bias: 2257.613824, T: 324248652, Avg. loss: 52895023.860959\n",
      "Total training time: 67.88 seconds.\n",
      "-- Epoch 813\n",
      "Norm: 24245.92, NNZs: 41, Bias: 2257.653057, T: 324647973, Avg. loss: 52835057.139444\n",
      "Total training time: 67.96 seconds.\n",
      "-- Epoch 814\n",
      "Norm: 24231.41, NNZs: 41, Bias: 2257.675714, T: 325047294, Avg. loss: 52775201.707037\n",
      "Total training time: 68.04 seconds.\n",
      "-- Epoch 815\n",
      "Norm: 24215.84, NNZs: 41, Bias: 2257.677579, T: 325446615, Avg. loss: 52715476.923080\n",
      "Total training time: 68.12 seconds.\n",
      "-- Epoch 816\n",
      "Norm: 24201.01, NNZs: 41, Bias: 2257.721426, T: 325845936, Avg. loss: 52655867.492859\n",
      "Total training time: 68.20 seconds.\n",
      "-- Epoch 817\n",
      "Norm: 24186.49, NNZs: 41, Bias: 2257.766082, T: 326245257, Avg. loss: 52596475.764017\n",
      "Total training time: 68.28 seconds.\n",
      "-- Epoch 818\n",
      "Norm: 24172.29, NNZs: 41, Bias: 2257.803826, T: 326644578, Avg. loss: 52537219.886731\n",
      "Total training time: 68.36 seconds.\n",
      "-- Epoch 819\n",
      "Norm: 24157.56, NNZs: 41, Bias: 2257.851439, T: 327043899, Avg. loss: 52478101.885350\n",
      "Total training time: 68.45 seconds.\n",
      "-- Epoch 820\n",
      "Norm: 24144.04, NNZs: 41, Bias: 2257.891046, T: 327443220, Avg. loss: 52419240.948324\n",
      "Total training time: 68.53 seconds.\n",
      "-- Epoch 821\n",
      "Norm: 24129.76, NNZs: 41, Bias: 2257.957791, T: 327842541, Avg. loss: 52360395.183302\n",
      "Total training time: 68.65 seconds.\n",
      "-- Epoch 822\n",
      "Norm: 24116.59, NNZs: 41, Bias: 2257.995956, T: 328241862, Avg. loss: 52301649.895534\n",
      "Total training time: 68.76 seconds.\n",
      "-- Epoch 823\n",
      "Norm: 24102.27, NNZs: 41, Bias: 2258.027831, T: 328641183, Avg. loss: 52243065.294226\n",
      "Total training time: 68.85 seconds.\n",
      "-- Epoch 824\n",
      "Norm: 24087.93, NNZs: 41, Bias: 2258.035261, T: 329040504, Avg. loss: 52184565.255383\n",
      "Total training time: 68.93 seconds.\n",
      "-- Epoch 825\n",
      "Norm: 24074.69, NNZs: 41, Bias: 2258.062460, T: 329439825, Avg. loss: 52126248.465502\n",
      "Total training time: 69.02 seconds.\n",
      "-- Epoch 826\n",
      "Norm: 24060.30, NNZs: 41, Bias: 2258.107092, T: 329839146, Avg. loss: 52068019.702726\n",
      "Total training time: 69.10 seconds.\n",
      "-- Epoch 827\n",
      "Norm: 24046.23, NNZs: 41, Bias: 2258.147095, T: 330238467, Avg. loss: 52010037.464878\n",
      "Total training time: 69.18 seconds.\n",
      "-- Epoch 828\n",
      "Norm: 24032.01, NNZs: 41, Bias: 2258.177830, T: 330637788, Avg. loss: 51952066.627744\n",
      "Total training time: 69.26 seconds.\n",
      "-- Epoch 829\n",
      "Norm: 24019.26, NNZs: 41, Bias: 2258.213132, T: 331037109, Avg. loss: 51894276.765660\n",
      "Total training time: 69.34 seconds.\n",
      "-- Epoch 830\n",
      "Norm: 24005.41, NNZs: 41, Bias: 2258.228223, T: 331436430, Avg. loss: 51836663.332029\n",
      "Total training time: 69.42 seconds.\n",
      "-- Epoch 831\n",
      "Norm: 23991.67, NNZs: 41, Bias: 2258.240284, T: 331835751, Avg. loss: 51779127.790184\n",
      "Total training time: 69.50 seconds.\n",
      "-- Epoch 832\n",
      "Norm: 23977.81, NNZs: 41, Bias: 2258.282072, T: 332235072, Avg. loss: 51721785.231549\n",
      "Total training time: 69.58 seconds.\n",
      "-- Epoch 833\n",
      "Norm: 23964.50, NNZs: 41, Bias: 2258.325392, T: 332634393, Avg. loss: 51664579.476975\n",
      "Total training time: 69.66 seconds.\n",
      "-- Epoch 834\n",
      "Norm: 23952.41, NNZs: 41, Bias: 2258.375864, T: 333033714, Avg. loss: 51607508.272854\n",
      "Total training time: 69.73 seconds.\n",
      "-- Epoch 835\n",
      "Norm: 23936.77, NNZs: 41, Bias: 2258.377544, T: 333433035, Avg. loss: 51550469.871995\n",
      "Total training time: 69.81 seconds.\n",
      "-- Epoch 836\n",
      "Norm: 23923.06, NNZs: 41, Bias: 2258.423835, T: 333832356, Avg. loss: 51493719.556305\n",
      "Total training time: 69.89 seconds.\n",
      "-- Epoch 837\n",
      "Norm: 23908.89, NNZs: 41, Bias: 2258.451243, T: 334231677, Avg. loss: 51436947.276878\n",
      "Total training time: 69.97 seconds.\n",
      "-- Epoch 838\n",
      "Norm: 23894.55, NNZs: 41, Bias: 2258.465841, T: 334630998, Avg. loss: 51380260.744590\n",
      "Total training time: 70.05 seconds.\n",
      "-- Epoch 839\n",
      "Norm: 23881.69, NNZs: 41, Bias: 2258.534290, T: 335030319, Avg. loss: 51323886.251328\n",
      "Total training time: 70.13 seconds.\n",
      "-- Epoch 840\n",
      "Norm: 23870.89, NNZs: 41, Bias: 2258.603118, T: 335429640, Avg. loss: 51267732.489247\n",
      "Total training time: 70.21 seconds.\n",
      "-- Epoch 841\n",
      "Norm: 23856.51, NNZs: 41, Bias: 2258.638629, T: 335828961, Avg. loss: 51211597.072336\n",
      "Total training time: 70.29 seconds.\n",
      "-- Epoch 842\n",
      "Norm: 23841.99, NNZs: 41, Bias: 2258.638622, T: 336228282, Avg. loss: 51155393.080612\n",
      "Total training time: 70.37 seconds.\n",
      "-- Epoch 843\n",
      "Norm: 23828.35, NNZs: 41, Bias: 2258.666787, T: 336627603, Avg. loss: 51099439.963841\n",
      "Total training time: 70.45 seconds.\n",
      "-- Epoch 844\n",
      "Norm: 23813.77, NNZs: 41, Bias: 2258.684729, T: 337026924, Avg. loss: 51043551.156140\n",
      "Total training time: 70.53 seconds.\n",
      "-- Epoch 845\n",
      "Norm: 23801.00, NNZs: 41, Bias: 2258.749258, T: 337426245, Avg. loss: 50987852.459129\n",
      "Total training time: 70.61 seconds.\n",
      "-- Epoch 846\n",
      "Norm: 23789.28, NNZs: 41, Bias: 2258.816207, T: 337825566, Avg. loss: 50932416.978985\n",
      "Total training time: 70.69 seconds.\n",
      "-- Epoch 847\n",
      "Norm: 23776.54, NNZs: 41, Bias: 2258.843302, T: 338224887, Avg. loss: 50876978.547826\n",
      "Total training time: 70.77 seconds.\n",
      "-- Epoch 848\n",
      "Norm: 23763.74, NNZs: 41, Bias: 2258.934896, T: 338624208, Avg. loss: 50821812.872794\n",
      "Total training time: 70.85 seconds.\n",
      "-- Epoch 849\n",
      "Norm: 23749.78, NNZs: 41, Bias: 2258.962295, T: 339023529, Avg. loss: 50766631.192706\n",
      "Total training time: 70.93 seconds.\n",
      "-- Epoch 850\n",
      "Norm: 23735.84, NNZs: 41, Bias: 2259.019118, T: 339422850, Avg. loss: 50711501.972618\n",
      "Total training time: 71.01 seconds.\n",
      "-- Epoch 851\n",
      "Norm: 23723.34, NNZs: 41, Bias: 2259.071197, T: 339822171, Avg. loss: 50656569.907718\n",
      "Total training time: 71.09 seconds.\n",
      "-- Epoch 852\n",
      "Norm: 23710.72, NNZs: 41, Bias: 2259.107181, T: 340221492, Avg. loss: 50601720.984880\n",
      "Total training time: 71.17 seconds.\n",
      "-- Epoch 853\n",
      "Norm: 23697.80, NNZs: 41, Bias: 2259.147728, T: 340620813, Avg. loss: 50547022.270910\n",
      "Total training time: 71.25 seconds.\n",
      "-- Epoch 854\n",
      "Norm: 23684.23, NNZs: 41, Bias: 2259.183391, T: 341020134, Avg. loss: 50492473.218156\n",
      "Total training time: 71.33 seconds.\n",
      "-- Epoch 855\n",
      "Norm: 23672.01, NNZs: 41, Bias: 2259.220558, T: 341419455, Avg. loss: 50438020.863903\n",
      "Total training time: 71.42 seconds.\n",
      "-- Epoch 856\n",
      "Norm: 23659.73, NNZs: 41, Bias: 2259.272781, T: 341818776, Avg. loss: 50383801.123467\n",
      "Total training time: 71.50 seconds.\n",
      "-- Epoch 857\n",
      "Norm: 23647.18, NNZs: 41, Bias: 2259.312788, T: 342218097, Avg. loss: 50329671.861954\n",
      "Total training time: 71.58 seconds.\n",
      "-- Epoch 858\n",
      "Norm: 23634.78, NNZs: 41, Bias: 2259.383569, T: 342617418, Avg. loss: 50275653.338686\n",
      "Total training time: 71.66 seconds.\n",
      "-- Epoch 859\n",
      "Norm: 23620.02, NNZs: 41, Bias: 2259.394863, T: 343016739, Avg. loss: 50221621.171545\n",
      "Total training time: 71.74 seconds.\n",
      "-- Epoch 860\n",
      "Norm: 23607.30, NNZs: 41, Bias: 2259.466193, T: 343416060, Avg. loss: 50167820.364811\n",
      "Total training time: 71.82 seconds.\n",
      "-- Epoch 861\n",
      "Norm: 23594.04, NNZs: 41, Bias: 2259.512531, T: 343815381, Avg. loss: 50114156.753706\n",
      "Total training time: 71.90 seconds.\n",
      "-- Epoch 862\n",
      "Norm: 23579.67, NNZs: 41, Bias: 2259.529755, T: 344214702, Avg. loss: 50060609.737554\n",
      "Total training time: 71.98 seconds.\n",
      "-- Epoch 863\n",
      "Norm: 23568.32, NNZs: 41, Bias: 2259.562397, T: 344614023, Avg. loss: 50007330.015145\n",
      "Total training time: 72.06 seconds.\n",
      "-- Epoch 864\n",
      "Norm: 23555.12, NNZs: 41, Bias: 2259.592790, T: 345013344, Avg. loss: 49954008.478134\n",
      "Total training time: 72.14 seconds.\n",
      "-- Epoch 865\n",
      "Norm: 23541.96, NNZs: 41, Bias: 2259.664759, T: 345412665, Avg. loss: 49900676.880148\n",
      "Total training time: 72.22 seconds.\n",
      "-- Epoch 866\n",
      "Norm: 23529.62, NNZs: 41, Bias: 2259.728914, T: 345811986, Avg. loss: 49847652.191856\n",
      "Total training time: 72.30 seconds.\n",
      "-- Epoch 867\n",
      "Norm: 23516.43, NNZs: 41, Bias: 2259.758156, T: 346211307, Avg. loss: 49794587.436898\n",
      "Total training time: 72.38 seconds.\n",
      "-- Epoch 868\n",
      "Norm: 23503.51, NNZs: 41, Bias: 2259.815992, T: 346610628, Avg. loss: 49741691.258460\n",
      "Total training time: 72.46 seconds.\n",
      "-- Epoch 869\n",
      "Norm: 23489.93, NNZs: 41, Bias: 2259.843323, T: 347009949, Avg. loss: 49688947.769522\n",
      "Total training time: 72.54 seconds.\n",
      "-- Epoch 870\n",
      "Norm: 23477.97, NNZs: 41, Bias: 2259.891941, T: 347409270, Avg. loss: 49636269.031859\n",
      "Total training time: 72.62 seconds.\n",
      "-- Epoch 871\n",
      "Norm: 23466.50, NNZs: 41, Bias: 2259.946259, T: 347808591, Avg. loss: 49583857.080963\n",
      "Total training time: 72.70 seconds.\n",
      "-- Epoch 872\n",
      "Norm: 23452.95, NNZs: 41, Bias: 2259.984766, T: 348207912, Avg. loss: 49531370.071564\n",
      "Total training time: 72.78 seconds.\n",
      "-- Epoch 873\n",
      "Norm: 23439.73, NNZs: 41, Bias: 2260.054085, T: 348607233, Avg. loss: 49479104.623555\n",
      "Total training time: 72.86 seconds.\n",
      "-- Epoch 874\n",
      "Norm: 23426.91, NNZs: 41, Bias: 2260.084344, T: 349006554, Avg. loss: 49426967.979093\n",
      "Total training time: 72.94 seconds.\n",
      "-- Epoch 875\n",
      "Norm: 23414.41, NNZs: 41, Bias: 2260.104041, T: 349405875, Avg. loss: 49374951.131284\n",
      "Total training time: 73.02 seconds.\n",
      "-- Epoch 876\n",
      "Norm: 23401.87, NNZs: 41, Bias: 2260.161147, T: 349805196, Avg. loss: 49323055.384140\n",
      "Total training time: 73.10 seconds.\n",
      "-- Epoch 877\n",
      "Norm: 23389.80, NNZs: 41, Bias: 2260.192822, T: 350204517, Avg. loss: 49271209.645194\n",
      "Total training time: 73.18 seconds.\n",
      "-- Epoch 878\n",
      "Norm: 23375.73, NNZs: 41, Bias: 2260.181183, T: 350603838, Avg. loss: 49219399.944968\n",
      "Total training time: 73.26 seconds.\n",
      "-- Epoch 879\n",
      "Norm: 23363.39, NNZs: 41, Bias: 2260.220855, T: 351003159, Avg. loss: 49167775.694285\n",
      "Total training time: 73.34 seconds.\n",
      "-- Epoch 880\n",
      "Norm: 23351.23, NNZs: 41, Bias: 2260.252533, T: 351402480, Avg. loss: 49116131.251158\n",
      "Total training time: 73.42 seconds.\n",
      "-- Epoch 881\n",
      "Norm: 23336.95, NNZs: 41, Bias: 2260.285970, T: 351801801, Avg. loss: 49064659.494315\n",
      "Total training time: 73.50 seconds.\n",
      "-- Epoch 882\n",
      "Norm: 23324.34, NNZs: 41, Bias: 2260.300414, T: 352201122, Avg. loss: 49013337.403785\n",
      "Total training time: 73.58 seconds.\n",
      "-- Epoch 883\n",
      "Norm: 23311.55, NNZs: 41, Bias: 2260.352055, T: 352600443, Avg. loss: 48962191.818894\n",
      "Total training time: 73.66 seconds.\n",
      "-- Epoch 884\n",
      "Norm: 23297.67, NNZs: 41, Bias: 2260.372345, T: 352999764, Avg. loss: 48911027.020471\n",
      "Total training time: 73.74 seconds.\n",
      "-- Epoch 885\n",
      "Norm: 23285.05, NNZs: 41, Bias: 2260.434410, T: 353399085, Avg. loss: 48860108.085455\n",
      "Total training time: 73.82 seconds.\n",
      "-- Epoch 886\n",
      "Norm: 23274.09, NNZs: 41, Bias: 2260.475231, T: 353798406, Avg. loss: 48809268.455924\n",
      "Total training time: 73.90 seconds.\n",
      "-- Epoch 887\n",
      "Norm: 23261.25, NNZs: 41, Bias: 2260.520191, T: 354197727, Avg. loss: 48758505.281721\n",
      "Total training time: 73.98 seconds.\n",
      "-- Epoch 888\n",
      "Norm: 23247.41, NNZs: 41, Bias: 2260.523018, T: 354597048, Avg. loss: 48707776.929266\n",
      "Total training time: 74.06 seconds.\n",
      "-- Epoch 889\n",
      "Norm: 23234.91, NNZs: 41, Bias: 2260.559652, T: 354996369, Avg. loss: 48657212.095787\n",
      "Total training time: 74.14 seconds.\n",
      "-- Epoch 890\n",
      "Norm: 23222.28, NNZs: 41, Bias: 2260.608408, T: 355395690, Avg. loss: 48606868.526808\n",
      "Total training time: 74.22 seconds.\n",
      "-- Epoch 891\n",
      "Norm: 23211.00, NNZs: 41, Bias: 2260.643604, T: 355795011, Avg. loss: 48556516.534375\n",
      "Total training time: 74.30 seconds.\n",
      "-- Epoch 892\n",
      "Norm: 23200.27, NNZs: 41, Bias: 2260.700897, T: 356194332, Avg. loss: 48506378.761932\n",
      "Total training time: 74.38 seconds.\n",
      "-- Epoch 893\n",
      "Norm: 23187.42, NNZs: 41, Bias: 2260.695957, T: 356593653, Avg. loss: 48456249.940967\n",
      "Total training time: 74.46 seconds.\n",
      "-- Epoch 894\n",
      "Norm: 23176.06, NNZs: 41, Bias: 2260.752457, T: 356992974, Avg. loss: 48406440.305520\n",
      "Total training time: 74.54 seconds.\n",
      "-- Epoch 895\n",
      "Norm: 23163.71, NNZs: 41, Bias: 2260.768133, T: 357392295, Avg. loss: 48356547.072248\n",
      "Total training time: 74.62 seconds.\n",
      "-- Epoch 896\n",
      "Norm: 23151.83, NNZs: 41, Bias: 2260.804711, T: 357791616, Avg. loss: 48306910.713925\n",
      "Total training time: 74.70 seconds.\n",
      "-- Epoch 897\n",
      "Norm: 23138.96, NNZs: 41, Bias: 2260.841599, T: 358190937, Avg. loss: 48257271.773601\n",
      "Total training time: 74.78 seconds.\n",
      "-- Epoch 898\n",
      "Norm: 23126.92, NNZs: 41, Bias: 2260.870158, T: 358590258, Avg. loss: 48207735.735825\n",
      "Total training time: 74.86 seconds.\n",
      "-- Epoch 899\n",
      "Norm: 23114.95, NNZs: 41, Bias: 2260.908398, T: 358989579, Avg. loss: 48158337.117509\n",
      "Total training time: 74.94 seconds.\n",
      "-- Epoch 900\n",
      "Norm: 23102.46, NNZs: 41, Bias: 2260.942247, T: 359388900, Avg. loss: 48108966.414057\n",
      "Total training time: 75.02 seconds.\n",
      "-- Epoch 901\n",
      "Norm: 23090.73, NNZs: 41, Bias: 2260.958944, T: 359788221, Avg. loss: 48059718.631869\n",
      "Total training time: 75.10 seconds.\n",
      "-- Epoch 902\n",
      "Norm: 23077.64, NNZs: 41, Bias: 2260.981284, T: 360187542, Avg. loss: 48010487.918855\n",
      "Total training time: 75.18 seconds.\n",
      "-- Epoch 903\n",
      "Norm: 23064.70, NNZs: 41, Bias: 2261.014264, T: 360586863, Avg. loss: 47961427.206666\n",
      "Total training time: 75.26 seconds.\n",
      "-- Epoch 904\n",
      "Norm: 23053.53, NNZs: 41, Bias: 2261.070738, T: 360986184, Avg. loss: 47912611.759109\n",
      "Total training time: 75.34 seconds.\n",
      "-- Epoch 905\n",
      "Norm: 23041.65, NNZs: 41, Bias: 2261.094207, T: 361385505, Avg. loss: 47863751.921588\n",
      "Total training time: 75.42 seconds.\n",
      "-- Epoch 906\n",
      "Norm: 23029.00, NNZs: 41, Bias: 2261.152506, T: 361784826, Avg. loss: 47815042.324425\n",
      "Total training time: 75.50 seconds.\n",
      "-- Epoch 907\n",
      "Norm: 23016.04, NNZs: 41, Bias: 2261.170074, T: 362184147, Avg. loss: 47766364.749293\n",
      "Total training time: 75.58 seconds.\n",
      "-- Epoch 908\n",
      "Norm: 23005.77, NNZs: 41, Bias: 2261.223938, T: 362583468, Avg. loss: 47717890.714745\n",
      "Total training time: 75.66 seconds.\n",
      "-- Epoch 909\n",
      "Norm: 22995.53, NNZs: 41, Bias: 2261.281284, T: 362982789, Avg. loss: 47669572.928594\n",
      "Total training time: 75.74 seconds.\n",
      "-- Epoch 910\n",
      "Norm: 22982.41, NNZs: 41, Bias: 2261.291985, T: 363382110, Avg. loss: 47621153.847871\n",
      "Total training time: 75.82 seconds.\n",
      "-- Epoch 911\n",
      "Norm: 22970.66, NNZs: 41, Bias: 2261.322586, T: 363781431, Avg. loss: 47572932.812004\n",
      "Total training time: 75.90 seconds.\n",
      "-- Epoch 912\n",
      "Norm: 22958.63, NNZs: 41, Bias: 2261.354902, T: 364180752, Avg. loss: 47524811.195656\n",
      "Total training time: 75.98 seconds.\n",
      "-- Epoch 913\n",
      "Norm: 22946.40, NNZs: 41, Bias: 2261.372136, T: 364580073, Avg. loss: 47476699.740271\n",
      "Total training time: 76.06 seconds.\n",
      "-- Epoch 914\n",
      "Norm: 22935.21, NNZs: 41, Bias: 2261.424772, T: 364979394, Avg. loss: 47428827.089242\n",
      "Total training time: 76.14 seconds.\n",
      "-- Epoch 915\n",
      "Norm: 22923.54, NNZs: 41, Bias: 2261.484579, T: 365378715, Avg. loss: 47381097.293463\n",
      "Total training time: 76.22 seconds.\n",
      "-- Epoch 916\n",
      "Norm: 22911.21, NNZs: 41, Bias: 2261.492346, T: 365778036, Avg. loss: 47333372.692526\n",
      "Total training time: 76.30 seconds.\n",
      "-- Epoch 917\n",
      "Norm: 22901.00, NNZs: 41, Bias: 2261.546776, T: 366177357, Avg. loss: 47285847.903179\n",
      "Total training time: 76.38 seconds.\n",
      "-- Epoch 918\n",
      "Norm: 22890.46, NNZs: 41, Bias: 2261.582381, T: 366576678, Avg. loss: 47238520.391118\n",
      "Total training time: 76.46 seconds.\n",
      "-- Epoch 919\n",
      "Norm: 22879.13, NNZs: 41, Bias: 2261.638559, T: 366975999, Avg. loss: 47191222.972369\n",
      "Total training time: 76.54 seconds.\n",
      "-- Epoch 920\n",
      "Norm: 22868.13, NNZs: 41, Bias: 2261.666986, T: 367375320, Avg. loss: 47143921.860289\n",
      "Total training time: 76.62 seconds.\n",
      "-- Epoch 921\n",
      "Norm: 22857.19, NNZs: 41, Bias: 2261.709975, T: 367774641, Avg. loss: 47096728.970758\n",
      "Total training time: 76.70 seconds.\n",
      "-- Epoch 922\n",
      "Norm: 22844.79, NNZs: 41, Bias: 2261.739652, T: 368173962, Avg. loss: 47049576.724661\n",
      "Total training time: 76.78 seconds.\n",
      "-- Epoch 923\n",
      "Norm: 22833.57, NNZs: 41, Bias: 2261.761052, T: 368573283, Avg. loss: 47002622.933806\n",
      "Total training time: 76.86 seconds.\n",
      "-- Epoch 924\n",
      "Norm: 22821.49, NNZs: 41, Bias: 2261.806288, T: 368972604, Avg. loss: 46955650.306558\n",
      "Total training time: 76.94 seconds.\n",
      "-- Epoch 925\n",
      "Norm: 22810.86, NNZs: 41, Bias: 2261.846051, T: 369371925, Avg. loss: 46908904.803348\n",
      "Total training time: 77.02 seconds.\n",
      "-- Epoch 926\n",
      "Norm: 22798.45, NNZs: 41, Bias: 2261.862665, T: 369771246, Avg. loss: 46862114.896951\n",
      "Total training time: 77.10 seconds.\n",
      "-- Epoch 927\n",
      "Norm: 22786.28, NNZs: 41, Bias: 2261.898130, T: 370170567, Avg. loss: 46815503.686642\n",
      "Total training time: 77.18 seconds.\n",
      "-- Epoch 928\n",
      "Norm: 22774.25, NNZs: 41, Bias: 2261.923611, T: 370569888, Avg. loss: 46768952.520674\n",
      "Total training time: 77.26 seconds.\n",
      "-- Epoch 929\n",
      "Norm: 22762.76, NNZs: 41, Bias: 2261.939367, T: 370969209, Avg. loss: 46722503.515610\n",
      "Total training time: 77.34 seconds.\n",
      "-- Epoch 930\n",
      "Norm: 22750.47, NNZs: 41, Bias: 2261.987208, T: 371368530, Avg. loss: 46676167.824587\n",
      "Total training time: 77.41 seconds.\n",
      "-- Epoch 931\n",
      "Norm: 22738.62, NNZs: 41, Bias: 2262.023903, T: 371767851, Avg. loss: 46629905.664704\n",
      "Total training time: 77.49 seconds.\n",
      "-- Epoch 932\n",
      "Norm: 22728.01, NNZs: 41, Bias: 2262.056483, T: 372167172, Avg. loss: 46583742.388761\n",
      "Total training time: 77.57 seconds.\n",
      "-- Epoch 933\n",
      "Norm: 22715.57, NNZs: 41, Bias: 2262.080568, T: 372566493, Avg. loss: 46537575.570597\n",
      "Total training time: 77.65 seconds.\n",
      "-- Epoch 934\n",
      "Norm: 22704.49, NNZs: 41, Bias: 2262.095589, T: 372965814, Avg. loss: 46491575.839998\n",
      "Total training time: 77.73 seconds.\n",
      "-- Epoch 935\n",
      "Norm: 22692.39, NNZs: 41, Bias: 2262.126568, T: 373365135, Avg. loss: 46445653.622719\n",
      "Total training time: 77.81 seconds.\n",
      "-- Epoch 936\n",
      "Norm: 22680.26, NNZs: 41, Bias: 2262.169194, T: 373764456, Avg. loss: 46399865.239021\n",
      "Total training time: 77.89 seconds.\n",
      "-- Epoch 937\n",
      "Norm: 22668.17, NNZs: 41, Bias: 2262.209720, T: 374163777, Avg. loss: 46354035.121016\n",
      "Total training time: 77.97 seconds.\n",
      "-- Epoch 938\n",
      "Norm: 22656.23, NNZs: 41, Bias: 2262.240862, T: 374563098, Avg. loss: 46308451.614643\n",
      "Total training time: 78.05 seconds.\n",
      "-- Epoch 939\n",
      "Norm: 22644.85, NNZs: 41, Bias: 2262.257104, T: 374962419, Avg. loss: 46262970.985130\n",
      "Total training time: 78.13 seconds.\n",
      "-- Epoch 940\n",
      "Norm: 22634.12, NNZs: 41, Bias: 2262.305705, T: 375361740, Avg. loss: 46217620.504979\n",
      "Total training time: 78.21 seconds.\n",
      "-- Epoch 941\n",
      "Norm: 22622.76, NNZs: 41, Bias: 2262.331369, T: 375761061, Avg. loss: 46172266.325105\n",
      "Total training time: 78.29 seconds.\n",
      "-- Epoch 942\n",
      "Norm: 22612.25, NNZs: 41, Bias: 2262.369032, T: 376160382, Avg. loss: 46127078.913000\n",
      "Total training time: 78.37 seconds.\n",
      "-- Epoch 943\n",
      "Norm: 22601.05, NNZs: 41, Bias: 2262.390713, T: 376559703, Avg. loss: 46081969.276962\n",
      "Total training time: 78.45 seconds.\n",
      "-- Epoch 944\n",
      "Norm: 22591.78, NNZs: 41, Bias: 2262.468319, T: 376959024, Avg. loss: 46036997.705757\n",
      "Total training time: 78.53 seconds.\n",
      "-- Epoch 945\n",
      "Norm: 22580.41, NNZs: 41, Bias: 2262.471607, T: 377358345, Avg. loss: 45991999.178979\n",
      "Total training time: 78.61 seconds.\n",
      "-- Epoch 946\n",
      "Norm: 22569.06, NNZs: 41, Bias: 2262.518840, T: 377757666, Avg. loss: 45947194.671216\n",
      "Total training time: 78.69 seconds.\n",
      "-- Epoch 947\n",
      "Norm: 22558.01, NNZs: 41, Bias: 2262.544981, T: 378156987, Avg. loss: 45902370.010845\n",
      "Total training time: 78.77 seconds.\n",
      "-- Epoch 948\n",
      "Norm: 22546.24, NNZs: 41, Bias: 2262.594150, T: 378556308, Avg. loss: 45857767.219608\n",
      "Total training time: 78.85 seconds.\n",
      "-- Epoch 949\n",
      "Norm: 22534.53, NNZs: 41, Bias: 2262.628691, T: 378955629, Avg. loss: 45813164.989388\n",
      "Total training time: 78.93 seconds.\n",
      "-- Epoch 950\n",
      "Norm: 22523.39, NNZs: 41, Bias: 2262.678783, T: 379354950, Avg. loss: 45768703.086931\n",
      "Total training time: 79.01 seconds.\n",
      "-- Epoch 951\n",
      "Norm: 22511.68, NNZs: 41, Bias: 2262.710718, T: 379754271, Avg. loss: 45724207.159156\n",
      "Total training time: 79.09 seconds.\n",
      "-- Epoch 952\n",
      "Norm: 22501.13, NNZs: 41, Bias: 2262.753060, T: 380153592, Avg. loss: 45679935.814547\n",
      "Total training time: 79.17 seconds.\n",
      "-- Epoch 953\n",
      "Norm: 22491.41, NNZs: 41, Bias: 2262.795232, T: 380552913, Avg. loss: 45635838.460203\n",
      "Total training time: 79.25 seconds.\n",
      "-- Epoch 954\n",
      "Norm: 22479.95, NNZs: 41, Bias: 2262.825809, T: 380952234, Avg. loss: 45591757.180287\n",
      "Total training time: 79.33 seconds.\n",
      "-- Epoch 955\n",
      "Norm: 22468.92, NNZs: 41, Bias: 2262.850580, T: 381351555, Avg. loss: 45547662.141770\n",
      "Total training time: 79.41 seconds.\n",
      "-- Epoch 956\n",
      "Norm: 22458.35, NNZs: 41, Bias: 2262.875362, T: 381750876, Avg. loss: 45503695.328422\n",
      "Total training time: 79.49 seconds.\n",
      "-- Epoch 957\n",
      "Norm: 22447.21, NNZs: 41, Bias: 2262.921015, T: 382150197, Avg. loss: 45459754.545377\n",
      "Total training time: 79.57 seconds.\n",
      "-- Epoch 958\n",
      "Norm: 22435.63, NNZs: 41, Bias: 2262.961482, T: 382549518, Avg. loss: 45416012.510067\n",
      "Total training time: 79.65 seconds.\n",
      "-- Epoch 959\n",
      "Norm: 22425.42, NNZs: 41, Bias: 2263.005274, T: 382948839, Avg. loss: 45372424.407740\n",
      "Total training time: 79.73 seconds.\n",
      "-- Epoch 960\n",
      "Norm: 22414.52, NNZs: 41, Bias: 2263.041383, T: 383348160, Avg. loss: 45328765.665648\n",
      "Total training time: 79.81 seconds.\n",
      "-- Epoch 961\n",
      "Norm: 22403.25, NNZs: 41, Bias: 2263.057247, T: 383747481, Avg. loss: 45285249.769501\n",
      "Total training time: 79.89 seconds.\n",
      "-- Epoch 962\n",
      "Norm: 22392.61, NNZs: 41, Bias: 2263.090272, T: 384146802, Avg. loss: 45241808.002427\n",
      "Total training time: 79.97 seconds.\n",
      "-- Epoch 963\n",
      "Norm: 22381.99, NNZs: 41, Bias: 2263.132538, T: 384546123, Avg. loss: 45198550.890771\n",
      "Total training time: 80.05 seconds.\n",
      "-- Epoch 964\n",
      "Norm: 22369.47, NNZs: 41, Bias: 2263.130787, T: 384945444, Avg. loss: 45155223.767884\n",
      "Total training time: 80.13 seconds.\n",
      "-- Epoch 965\n",
      "Norm: 22358.21, NNZs: 41, Bias: 2263.154993, T: 385344765, Avg. loss: 45112032.710285\n",
      "Total training time: 80.21 seconds.\n",
      "-- Epoch 966\n",
      "Norm: 22348.00, NNZs: 41, Bias: 2263.201985, T: 385744086, Avg. loss: 45068997.292163\n",
      "Total training time: 80.29 seconds.\n",
      "-- Epoch 967\n",
      "Norm: 22337.57, NNZs: 41, Bias: 2263.252249, T: 386143407, Avg. loss: 45025981.720568\n",
      "Total training time: 80.37 seconds.\n",
      "-- Epoch 968\n",
      "Norm: 22327.03, NNZs: 41, Bias: 2263.291798, T: 386542728, Avg. loss: 44983103.065147\n",
      "Total training time: 80.45 seconds.\n",
      "-- Epoch 969\n",
      "Norm: 22315.28, NNZs: 41, Bias: 2263.307013, T: 386942049, Avg. loss: 44940243.845940\n",
      "Total training time: 80.53 seconds.\n",
      "-- Epoch 970\n",
      "Norm: 22304.62, NNZs: 41, Bias: 2263.313631, T: 387341370, Avg. loss: 44897543.802112\n",
      "Total training time: 80.61 seconds.\n",
      "-- Epoch 971\n",
      "Norm: 22293.81, NNZs: 41, Bias: 2263.335407, T: 387740691, Avg. loss: 44854888.762705\n",
      "Total training time: 80.69 seconds.\n",
      "-- Epoch 972\n",
      "Norm: 22283.21, NNZs: 41, Bias: 2263.377671, T: 388140012, Avg. loss: 44812379.745127\n",
      "Total training time: 80.77 seconds.\n",
      "-- Epoch 973\n",
      "Norm: 22273.62, NNZs: 41, Bias: 2263.413073, T: 388539333, Avg. loss: 44769964.735392\n",
      "Total training time: 80.85 seconds.\n",
      "-- Epoch 974\n",
      "Norm: 22262.65, NNZs: 41, Bias: 2263.436436, T: 388938654, Avg. loss: 44727598.385884\n",
      "Total training time: 80.93 seconds.\n",
      "-- Epoch 975\n",
      "Norm: 22252.25, NNZs: 41, Bias: 2263.453908, T: 389337975, Avg. loss: 44685168.274347\n",
      "Total training time: 81.01 seconds.\n",
      "-- Epoch 976\n",
      "Norm: 22242.09, NNZs: 41, Bias: 2263.497860, T: 389737296, Avg. loss: 44642936.674694\n",
      "Total training time: 81.09 seconds.\n",
      "-- Epoch 977\n",
      "Norm: 22232.54, NNZs: 41, Bias: 2263.557355, T: 390136617, Avg. loss: 44600807.972674\n",
      "Total training time: 81.17 seconds.\n",
      "-- Epoch 978\n",
      "Norm: 22221.91, NNZs: 41, Bias: 2263.598981, T: 390535938, Avg. loss: 44558749.659424\n",
      "Total training time: 81.24 seconds.\n",
      "-- Epoch 979\n",
      "Norm: 22211.00, NNZs: 41, Bias: 2263.613112, T: 390935259, Avg. loss: 44516751.681003\n",
      "Total training time: 81.32 seconds.\n",
      "-- Epoch 980\n",
      "Norm: 22200.65, NNZs: 41, Bias: 2263.626516, T: 391334580, Avg. loss: 44474730.510782\n",
      "Total training time: 81.40 seconds.\n",
      "-- Epoch 981\n",
      "Norm: 22191.18, NNZs: 41, Bias: 2263.684748, T: 391733901, Avg. loss: 44432922.459187\n",
      "Total training time: 81.48 seconds.\n",
      "-- Epoch 982\n",
      "Norm: 22179.83, NNZs: 41, Bias: 2263.703023, T: 392133222, Avg. loss: 44391149.583551\n",
      "Total training time: 81.56 seconds.\n",
      "-- Epoch 983\n",
      "Norm: 22170.25, NNZs: 41, Bias: 2263.760623, T: 392532543, Avg. loss: 44349589.250382\n",
      "Total training time: 81.64 seconds.\n",
      "-- Epoch 984\n",
      "Norm: 22160.81, NNZs: 41, Bias: 2263.796473, T: 392931864, Avg. loss: 44308155.020957\n",
      "Total training time: 81.72 seconds.\n",
      "-- Epoch 985\n",
      "Norm: 22150.41, NNZs: 41, Bias: 2263.810414, T: 393331185, Avg. loss: 44266669.365381\n",
      "Total training time: 81.80 seconds.\n",
      "-- Epoch 986\n",
      "Norm: 22140.23, NNZs: 41, Bias: 2263.835723, T: 393730506, Avg. loss: 44225318.783144\n",
      "Total training time: 81.88 seconds.\n",
      "-- Epoch 987\n",
      "Norm: 22130.06, NNZs: 41, Bias: 2263.876967, T: 394129827, Avg. loss: 44183998.314695\n",
      "Total training time: 81.96 seconds.\n",
      "-- Epoch 988\n",
      "Norm: 22119.78, NNZs: 41, Bias: 2263.938324, T: 394529148, Avg. loss: 44142866.908316\n",
      "Total training time: 82.04 seconds.\n",
      "-- Epoch 989\n",
      "Norm: 22108.55, NNZs: 41, Bias: 2263.971672, T: 394928469, Avg. loss: 44101668.450710\n",
      "Total training time: 82.12 seconds.\n",
      "-- Epoch 990\n",
      "Norm: 22096.98, NNZs: 41, Bias: 2263.975610, T: 395327790, Avg. loss: 44060461.573858\n",
      "Total training time: 82.20 seconds.\n",
      "-- Epoch 991\n",
      "Norm: 22085.96, NNZs: 41, Bias: 2264.007066, T: 395727111, Avg. loss: 44019441.126471\n",
      "Total training time: 82.28 seconds.\n",
      "-- Epoch 992\n",
      "Norm: 22075.48, NNZs: 41, Bias: 2264.030901, T: 396126432, Avg. loss: 43978553.183812\n",
      "Total training time: 82.36 seconds.\n",
      "-- Epoch 993\n",
      "Norm: 22065.97, NNZs: 41, Bias: 2264.051076, T: 396525753, Avg. loss: 43937742.598137\n",
      "Total training time: 82.44 seconds.\n",
      "-- Epoch 994\n",
      "Norm: 22055.71, NNZs: 41, Bias: 2264.095944, T: 396925074, Avg. loss: 43896989.665267\n",
      "Total training time: 82.52 seconds.\n",
      "-- Epoch 995\n",
      "Norm: 22045.44, NNZs: 41, Bias: 2264.118801, T: 397324395, Avg. loss: 43856308.751018\n",
      "Total training time: 82.60 seconds.\n",
      "-- Epoch 996\n",
      "Norm: 22033.81, NNZs: 41, Bias: 2264.120201, T: 397723716, Avg. loss: 43815552.010275\n",
      "Total training time: 82.68 seconds.\n",
      "-- Epoch 997\n",
      "Norm: 22022.64, NNZs: 41, Bias: 2264.147241, T: 398123037, Avg. loss: 43774990.510285\n",
      "Total training time: 82.76 seconds.\n",
      "-- Epoch 998\n",
      "Norm: 22012.04, NNZs: 41, Bias: 2264.187728, T: 398522358, Avg. loss: 43734483.877515\n",
      "Total training time: 82.84 seconds.\n",
      "-- Epoch 999\n",
      "Norm: 22001.55, NNZs: 41, Bias: 2264.179404, T: 398921679, Avg. loss: 43694055.887250\n",
      "Total training time: 82.92 seconds.\n",
      "-- Epoch 1000\n",
      "Norm: 21991.55, NNZs: 41, Bias: 2264.205763, T: 399321000, Avg. loss: 43653832.926655\n",
      "Total training time: 83.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='modified_huber', n_iter=1000,\n",
       "       n_jobs=-1, penalty='l2', power_t=0.5, random_state=None,\n",
       "       shuffle=True, verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd = SGDClassifier(loss='modified_huber', verbose=2, n_jobs=-1, n_iter=1000)\n",
    "sgd.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72123930371806144"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembled Model #4: AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/utils/validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc = AdaBoostClassifier()\n",
    "abc.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83326947493369996"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembled Model #5: Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_estimators=50 .................................................\n",
      "[CV] n_estimators=100 ................................................\n",
      "[CV] n_estimators=150 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_estimators=200 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_estimators=250 ................................................\n",
      "[CV] n_estimators=300 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_estimators=350 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_estimators=400 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_estimators=450 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_estimators=500 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_estimators=50 .................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_estimators=100 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_estimators=150 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_estimators=200 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_estimators=250 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_estimators=300 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_estimators=350 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_estimators=400 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_estimators=450 ................................................\n",
      "[CV] n_estimators=500 ................................................\n",
      "[CV] n_estimators=50 .................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_estimators=100 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_estimators=150 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_estimators=200 ................................................\n",
      "[CV] n_estimators=250 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_estimators=300 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_estimators=350 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_estimators=400 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_estimators=450 ................................................\n",
      "[CV] n_estimators=500 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "Process ForkPoolWorker-312:\n",
      "Process ForkPoolWorker-310:\n",
      "Process ForkPoolWorker-311:\n",
      "Process ForkPoolWorker-308:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-309:\n",
      "  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-307:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-749f4a63370f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m bc_train_scores, bc_valid_scores = validation_curve(BaggingClassifier(n_jobs=-1), X_train, Y_train.values.reshape(-1), \n\u001b[0;32m----> 4\u001b[0;31m                                              'n_estimators', np.arange(50,550,50), n_jobs=-1, verbose=2)\n\u001b[0m",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mvalidation_curve\u001b[0;34m(estimator, X, y, param_name, param_range, groups, cv, scoring, n_jobs, pre_dispatch, verbose)\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m         parameters={param_name: v}, fit_params=None, return_train_score=True)\n\u001b[0;32m--> 954\u001b[0;31m         for train, test in cv.split(X, y, groups) for v in param_range)\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    766\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0;31m# check if timeout supported in backend future implementation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'timeout'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "bc_train_scores, bc_valid_scores = validation_curve(BaggingClassifier(n_jobs=-1), X_train, Y_train.values.reshape(-1), \n",
    "                                             'n_estimators', np.arange(50,550,50), n_jobs=-1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bc_train_scores_mean = np.mean(bc_train_scores, axis=1)\n",
    "bc_train_scores_std = np.std(bc_train_scores, axis=1)\n",
    "bc_valid_scores_mean = np.mean(bc_valid_scores, axis=1)\n",
    "bc_valid_scores_std = np.std(bc_valid_scores, axis=1)\n",
    "bc_param_range = np.arange(50,550,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEXCAYAAABcRGizAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSExKFiIiEpOShYiIxPT/AQqrmL4mshYhAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f034f2e0dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Validation Curve with Estimators\")\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"Score\")\n",
    "lw=2\n",
    "plt.plot(bc_param_range, bc_train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.plot(bc_param_range, bc_valid_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/utils/validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "Building estimator 1 of 5 for this parallel run (total 200)...\n",
      "Building estimator 1 of 5 for this parallel run (total 200)...\n",
      "Building estimator 1 of 5 for this parallel run (total 200)...\n",
      "Building estimator 1 of 5 for this parallel run (total 200)...\n",
      "Building estimator 1 of 5 for this parallel run (total 200)...\n",
      "Building estimator 1 of 5 for this parallel run (total 200)...\n",
      "Building estimator 1 of 5 for this parallel run (total 200)...\n",
      "Building estimator 1 of 5 for this parallel run (total 200)...\n",
      "Building estimator 1 of 5 for this parallel run (total 200)...\n",
      "Building estimator 1 of 5 for this parallel run (total 200)...\n",
      "Building estimator 1 of 5 for this parallel run (total 200)...\n",
      "Building estimator 1 of 5 for this parallel run (total 200)...\n",
      "Building estimator 1 of 5 for this parallel run (total 200)...\n",
      "Building estimator 1 of 5 for this parallel run (total 200)...\n",
      "Building estimator 1 of 5 for this parallel run (total 200)...\n",
      "Building estimator 1 of 5 for this parallel run (total 200)...\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "Building estimator 2 of 5 for this parallel run (total 200)...\n",
      "Building estimator 2 of 5 for this parallel run (total 200)...\n",
      "Building estimator 2 of 5 for this parallel run (total 200)...\n",
      "Building estimator 2 of 5 for this parallel run (total 200)...\n",
      "Building estimator 2 of 5 for this parallel run (total 200)...\n",
      "Building estimator 2 of 5 for this parallel run (total 200)...\n",
      "Building estimator 2 of 5 for this parallel run (total 200)...\n",
      "Building estimator 2 of 5 for this parallel run (total 200)...\n",
      "Building estimator 2 of 5 for this parallel run (total 200)...\n",
      "Building estimator 2 of 5 for this parallel run (total 200)...\n",
      "Building estimator 2 of 5 for this parallel run (total 200)...\n",
      "Building estimator 2 of 5 for this parallel run (total 200)...\n",
      "Building estimator 2 of 5 for this parallel run (total 200)...\n",
      "Building estimator 2 of 5 for this parallel run (total 200)...\n",
      "Building estimator 2 of 5 for this parallel run (total 200)...\n",
      "Building estimator 2 of 5 for this parallel run (total 200)...\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "Building estimator 3 of 5 for this parallel run (total 200)...\n",
      "Building estimator 3 of 5 for this parallel run (total 200)...\n",
      "Building estimator 3 of 5 for this parallel run (total 200)...\n",
      "Building estimator 3 of 5 for this parallel run (total 200)...\n",
      "Building estimator 3 of 5 for this parallel run (total 200)...\n",
      "Building estimator 3 of 5 for this parallel run (total 200)...\n",
      "Building estimator 3 of 5 for this parallel run (total 200)...\n",
      "Building estimator 3 of 5 for this parallel run (total 200)...\n",
      "Building estimator 3 of 5 for this parallel run (total 200)...\n",
      "Building estimator 3 of 5 for this parallel run (total 200)...\n",
      "Building estimator 3 of 5 for this parallel run (total 200)...\n",
      "Building estimator 3 of 5 for this parallel run (total 200)...\n",
      "Building estimator 3 of 5 for this parallel run (total 200)...\n",
      "Building estimator 3 of 5 for this parallel run (total 200)...\n",
      "Building estimator 3 of 5 for this parallel run (total 200)...\n",
      "Building estimator 3 of 5 for this parallel run (total 200)...\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "Building estimator 4 of 5 for this parallel run (total 200)...\n",
      "Building estimator 4 of 5 for this parallel run (total 200)...\n",
      "Building estimator 4 of 5 for this parallel run (total 200)...\n",
      "Building estimator 4 of 5 for this parallel run (total 200)...\n",
      "Building estimator 4 of 5 for this parallel run (total 200)...\n",
      "Building estimator 4 of 5 for this parallel run (total 200)...\n",
      "Building estimator 4 of 5 for this parallel run (total 200)...\n",
      "Building estimator 4 of 5 for this parallel run (total 200)...\n",
      "Building estimator 4 of 5 for this parallel run (total 200)...\n",
      "Building estimator 4 of 5 for this parallel run (total 200)...\n",
      "Building estimator 4 of 5 for this parallel run (total 200)...\n",
      "Building estimator 4 of 5 for this parallel run (total 200)...\n",
      "Building estimator 4 of 5 for this parallel run (total 200)...\n",
      "Building estimator 4 of 5 for this parallel run (total 200)...\n",
      "Building estimator 4 of 5 for this parallel run (total 200)...\n",
      "Building estimator 4 of 5 for this parallel run (total 200)...\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "Building estimator 5 of 5 for this parallel run (total 200)...\n",
      "Building estimator 5 of 5 for this parallel run (total 200)...\n",
      "Building estimator 5 of 5 for this parallel run (total 200)...\n",
      "Building estimator 5 of 5 for this parallel run (total 200)...\n",
      "Building estimator 5 of 5 for this parallel run (total 200)...\n",
      "Building estimator 5 of 5 for this parallel run (total 200)...\n",
      "Building estimator 5 of 5 for this parallel run (total 200)...\n",
      "Building estimator 5 of 5 for this parallel run (total 200)...\n",
      "Building estimator 5 of 5 for this parallel run (total 200)...\n",
      "Building estimator 5 of 5 for this parallel run (total 200)...\n",
      "Building estimator 5 of 5 for this parallel run (total 200)...\n",
      "Building estimator 5 of 5 for this parallel run (total 200)...\n",
      "Building estimator 5 of 5 for this parallel run (total 200)...\n",
      "Building estimator 5 of 5 for this parallel run (total 200)...\n",
      "Building estimator 5 of 5 for this parallel run (total 200)...\n",
      "Building estimator 5 of 5 for this parallel run (total 200)...\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=36)]: Done   3 out of  36 | elapsed:  1.3min remaining: 14.4min\n",
      "[Parallel(n_jobs=36)]: Done  22 out of  36 | elapsed:  1.4min remaining:   54.5s\n",
      "[Parallel(n_jobs=36)]: Done  36 out of  36 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=None, bootstrap=True,\n",
       "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
       "         n_estimators=200, n_jobs=-1, oob_score=False, random_state=None,\n",
       "         verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc = BaggingClassifier(n_estimators=200, n_jobs=-1, verbose=2)\n",
    "bc.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=36)]: Done   3 out of  36 | elapsed:    1.0s remaining:   11.2s\n",
      "[Parallel(n_jobs=36)]: Done  22 out of  36 | elapsed:    3.7s remaining:    2.4s\n",
      "[Parallel(n_jobs=36)]: Done  36 out of  36 | elapsed:    5.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.90716491243886499"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembled Model #6: ExtraTrees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/ipykernel/__main__.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 2 of 200building tree 3 of 200building tree 4 of 200building tree 5 of 200building tree 6 of 200building tree 7 of 200building tree 8 of 200building tree 9 of 200building tree 11 of 200building tree 12 of 200building tree 13 of 200building tree 15 of 200building tree 16 of 200building tree 18 of 200building tree 19 of 200building tree 20 of 200building tree 21 of 200building tree 22 of 200building tree 14 of 200building tree 23 of 200building tree 17 of 200building tree 1 of 200building tree 10 of 200building tree 26 of 200building tree 27 of 200building tree 28 of 200building tree 24 of 200building tree 29 of 200building tree 30 of 200building tree 25 of 200building tree 31 of 200building tree 32 of 200building tree 33 of 200building tree 34 of 200building tree 35 of 200\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "building tree 36 of 200\n",
      "\n",
      "building tree 37 of 200\n",
      "building tree 38 of 200\n",
      "building tree 39 of 200\n",
      "building tree 40 of 200\n",
      "building tree 41 of 200\n",
      "building tree 42 of 200\n",
      "building tree 44 of 200\n",
      "building tree 45 of 200building tree 46 of 200building tree 48 of 200building tree 43 of 200building tree 47 of 200\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "building tree 49 of 200\n",
      "building tree 50 of 200\n",
      "building tree 51 of 200\n",
      "building tree 52 of 200\n",
      "building tree 53 of 200\n",
      "building tree 54 of 200\n",
      "building tree 55 of 200\n",
      "building tree 56 of 200\n",
      "building tree 57 of 200\n",
      "building tree 58 of 200\n",
      "building tree 59 of 200\n",
      "building tree 60 of 200\n",
      "building tree 61 of 200\n",
      "building tree 62 of 200\n",
      "building tree 63 of 200\n",
      "building tree 64 of 200building tree 65 of 200\n",
      "\n",
      "building tree 66 of 200\n",
      "building tree 67 of 200\n",
      "building tree 68 of 200\n",
      "building tree 69 of 200\n",
      "building tree 70 of 200\n",
      "building tree 71 of 200\n",
      "building tree 72 of 200\n",
      "building tree 73 of 200\n",
      "building tree 74 of 200\n",
      "building tree 75 of 200\n",
      "building tree 76 of 200building tree 77 of 200\n",
      "\n",
      "building tree 78 of 200\n",
      "building tree 79 of 200\n",
      "building tree 80 of 200\n",
      "building tree 81 of 200\n",
      "building tree 82 of 200\n",
      "building tree 83 of 200\n",
      "building tree 84 of 200\n",
      "building tree 85 of 200\n",
      "building tree 86 of 200\n",
      "building tree 87 of 200\n",
      "building tree 88 of 200\n",
      "building tree 89 of 200\n",
      "building tree 90 of 200\n",
      "building tree 91 of 200\n",
      "building tree 92 of 200\n",
      "building tree 93 of 200\n",
      "building tree 94 of 200\n",
      "building tree 95 of 200\n",
      "building tree 96 of 200\n",
      "building tree 97 of 200\n",
      "building tree 98 of 200\n",
      "building tree 99 of 200\n",
      "building tree 100 of 200\n",
      "building tree 101 of 200\n",
      "building tree 102 of 200\n",
      "building tree 103 of 200\n",
      "building tree 104 of 200\n",
      "building tree 105 of 200\n",
      "building tree 106 of 200\n",
      "building tree 107 of 200\n",
      "building tree 108 of 200\n",
      "building tree 109 of 200\n",
      "building tree 110 of 200\n",
      "building tree 111 of 200\n",
      "building tree 112 of 200\n",
      "building tree 113 of 200\n",
      "building tree 114 of 200\n",
      "building tree 115 of 200\n",
      "building tree 116 of 200\n",
      "building tree 117 of 200\n",
      "building tree 118 of 200\n",
      "building tree 119 of 200building tree 120 of 200\n",
      "\n",
      "building tree 121 of 200\n",
      "building tree 122 of 200\n",
      "building tree 123 of 200\n",
      "building tree 124 of 200\n",
      "building tree 125 of 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:    7.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 126 of 200\n",
      "building tree 127 of 200\n",
      "building tree 129 of 200building tree 128 of 200\n",
      "\n",
      "building tree 130 of 200\n",
      "building tree 131 of 200\n",
      "building tree 132 of 200\n",
      "building tree 133 of 200\n",
      "building tree 134 of 200\n",
      "building tree 135 of 200\n",
      "building tree 136 of 200\n",
      "building tree 137 of 200\n",
      "building tree 138 of 200\n",
      "building tree 139 of 200\n",
      "building tree 140 of 200\n",
      "building tree 141 of 200\n",
      "building tree 142 of 200\n",
      "building tree 143 of 200\n",
      "building tree 144 of 200\n",
      "building tree 145 of 200\n",
      "building tree 146 of 200\n",
      "building tree 147 of 200\n",
      "building tree 148 of 200\n",
      "building tree 149 of 200\n",
      "building tree 150 of 200\n",
      "building tree 151 of 200\n",
      "building tree 152 of 200\n",
      "building tree 153 of 200\n",
      "building tree 154 of 200building tree 155 of 200\n",
      "\n",
      "building tree 156 of 200\n",
      "building tree 157 of 200\n",
      "building tree 158 of 200\n",
      "building tree 159 of 200\n",
      "building tree 160 of 200\n",
      "building tree 161 of 200\n",
      "building tree 162 of 200\n",
      "building tree 163 of 200building tree 164 of 200\n",
      "\n",
      "building tree 165 of 200\n",
      "building tree 166 of 200\n",
      "building tree 167 of 200\n",
      "building tree 168 of 200\n",
      "building tree 169 of 200\n",
      "building tree 170 of 200\n",
      "building tree 171 of 200\n",
      "building tree 172 of 200\n",
      "building tree 173 of 200\n",
      "building tree 174 of 200\n",
      "building tree 175 of 200\n",
      "building tree 176 of 200\n",
      "building tree 177 of 200\n",
      "building tree 178 of 200\n",
      "building tree 179 of 200\n",
      "building tree 180 of 200\n",
      "building tree 181 of 200\n",
      "building tree 182 of 200\n",
      "building tree 183 of 200\n",
      "building tree 184 of 200\n",
      "building tree 185 of 200\n",
      "building tree 186 of 200\n",
      "building tree 187 of 200\n",
      "building tree 188 of 200\n",
      "building tree 189 of 200building tree 191 of 200\n",
      "\n",
      "building tree 190 of 200\n",
      "building tree 192 of 200\n",
      "building tree 193 of 200\n",
      "building tree 194 of 200\n",
      "building tree 195 of 200building tree 196 of 200\n",
      "\n",
      "building tree 197 of 200\n",
      "building tree 198 of 200\n",
      "building tree 199 of 200\n",
      "building tree 200 of 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   13.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=200, n_jobs=-1, oob_score=False, random_state=None,\n",
       "           verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etc = ExtraTreesClassifier(n_estimators=200, n_jobs=-1, verbose=2)\n",
    "etc.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=36)]: Done  90 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=36)]: Done 200 out of 200 | elapsed:    1.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8421119850946982"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etc.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembled Model #7: GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/utils/validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.0483           10.84m\n",
      "         2           1.0173           11.33m\n",
      "         3           0.9938           11.15m\n",
      "         4           0.9753           11.04m\n",
      "         5           0.9603           10.97m\n",
      "         6           0.9479           11.04m\n",
      "         7           0.9375           11.10m\n",
      "         8           0.9287           11.12m\n",
      "         9           0.9212           11.14m\n",
      "        10           0.9138           11.28m\n",
      "        11           0.9082           11.27m\n",
      "        12           0.9024           11.35m\n",
      "        13           0.8963           11.44m\n",
      "        14           0.8923           11.41m\n",
      "        15           0.8882           11.47m\n",
      "        16           0.8844           11.48m\n",
      "        17           0.8813           11.51m\n",
      "        18           0.8782           11.56m\n",
      "        19           0.8756           11.56m\n",
      "        20           0.8728           11.56m\n",
      "        21           0.8701           11.53m\n",
      "        22           0.8644           11.61m\n",
      "        23           0.8618           11.69m\n",
      "        24           0.8596           11.71m\n",
      "        25           0.8568           11.76m\n",
      "        26           0.8549           11.82m\n",
      "        27           0.8534           11.82m\n",
      "        28           0.8501           11.86m\n",
      "        29           0.8484           11.82m\n",
      "        30           0.8469           11.83m\n",
      "        31           0.8440           11.86m\n",
      "        32           0.8423           11.85m\n",
      "        33           0.8410           11.85m\n",
      "        34           0.8398           11.87m\n",
      "        35           0.8386           11.82m\n",
      "        36           0.8373           11.83m\n",
      "        37           0.8362           11.81m\n",
      "        38           0.8352           11.76m\n",
      "        39           0.8341           11.74m\n",
      "        40           0.8313           11.76m\n",
      "        41           0.8291           11.75m\n",
      "        42           0.8282           11.74m\n",
      "        43           0.8257           11.73m\n",
      "        44           0.8248           11.75m\n",
      "        45           0.8240           11.77m\n",
      "        46           0.8231           11.77m\n",
      "        47           0.8205           11.79m\n",
      "        48           0.8197           11.76m\n",
      "        49           0.8188           11.74m\n",
      "        50           0.8178           11.71m\n",
      "        51           0.8172           11.69m\n",
      "        52           0.8156           11.68m\n",
      "        53           0.8147           11.68m\n",
      "        54           0.8138           11.65m\n",
      "        55           0.8129           11.65m\n",
      "        56           0.8120           11.65m\n",
      "        57           0.8113           11.66m\n",
      "        58           0.8093           11.64m\n",
      "        59           0.8085           11.63m\n",
      "        60           0.8077           11.63m\n",
      "        61           0.8071           11.59m\n",
      "        62           0.8040           11.57m\n",
      "        63           0.8028           11.57m\n",
      "        64           0.8016           11.54m\n",
      "        65           0.8009           11.54m\n",
      "        66           0.8001           11.56m\n",
      "        67           0.7994           11.59m\n",
      "        68           0.7982           11.57m\n",
      "        69           0.7964           11.55m\n",
      "        70           0.7945           11.54m\n",
      "        71           0.7937           11.54m\n",
      "        72           0.7883           11.55m\n",
      "        73           0.7864           11.53m\n",
      "        74           0.7849           11.52m\n",
      "        75           0.7835           11.51m\n",
      "        76           0.7822           11.50m\n",
      "        77           0.7805           11.50m\n",
      "        78           0.7795           11.49m\n",
      "        79           0.7781           11.49m\n",
      "        80           0.7770           11.46m\n",
      "        81           0.7761           11.45m\n",
      "        82           0.7754           11.46m\n",
      "        83           0.7747           11.47m\n",
      "        84           0.7741           11.46m\n",
      "        85           0.7737           11.44m\n",
      "        86           0.7732           11.43m\n",
      "        87           0.7726           11.41m\n",
      "        88           0.7713           11.39m\n",
      "        89           0.7703           11.37m\n",
      "        90           0.7670           11.36m\n",
      "        91           0.7662           11.35m\n",
      "        92           0.7632           11.35m\n",
      "        93           0.7624           11.34m\n",
      "        94           0.7591           11.34m\n",
      "        95           0.7563           11.34m\n",
      "        96           0.7548           11.32m\n",
      "        97           0.7533           11.30m\n",
      "        98           0.7520           11.28m\n",
      "        99           0.7511           11.28m\n",
      "       100           0.7499           11.27m\n",
      "       101           0.7485           11.27m\n",
      "       102           0.7479           11.26m\n",
      "       103           0.7464           11.24m\n",
      "       104           0.7458           11.22m\n",
      "       105           0.7427           11.23m\n",
      "       106           0.7421           11.21m\n",
      "       107           0.7390           11.21m\n",
      "       108           0.7381           11.21m\n",
      "       109           0.7359           11.21m\n",
      "       110           0.7342           11.20m\n",
      "       111           0.7332           11.20m\n",
      "       112           0.7324           11.18m\n",
      "       113           0.7309           11.18m\n",
      "       114           0.7306           11.15m\n",
      "       115           0.7302           11.15m\n",
      "       116           0.7297           11.12m\n",
      "       117           0.7286           11.12m\n",
      "       118           0.7282           11.11m\n",
      "       119           0.7276           11.09m\n",
      "       120           0.7271           11.07m\n",
      "       121           0.7268           11.06m\n",
      "       122           0.7263           11.06m\n",
      "       123           0.7241           11.04m\n",
      "       124           0.7219           11.03m\n",
      "       125           0.7200           11.03m\n",
      "       126           0.7180           11.01m\n",
      "       127           0.7172           11.01m\n",
      "       128           0.7161           10.99m\n",
      "       129           0.7155           10.98m\n",
      "       130           0.7136           10.98m\n",
      "       131           0.7129           10.97m\n",
      "       132           0.7124           10.96m\n",
      "       133           0.7121           10.94m\n",
      "       134           0.7118           10.92m\n",
      "       135           0.7114           10.91m\n",
      "       136           0.7100           10.89m\n",
      "       137           0.7096           10.87m\n",
      "       138           0.7078           10.87m\n",
      "       139           0.7072           10.87m\n",
      "       140           0.7059           10.85m\n",
      "       141           0.7044           10.84m\n",
      "       142           0.7029           10.83m\n",
      "       143           0.7022           10.81m\n",
      "       144           0.7005           10.81m\n",
      "       145           0.6991           10.79m\n",
      "       146           0.6979           10.78m\n",
      "       147           0.6975           10.76m\n",
      "       148           0.6970           10.75m\n",
      "       149           0.6961           10.73m\n",
      "       150           0.6953           10.72m\n",
      "       151           0.6951           10.70m\n",
      "       152           0.6948           10.69m\n",
      "       153           0.6933           10.69m\n",
      "       154           0.6921           10.68m\n",
      "       155           0.6912           10.67m\n",
      "       156           0.6902           10.65m\n",
      "       157           0.6889           10.64m\n",
      "       158           0.6884           10.64m\n",
      "       159           0.6880           10.62m\n",
      "       160           0.6877           10.60m\n",
      "       161           0.6862           10.59m\n",
      "       162           0.6860           10.57m\n",
      "       163           0.6847           10.56m\n",
      "       164           0.6843           10.55m\n",
      "       165           0.6840           10.54m\n",
      "       166           0.6837           10.53m\n",
      "       167           0.6820           10.52m\n",
      "       168           0.6816           10.50m\n",
      "       169           0.6799           10.50m\n",
      "       170           0.6789           10.49m\n",
      "       171           0.6781           10.48m\n",
      "       172           0.6777           10.47m\n",
      "       173           0.6774           10.45m\n",
      "       174           0.6758           10.44m\n",
      "       175           0.6748           10.43m\n",
      "       176           0.6745           10.41m\n",
      "       177           0.6739           10.40m\n",
      "       178           0.6733           10.39m\n",
      "       179           0.6727           10.37m\n",
      "       180           0.6724           10.36m\n",
      "       181           0.6722           10.35m\n",
      "       182           0.6720           10.34m\n",
      "       183           0.6704           10.33m\n",
      "       184           0.6696           10.32m\n",
      "       185           0.6692           10.31m\n",
      "       186           0.6690           10.30m\n",
      "       187           0.6687           10.28m\n",
      "       188           0.6675           10.27m\n",
      "       189           0.6671           10.26m\n",
      "       190           0.6670           10.24m\n",
      "       191           0.6665           10.23m\n",
      "       192           0.6662           10.23m\n",
      "       193           0.6660           10.21m\n",
      "       194           0.6656           10.20m\n",
      "       195           0.6653           10.19m\n",
      "       196           0.6650           10.18m\n",
      "       197           0.6639           10.17m\n",
      "       198           0.6634           10.15m\n",
      "       199           0.6621           10.14m\n",
      "       200           0.6617           10.14m\n",
      "       201           0.6615           10.12m\n",
      "       202           0.6610           10.11m\n",
      "       203           0.6608           10.11m\n",
      "       204           0.6605           10.09m\n",
      "       205           0.6601           10.07m\n",
      "       206           0.6596           10.05m\n",
      "       207           0.6594           10.03m\n",
      "       208           0.6583           10.03m\n",
      "       209           0.6576           10.01m\n",
      "       210           0.6572           10.00m\n",
      "       211           0.6557            9.99m\n",
      "       212           0.6549            9.98m\n",
      "       213           0.6542            9.96m\n",
      "       214           0.6534            9.95m\n",
      "       215           0.6532            9.95m\n",
      "       216           0.6529            9.94m\n",
      "       217           0.6527            9.92m\n",
      "       218           0.6516            9.91m\n",
      "       219           0.6513            9.90m\n",
      "       220           0.6505            9.89m\n",
      "       221           0.6501            9.88m\n",
      "       222           0.6498            9.87m\n",
      "       223           0.6487            9.86m\n",
      "       224           0.6483            9.85m\n",
      "       225           0.6477            9.84m\n",
      "       226           0.6471            9.82m\n",
      "       227           0.6469            9.81m\n",
      "       228           0.6467            9.79m\n",
      "       229           0.6464            9.78m\n",
      "       230           0.6460            9.76m\n",
      "       231           0.6457            9.75m\n",
      "       232           0.6445            9.74m\n",
      "       233           0.6440            9.73m\n",
      "       234           0.6439            9.71m\n",
      "       235           0.6437            9.69m\n",
      "       236           0.6435            9.68m\n",
      "       237           0.6431            9.66m\n",
      "       238           0.6430            9.65m\n",
      "       239           0.6428            9.64m\n",
      "       240           0.6419            9.62m\n",
      "       241           0.6415            9.61m\n",
      "       242           0.6407            9.60m\n",
      "       243           0.6403            9.59m\n",
      "       244           0.6396            9.58m\n",
      "       245           0.6394            9.56m\n",
      "       246           0.6391            9.55m\n",
      "       247           0.6389            9.53m\n",
      "       248           0.6386            9.52m\n",
      "       249           0.6376            9.50m\n",
      "       250           0.6368            9.49m\n",
      "       251           0.6366            9.48m\n",
      "       252           0.6365            9.47m\n",
      "       253           0.6363            9.45m\n",
      "       254           0.6362            9.44m\n",
      "       255           0.6360            9.43m\n",
      "       256           0.6357            9.41m\n",
      "       257           0.6352            9.41m\n",
      "       258           0.6351            9.39m\n",
      "       259           0.6343            9.38m\n",
      "       260           0.6332            9.37m\n",
      "       261           0.6328            9.36m\n",
      "       262           0.6326            9.34m\n",
      "       263           0.6320            9.33m\n",
      "       264           0.6314            9.32m\n",
      "       265           0.6309            9.31m\n",
      "       266           0.6307            9.29m\n",
      "       267           0.6298            9.28m\n",
      "       268           0.6294            9.27m\n",
      "       269           0.6288            9.26m\n",
      "       270           0.6286            9.25m\n",
      "       271           0.6282            9.24m\n",
      "       272           0.6280            9.23m\n",
      "       273           0.6275            9.21m\n",
      "       274           0.6271            9.20m\n",
      "       275           0.6264            9.19m\n",
      "       276           0.6262            9.18m\n",
      "       277           0.6257            9.16m\n",
      "       278           0.6250            9.15m\n",
      "       279           0.6243            9.14m\n",
      "       280           0.6241            9.12m\n",
      "       281           0.6240            9.11m\n",
      "       282           0.6238            9.09m\n",
      "       283           0.6237            9.09m\n",
      "       284           0.6236            9.07m\n",
      "       285           0.6234            9.06m\n",
      "       286           0.6232            9.05m\n",
      "       287           0.6230            9.03m\n",
      "       288           0.6226            9.02m\n",
      "       289           0.6219            9.01m\n",
      "       290           0.6212            9.00m\n",
      "       291           0.6207            8.99m\n",
      "       292           0.6201            8.98m\n",
      "       293           0.6199            8.96m\n",
      "       294           0.6198            8.95m\n",
      "       295           0.6197            8.93m\n",
      "       296           0.6196            8.92m\n",
      "       297           0.6191            8.91m\n",
      "       298           0.6190            8.89m\n",
      "       299           0.6183            8.88m\n",
      "       300           0.6180            8.87m\n",
      "       301           0.6177            8.86m\n",
      "       302           0.6174            8.84m\n",
      "       303           0.6171            8.83m\n",
      "       304           0.6166            8.81m\n",
      "       305           0.6164            8.80m\n",
      "       306           0.6161            8.79m\n",
      "       307           0.6159            8.78m\n",
      "       308           0.6157            8.76m\n",
      "       309           0.6151            8.75m\n",
      "       310           0.6147            8.74m\n",
      "       311           0.6142            8.73m\n",
      "       312           0.6141            8.72m\n",
      "       313           0.6139            8.71m\n",
      "       314           0.6134            8.69m\n",
      "       315           0.6130            8.68m\n",
      "       316           0.6128            8.67m\n",
      "       317           0.6122            8.66m\n",
      "       318           0.6120            8.64m\n",
      "       319           0.6117            8.63m\n",
      "       320           0.6116            8.61m\n",
      "       321           0.6115            8.60m\n",
      "       322           0.6114            8.59m\n",
      "       323           0.6113            8.57m\n",
      "       324           0.6112            8.56m\n",
      "       325           0.6107            8.55m\n",
      "       326           0.6106            8.54m\n",
      "       327           0.6105            8.53m\n",
      "       328           0.6102            8.51m\n",
      "       329           0.6098            8.50m\n",
      "       330           0.6095            8.49m\n",
      "       331           0.6090            8.47m\n",
      "       332           0.6087            8.45m\n",
      "       333           0.6085            8.44m\n",
      "       334           0.6080            8.43m\n",
      "       335           0.6076            8.42m\n",
      "       336           0.6074            8.41m\n",
      "       337           0.6070            8.40m\n",
      "       338           0.6066            8.38m\n",
      "       339           0.6065            8.37m\n",
      "       340           0.6057            8.35m\n",
      "       341           0.6056            8.34m\n",
      "       342           0.6055            8.33m\n",
      "       343           0.6052            8.31m\n",
      "       344           0.6050            8.30m\n",
      "       345           0.6044            8.29m\n",
      "       346           0.6038            8.28m\n",
      "       347           0.6036            8.27m\n",
      "       348           0.6034            8.26m\n",
      "       349           0.6034            8.24m\n",
      "       350           0.6033            8.23m\n",
      "       351           0.6030            8.21m\n",
      "       352           0.6026            8.20m\n",
      "       353           0.6023            8.19m\n",
      "       354           0.6018            8.17m\n",
      "       355           0.6017            8.16m\n",
      "       356           0.6014            8.15m\n",
      "       357           0.6013            8.14m\n",
      "       358           0.6012            8.12m\n",
      "       359           0.6006            8.11m\n",
      "       360           0.6003            8.10m\n",
      "       361           0.6000            8.08m\n",
      "       362           0.5997            8.07m\n",
      "       363           0.5994            8.06m\n",
      "       364           0.5988            8.05m\n",
      "       365           0.5983            8.03m\n",
      "       366           0.5980            8.02m\n",
      "       367           0.5976            8.01m\n",
      "       368           0.5970            7.99m\n",
      "       369           0.5967            7.98m\n",
      "       370           0.5965            7.97m\n",
      "       371           0.5963            7.96m\n",
      "       372           0.5959            7.95m\n",
      "       373           0.5956            7.94m\n",
      "       374           0.5952            7.92m\n",
      "       375           0.5951            7.91m\n",
      "       376           0.5948            7.89m\n",
      "       377           0.5947            7.88m\n",
      "       378           0.5946            7.87m\n",
      "       379           0.5945            7.85m\n",
      "       380           0.5944            7.84m\n",
      "       381           0.5942            7.83m\n",
      "       382           0.5938            7.82m\n",
      "       383           0.5937            7.81m\n",
      "       384           0.5935            7.79m\n",
      "       385           0.5934            7.78m\n",
      "       386           0.5932            7.77m\n",
      "       387           0.5931            7.75m\n",
      "       388           0.5926            7.74m\n",
      "       389           0.5922            7.73m\n",
      "       390           0.5919            7.71m\n",
      "       391           0.5917            7.70m\n",
      "       392           0.5916            7.69m\n",
      "       393           0.5915            7.67m\n",
      "       394           0.5910            7.66m\n",
      "       395           0.5909            7.65m\n",
      "       396           0.5905            7.63m\n",
      "       397           0.5903            7.62m\n",
      "       398           0.5900            7.61m\n",
      "       399           0.5896            7.60m\n",
      "       400           0.5895            7.59m\n",
      "       401           0.5892            7.57m\n",
      "       402           0.5887            7.56m\n",
      "       403           0.5886            7.55m\n",
      "       404           0.5882            7.54m\n",
      "       405           0.5879            7.53m\n",
      "       406           0.5878            7.51m\n",
      "       407           0.5877            7.50m\n",
      "       408           0.5874            7.49m\n",
      "       409           0.5869            7.48m\n",
      "       410           0.5866            7.47m\n",
      "       411           0.5862            7.46m\n",
      "       412           0.5861            7.44m\n",
      "       413           0.5857            7.43m\n",
      "       414           0.5855            7.42m\n",
      "       415           0.5854            7.41m\n",
      "       416           0.5852            7.40m\n",
      "       417           0.5848            7.38m\n",
      "       418           0.5846            7.37m\n",
      "       419           0.5845            7.36m\n",
      "       420           0.5843            7.35m\n",
      "       421           0.5838            7.34m\n",
      "       422           0.5838            7.32m\n",
      "       423           0.5836            7.31m\n",
      "       424           0.5835            7.30m\n",
      "       425           0.5834            7.28m\n",
      "       426           0.5833            7.27m\n",
      "       427           0.5832            7.26m\n",
      "       428           0.5831            7.25m\n",
      "       429           0.5829            7.23m\n",
      "       430           0.5828            7.22m\n",
      "       431           0.5825            7.20m\n",
      "       432           0.5825            7.19m\n",
      "       433           0.5824            7.18m\n",
      "       434           0.5822            7.17m\n",
      "       435           0.5822            7.15m\n",
      "       436           0.5821            7.14m\n",
      "       437           0.5819            7.12m\n",
      "       438           0.5816            7.11m\n",
      "       439           0.5815            7.10m\n",
      "       440           0.5812            7.09m\n",
      "       441           0.5811            7.07m\n",
      "       442           0.5809            7.06m\n",
      "       443           0.5809            7.04m\n",
      "       444           0.5808            7.03m\n",
      "       445           0.5803            7.02m\n",
      "       446           0.5799            7.01m\n",
      "       447           0.5798            6.99m\n",
      "       448           0.5797            6.98m\n",
      "       449           0.5796            6.97m\n",
      "       450           0.5796            6.96m\n",
      "       451           0.5795            6.94m\n",
      "       452           0.5793            6.93m\n",
      "       453           0.5790            6.92m\n",
      "       454           0.5788            6.90m\n",
      "       455           0.5788            6.89m\n",
      "       456           0.5787            6.87m\n",
      "       457           0.5787            6.86m\n",
      "       458           0.5786            6.85m\n",
      "       459           0.5784            6.83m\n",
      "       460           0.5781            6.82m\n",
      "       461           0.5781            6.80m\n",
      "       462           0.5780            6.79m\n",
      "       463           0.5779            6.78m\n",
      "       464           0.5778            6.76m\n",
      "       465           0.5777            6.75m\n",
      "       466           0.5777            6.73m\n",
      "       467           0.5772            6.72m\n",
      "       468           0.5771            6.71m\n",
      "       469           0.5769            6.70m\n",
      "       470           0.5765            6.69m\n",
      "       471           0.5759            6.68m\n",
      "       472           0.5757            6.67m\n",
      "       473           0.5753            6.65m\n",
      "       474           0.5751            6.64m\n",
      "       475           0.5749            6.63m\n",
      "       476           0.5747            6.62m\n",
      "       477           0.5745            6.60m\n",
      "       478           0.5741            6.59m\n",
      "       479           0.5738            6.58m\n",
      "       480           0.5736            6.57m\n",
      "       481           0.5732            6.56m\n",
      "       482           0.5730            6.55m\n",
      "       483           0.5727            6.53m\n",
      "       484           0.5724            6.52m\n",
      "       485           0.5720            6.51m\n",
      "       486           0.5719            6.50m\n",
      "       487           0.5717            6.49m\n",
      "       488           0.5714            6.47m\n",
      "       489           0.5712            6.46m\n",
      "       490           0.5711            6.45m\n",
      "       491           0.5708            6.44m\n",
      "       492           0.5706            6.42m\n",
      "       493           0.5704            6.41m\n",
      "       494           0.5700            6.40m\n",
      "       495           0.5699            6.39m\n",
      "       496           0.5697            6.37m\n",
      "       497           0.5694            6.36m\n",
      "       498           0.5690            6.35m\n",
      "       499           0.5686            6.34m\n",
      "       500           0.5684            6.32m\n",
      "       501           0.5680            6.31m\n",
      "       502           0.5680            6.30m\n",
      "       503           0.5679            6.29m\n",
      "       504           0.5676            6.27m\n",
      "       505           0.5675            6.26m\n",
      "       506           0.5674            6.25m\n",
      "       507           0.5673            6.23m\n",
      "       508           0.5671            6.22m\n",
      "       509           0.5671            6.21m\n",
      "       510           0.5670            6.19m\n",
      "       511           0.5668            6.18m\n",
      "       512           0.5667            6.17m\n",
      "       513           0.5663            6.15m\n",
      "       514           0.5662            6.14m\n",
      "       515           0.5661            6.13m\n",
      "       516           0.5658            6.12m\n",
      "       517           0.5657            6.10m\n",
      "       518           0.5654            6.09m\n",
      "       519           0.5651            6.08m\n",
      "       520           0.5649            6.07m\n",
      "       521           0.5647            6.06m\n",
      "       522           0.5645            6.04m\n",
      "       523           0.5644            6.03m\n",
      "       524           0.5641            6.02m\n",
      "       525           0.5638            6.01m\n",
      "       526           0.5636            5.99m\n",
      "       527           0.5635            5.98m\n",
      "       528           0.5632            5.97m\n",
      "       529           0.5630            5.96m\n",
      "       530           0.5627            5.95m\n",
      "       531           0.5625            5.94m\n",
      "       532           0.5623            5.92m\n",
      "       533           0.5620            5.91m\n",
      "       534           0.5619            5.90m\n",
      "       535           0.5615            5.89m\n",
      "       536           0.5612            5.87m\n",
      "       537           0.5610            5.86m\n",
      "       538           0.5609            5.85m\n",
      "       539           0.5608            5.83m\n",
      "       540           0.5607            5.82m\n",
      "       541           0.5606            5.81m\n",
      "       542           0.5604            5.79m\n",
      "       543           0.5601            5.78m\n",
      "       544           0.5599            5.77m\n",
      "       545           0.5598            5.76m\n",
      "       546           0.5595            5.74m\n",
      "       547           0.5591            5.73m\n",
      "       548           0.5588            5.72m\n",
      "       549           0.5585            5.71m\n",
      "       550           0.5583            5.70m\n",
      "       551           0.5582            5.69m\n",
      "       552           0.5582            5.67m\n",
      "       553           0.5581            5.66m\n",
      "       554           0.5580            5.65m\n",
      "       555           0.5578            5.63m\n",
      "       556           0.5575            5.62m\n",
      "       557           0.5574            5.61m\n",
      "       558           0.5572            5.60m\n",
      "       559           0.5571            5.58m\n",
      "       560           0.5569            5.57m\n",
      "       561           0.5568            5.56m\n",
      "       562           0.5568            5.54m\n",
      "       563           0.5566            5.53m\n",
      "       564           0.5564            5.52m\n",
      "       565           0.5563            5.51m\n",
      "       566           0.5562            5.49m\n",
      "       567           0.5561            5.48m\n",
      "       568           0.5559            5.47m\n",
      "       569           0.5557            5.45m\n",
      "       570           0.5555            5.44m\n",
      "       571           0.5555            5.43m\n",
      "       572           0.5552            5.42m\n",
      "       573           0.5551            5.40m\n",
      "       574           0.5550            5.39m\n",
      "       575           0.5549            5.38m\n",
      "       576           0.5549            5.36m\n",
      "       577           0.5548            5.35m\n",
      "       578           0.5547            5.34m\n",
      "       579           0.5545            5.33m\n",
      "       580           0.5545            5.31m\n",
      "       581           0.5542            5.30m\n",
      "       582           0.5541            5.29m\n",
      "       583           0.5541            5.27m\n",
      "       584           0.5538            5.26m\n",
      "       585           0.5537            5.25m\n",
      "       586           0.5534            5.24m\n",
      "       587           0.5534            5.22m\n",
      "       588           0.5534            5.21m\n",
      "       589           0.5533            5.20m\n",
      "       590           0.5530            5.18m\n",
      "       591           0.5528            5.17m\n",
      "       592           0.5526            5.16m\n",
      "       593           0.5525            5.15m\n",
      "       594           0.5523            5.13m\n",
      "       595           0.5522            5.12m\n",
      "       596           0.5522            5.11m\n",
      "       597           0.5520            5.09m\n",
      "       598           0.5519            5.08m\n",
      "       599           0.5518            5.07m\n",
      "       600           0.5518            5.06m\n",
      "       601           0.5516            5.05m\n",
      "       602           0.5513            5.03m\n",
      "       603           0.5512            5.02m\n",
      "       604           0.5510            5.01m\n",
      "       605           0.5509            5.00m\n",
      "       606           0.5509            4.98m\n",
      "       607           0.5508            4.97m\n",
      "       608           0.5508            4.96m\n",
      "       609           0.5507            4.94m\n",
      "       610           0.5505            4.93m\n",
      "       611           0.5504            4.92m\n",
      "       612           0.5504            4.90m\n",
      "       613           0.5503            4.89m\n",
      "       614           0.5501            4.88m\n",
      "       615           0.5499            4.86m\n",
      "       616           0.5498            4.85m\n",
      "       617           0.5498            4.84m\n",
      "       618           0.5495            4.82m\n",
      "       619           0.5494            4.81m\n",
      "       620           0.5493            4.80m\n",
      "       621           0.5492            4.79m\n",
      "       622           0.5491            4.77m\n",
      "       623           0.5489            4.76m\n",
      "       624           0.5487            4.75m\n",
      "       625           0.5486            4.74m\n",
      "       626           0.5486            4.72m\n",
      "       627           0.5485            4.71m\n",
      "       628           0.5485            4.70m\n",
      "       629           0.5482            4.68m\n",
      "       630           0.5481            4.67m\n",
      "       631           0.5478            4.66m\n",
      "       632           0.5477            4.65m\n",
      "       633           0.5476            4.63m\n",
      "       634           0.5474            4.62m\n",
      "       635           0.5472            4.61m\n",
      "       636           0.5472            4.60m\n",
      "       637           0.5470            4.58m\n",
      "       638           0.5469            4.57m\n",
      "       639           0.5467            4.56m\n",
      "       640           0.5466            4.55m\n",
      "       641           0.5464            4.54m\n",
      "       642           0.5462            4.52m\n",
      "       643           0.5461            4.51m\n",
      "       644           0.5460            4.50m\n",
      "       645           0.5459            4.48m\n",
      "       646           0.5457            4.47m\n",
      "       647           0.5456            4.46m\n",
      "       648           0.5454            4.45m\n",
      "       649           0.5452            4.43m\n",
      "       650           0.5452            4.42m\n",
      "       651           0.5452            4.41m\n",
      "       652           0.5450            4.40m\n",
      "       653           0.5448            4.38m\n",
      "       654           0.5448            4.37m\n",
      "       655           0.5446            4.36m\n",
      "       656           0.5444            4.34m\n",
      "       657           0.5442            4.33m\n",
      "       658           0.5442            4.32m\n",
      "       659           0.5442            4.31m\n",
      "       660           0.5441            4.29m\n",
      "       661           0.5441            4.28m\n",
      "       662           0.5439            4.27m\n",
      "       663           0.5438            4.26m\n",
      "       664           0.5438            4.24m\n",
      "       665           0.5437            4.23m\n",
      "       666           0.5437            4.21m\n",
      "       667           0.5436            4.20m\n",
      "       668           0.5435            4.19m\n",
      "       669           0.5434            4.18m\n",
      "       670           0.5431            4.16m\n",
      "       671           0.5429            4.15m\n",
      "       672           0.5427            4.14m\n",
      "       673           0.5426            4.13m\n",
      "       674           0.5424            4.11m\n",
      "       675           0.5422            4.10m\n",
      "       676           0.5421            4.09m\n",
      "       677           0.5421            4.07m\n",
      "       678           0.5420            4.06m\n",
      "       679           0.5420            4.05m\n",
      "       680           0.5419            4.03m\n",
      "       681           0.5419            4.02m\n",
      "       682           0.5419            4.01m\n",
      "       683           0.5418            4.00m\n",
      "       684           0.5418            3.98m\n",
      "       685           0.5417            3.97m\n",
      "       686           0.5416            3.96m\n",
      "       687           0.5415            3.94m\n",
      "       688           0.5414            3.93m\n",
      "       689           0.5414            3.92m\n",
      "       690           0.5412            3.91m\n",
      "       691           0.5411            3.89m\n",
      "       692           0.5410            3.88m\n",
      "       693           0.5410            3.87m\n",
      "       694           0.5408            3.86m\n",
      "       695           0.5407            3.84m\n",
      "       696           0.5406            3.83m\n",
      "       697           0.5404            3.82m\n",
      "       698           0.5403            3.81m\n",
      "       699           0.5403            3.79m\n",
      "       700           0.5402            3.78m\n",
      "       701           0.5402            3.77m\n",
      "       702           0.5400            3.76m\n",
      "       703           0.5400            3.74m\n",
      "       704           0.5399            3.73m\n",
      "       705           0.5399            3.72m\n",
      "       706           0.5397            3.70m\n",
      "       707           0.5395            3.69m\n",
      "       708           0.5394            3.68m\n",
      "       709           0.5393            3.67m\n",
      "       710           0.5392            3.65m\n",
      "       711           0.5391            3.64m\n",
      "       712           0.5390            3.63m\n",
      "       713           0.5389            3.62m\n",
      "       714           0.5389            3.60m\n",
      "       715           0.5388            3.59m\n",
      "       716           0.5387            3.58m\n",
      "       717           0.5385            3.57m\n",
      "       718           0.5384            3.55m\n",
      "       719           0.5383            3.54m\n",
      "       720           0.5382            3.53m\n",
      "       721           0.5381            3.52m\n",
      "       722           0.5380            3.50m\n",
      "       723           0.5379            3.49m\n",
      "       724           0.5378            3.48m\n",
      "       725           0.5378            3.46m\n",
      "       726           0.5377            3.45m\n",
      "       727           0.5377            3.44m\n",
      "       728           0.5377            3.43m\n",
      "       729           0.5376            3.41m\n",
      "       730           0.5375            3.40m\n",
      "       731           0.5373            3.39m\n",
      "       732           0.5373            3.37m\n",
      "       733           0.5373            3.36m\n",
      "       734           0.5373            3.35m\n",
      "       735           0.5370            3.33m\n",
      "       736           0.5370            3.32m\n",
      "       737           0.5369            3.31m\n",
      "       738           0.5369            3.30m\n",
      "       739           0.5367            3.29m\n",
      "       740           0.5366            3.27m\n",
      "       741           0.5364            3.26m\n",
      "       742           0.5363            3.25m\n",
      "       743           0.5362            3.24m\n",
      "       744           0.5360            3.22m\n",
      "       745           0.5360            3.21m\n",
      "       746           0.5358            3.20m\n",
      "       747           0.5357            3.19m\n",
      "       748           0.5357            3.17m\n",
      "       749           0.5356            3.16m\n",
      "       750           0.5354            3.15m\n",
      "       751           0.5354            3.13m\n",
      "       752           0.5352            3.12m\n",
      "       753           0.5351            3.11m\n",
      "       754           0.5349            3.10m\n",
      "       755           0.5349            3.08m\n",
      "       756           0.5348            3.07m\n",
      "       757           0.5348            3.06m\n",
      "       758           0.5346            3.05m\n",
      "       759           0.5345            3.03m\n",
      "       760           0.5344            3.02m\n",
      "       761           0.5344            3.01m\n",
      "       762           0.5344            2.99m\n",
      "       763           0.5343            2.98m\n",
      "       764           0.5343            2.97m\n",
      "       765           0.5341            2.96m\n",
      "       766           0.5340            2.94m\n",
      "       767           0.5339            2.93m\n",
      "       768           0.5336            2.92m\n",
      "       769           0.5335            2.91m\n",
      "       770           0.5335            2.90m\n",
      "       771           0.5335            2.88m\n",
      "       772           0.5334            2.87m\n",
      "       773           0.5334            2.86m\n",
      "       774           0.5334            2.84m\n",
      "       775           0.5333            2.83m\n",
      "       776           0.5332            2.82m\n",
      "       777           0.5331            2.81m\n",
      "       778           0.5330            2.79m\n",
      "       779           0.5329            2.78m\n",
      "       780           0.5328            2.77m\n",
      "       781           0.5326            2.76m\n",
      "       782           0.5325            2.74m\n",
      "       783           0.5325            2.73m\n",
      "       784           0.5324            2.72m\n",
      "       785           0.5323            2.71m\n",
      "       786           0.5322            2.69m\n",
      "       787           0.5322            2.68m\n",
      "       788           0.5321            2.67m\n",
      "       789           0.5321            2.65m\n",
      "       790           0.5320            2.64m\n",
      "       791           0.5319            2.63m\n",
      "       792           0.5318            2.61m\n",
      "       793           0.5317            2.60m\n",
      "       794           0.5317            2.59m\n",
      "       795           0.5316            2.58m\n",
      "       796           0.5314            2.56m\n",
      "       797           0.5314            2.55m\n",
      "       798           0.5312            2.54m\n",
      "       799           0.5311            2.53m\n",
      "       800           0.5311            2.51m\n",
      "       801           0.5310            2.50m\n",
      "       802           0.5310            2.49m\n",
      "       803           0.5310            2.48m\n",
      "       804           0.5309            2.46m\n",
      "       805           0.5307            2.45m\n",
      "       806           0.5306            2.44m\n",
      "       807           0.5304            2.42m\n",
      "       808           0.5301            2.41m\n",
      "       809           0.5300            2.40m\n",
      "       810           0.5300            2.39m\n",
      "       811           0.5299            2.37m\n",
      "       812           0.5298            2.36m\n",
      "       813           0.5297            2.35m\n",
      "       814           0.5297            2.34m\n",
      "       815           0.5296            2.32m\n",
      "       816           0.5295            2.31m\n",
      "       817           0.5294            2.30m\n",
      "       818           0.5293            2.29m\n",
      "       819           0.5293            2.27m\n",
      "       820           0.5292            2.26m\n",
      "       821           0.5292            2.25m\n",
      "       822           0.5291            2.24m\n",
      "       823           0.5291            2.22m\n",
      "       824           0.5291            2.21m\n",
      "       825           0.5291            2.20m\n",
      "       826           0.5289            2.18m\n",
      "       827           0.5289            2.17m\n",
      "       828           0.5289            2.16m\n",
      "       829           0.5288            2.15m\n",
      "       830           0.5287            2.13m\n",
      "       831           0.5286            2.12m\n",
      "       832           0.5285            2.11m\n",
      "       833           0.5285            2.10m\n",
      "       834           0.5285            2.08m\n",
      "       835           0.5285            2.07m\n",
      "       836           0.5284            2.06m\n",
      "       837           0.5282            2.05m\n",
      "       838           0.5281            2.03m\n",
      "       839           0.5279            2.02m\n",
      "       840           0.5279            2.01m\n",
      "       841           0.5279            2.00m\n",
      "       842           0.5278            1.98m\n",
      "       843           0.5278            1.97m\n",
      "       844           0.5277            1.96m\n",
      "       845           0.5276            1.94m\n",
      "       846           0.5274            1.93m\n",
      "       847           0.5272            1.92m\n",
      "       848           0.5272            1.91m\n",
      "       849           0.5270            1.90m\n",
      "       850           0.5269            1.88m\n",
      "       851           0.5267            1.87m\n",
      "       852           0.5265            1.86m\n",
      "       853           0.5265            1.84m\n",
      "       854           0.5264            1.83m\n",
      "       855           0.5263            1.82m\n",
      "       856           0.5262            1.81m\n",
      "       857           0.5261            1.79m\n",
      "       858           0.5260            1.78m\n",
      "       859           0.5258            1.77m\n",
      "       860           0.5257            1.76m\n",
      "       861           0.5256            1.75m\n",
      "       862           0.5255            1.73m\n",
      "       863           0.5255            1.72m\n",
      "       864           0.5255            1.71m\n",
      "       865           0.5254            1.69m\n",
      "       866           0.5253            1.68m\n",
      "       867           0.5253            1.67m\n",
      "       868           0.5252            1.66m\n",
      "       869           0.5251            1.64m\n",
      "       870           0.5250            1.63m\n",
      "       871           0.5250            1.62m\n",
      "       872           0.5250            1.61m\n",
      "       873           0.5250            1.59m\n",
      "       874           0.5249            1.58m\n",
      "       875           0.5248            1.57m\n",
      "       876           0.5248            1.56m\n",
      "       877           0.5247            1.54m\n",
      "       878           0.5247            1.53m\n",
      "       879           0.5246            1.52m\n",
      "       880           0.5245            1.50m\n",
      "       881           0.5244            1.49m\n",
      "       882           0.5243            1.48m\n",
      "       883           0.5242            1.47m\n",
      "       884           0.5241            1.45m\n",
      "       885           0.5240            1.44m\n",
      "       886           0.5239            1.43m\n",
      "       887           0.5239            1.42m\n",
      "       888           0.5239            1.40m\n",
      "       889           0.5238            1.39m\n",
      "       890           0.5237            1.38m\n",
      "       891           0.5237            1.37m\n",
      "       892           0.5236            1.35m\n",
      "       893           0.5236            1.34m\n",
      "       894           0.5236            1.33m\n",
      "       895           0.5236            1.32m\n",
      "       896           0.5235            1.30m\n",
      "       897           0.5235            1.29m\n",
      "       898           0.5235            1.28m\n",
      "       899           0.5234            1.26m\n",
      "       900           0.5234            1.25m\n",
      "       901           0.5233            1.24m\n",
      "       902           0.5233            1.23m\n",
      "       903           0.5232            1.21m\n",
      "       904           0.5231            1.20m\n",
      "       905           0.5230            1.19m\n",
      "       906           0.5229            1.18m\n",
      "       907           0.5227            1.16m\n",
      "       908           0.5226            1.15m\n",
      "       909           0.5225            1.14m\n",
      "       910           0.5223            1.13m\n",
      "       911           0.5223            1.11m\n",
      "       912           0.5221            1.10m\n",
      "       913           0.5221            1.09m\n",
      "       914           0.5220            1.08m\n",
      "       915           0.5219            1.06m\n",
      "       916           0.5218            1.05m\n",
      "       917           0.5217            1.04m\n",
      "       918           0.5216            1.03m\n",
      "       919           0.5215            1.01m\n",
      "       920           0.5215            1.00m\n",
      "       921           0.5213           59.33s\n",
      "       922           0.5211           58.59s\n",
      "       923           0.5211           57.85s\n",
      "       924           0.5210           57.09s\n",
      "       925           0.5210           56.34s\n",
      "       926           0.5208           55.59s\n",
      "       927           0.5207           54.84s\n",
      "       928           0.5205           54.09s\n",
      "       929           0.5205           53.34s\n",
      "       930           0.5204           52.58s\n",
      "       931           0.5203           51.84s\n",
      "       932           0.5202           51.09s\n",
      "       933           0.5201           50.34s\n",
      "       934           0.5200           49.59s\n",
      "       935           0.5199           48.84s\n",
      "       936           0.5198           48.09s\n",
      "       937           0.5198           47.33s\n",
      "       938           0.5196           46.58s\n",
      "       939           0.5195           45.83s\n",
      "       940           0.5194           45.08s\n",
      "       941           0.5194           44.33s\n",
      "       942           0.5194           43.57s\n",
      "       943           0.5193           42.82s\n",
      "       944           0.5193           42.07s\n",
      "       945           0.5192           41.32s\n",
      "       946           0.5192           40.56s\n",
      "       947           0.5192           39.81s\n",
      "       948           0.5192           39.05s\n",
      "       949           0.5191           38.29s\n",
      "       950           0.5191           37.54s\n",
      "       951           0.5189           36.79s\n",
      "       952           0.5188           36.04s\n",
      "       953           0.5187           35.29s\n",
      "       954           0.5187           34.54s\n",
      "       955           0.5186           33.79s\n",
      "       956           0.5185           33.04s\n",
      "       957           0.5184           32.28s\n",
      "       958           0.5183           31.53s\n",
      "       959           0.5182           30.78s\n",
      "       960           0.5182           30.02s\n",
      "       961           0.5181           29.27s\n",
      "       962           0.5181           28.52s\n",
      "       963           0.5181           27.76s\n",
      "       964           0.5180           27.01s\n",
      "       965           0.5180           26.26s\n",
      "       966           0.5179           25.50s\n",
      "       967           0.5179           24.75s\n",
      "       968           0.5179           24.00s\n",
      "       969           0.5178           23.25s\n",
      "       970           0.5178           22.50s\n",
      "       971           0.5177           21.76s\n",
      "       972           0.5176           21.00s\n",
      "       973           0.5176           20.25s\n",
      "       974           0.5175           19.50s\n",
      "       975           0.5175           18.74s\n",
      "       976           0.5174           17.99s\n",
      "       977           0.5174           17.25s\n",
      "       978           0.5173           16.50s\n",
      "       979           0.5173           15.74s\n",
      "       980           0.5172           14.99s\n",
      "       981           0.5172           14.24s\n",
      "       982           0.5171           13.49s\n",
      "       983           0.5170           12.74s\n",
      "       984           0.5170           11.99s\n",
      "       985           0.5170           11.24s\n",
      "       986           0.5170           10.49s\n",
      "       987           0.5170            9.74s\n",
      "       988           0.5169            8.99s\n",
      "       989           0.5169            8.24s\n",
      "       990           0.5169            7.49s\n",
      "       991           0.5168            6.74s\n",
      "       992           0.5167            5.99s\n",
      "       993           0.5167            5.24s\n",
      "       994           0.5167            4.49s\n",
      "       995           0.5166            3.74s\n",
      "       996           0.5165            2.99s\n",
      "       997           0.5165            2.24s\n",
      "       998           0.5164            1.50s\n",
      "       999           0.5163            0.75s\n",
      "      1000           0.5163            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=1000, presort='auto', random_state=None,\n",
       "              subsample=1.0, verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier(n_estimators=1000, max_depth=3, verbose=2)\n",
    "gbc.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90215390625586933"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembled Model #8: PassiveAggressiveClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/utils/validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.00, NNZs: 41, Bias: -0.000001, T: 399321, Avg. loss: 1.138124\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.00, NNZs: 41, Bias: -0.000002, T: 798642, Avg. loss: 1.126260\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.00, NNZs: 41, Bias: -0.000003, T: 1197963, Avg. loss: 1.120333\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.00, NNZs: 41, Bias: -0.000004, T: 1597284, Avg. loss: 1.114581\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.00, NNZs: 41, Bias: -0.000006, T: 1996605, Avg. loss: 1.109557\n",
      "Total training time: 0.50 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PassiveAggressiveClassifier(C=1.0, class_weight=None, fit_intercept=True,\n",
       "              loss='hinge', n_iter=5, n_jobs=-1, random_state=None,\n",
       "              shuffle=True, verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pac = PassiveAggressiveClassifier(n_iter=5, verbose=2, n_jobs=-1)\n",
    "pac.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81366118986980396"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pac.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembled Model #9: RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/ridge.py:780: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
       "        max_iter=None, normalize=False, random_state=None, solver='auto',\n",
       "        tol=0.001)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc = RidgeClassifier()\n",
    "rc.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79612642460576832"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Emsemble Model with LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=36)]: Done  50 out of 100 | elapsed:    0.8s remaining:    0.8s\n",
      "[Parallel(n_jobs=36)]: Done  71 out of 100 | elapsed:    0.9s remaining:    0.4s\n",
      "[Parallel(n_jobs=36)]: Done  92 out of 100 | elapsed:    1.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=36)]: Done 100 out of 100 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=36)]: Done   3 out of  36 | elapsed:    2.4s remaining:   26.4s\n",
      "[Parallel(n_jobs=36)]: Done  22 out of  36 | elapsed:    7.2s remaining:    4.6s\n",
      "[Parallel(n_jobs=36)]: Done  36 out of  36 | elapsed:   10.6s finished\n",
      "[Parallel(n_jobs=36)]: Done  90 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=36)]: Done 200 out of 200 | elapsed:    2.8s finished\n"
     ]
    }
   ],
   "source": [
    "pred_train_rf = rf.predict(X_train)\n",
    "pred_train_knc = knc.predict(X_train)\n",
    "pred_train_sgd = sgd.predict(X_train)\n",
    "pred_train_abc = abc.predict(X_train)\n",
    "pred_train_bc = bc.predict(X_train)\n",
    "pred_train_etc = etc.predict(X_train)\n",
    "pred_train_gbc = gbc.predict(X_train)\n",
    "pred_train_pac = pac.predict(X_train)\n",
    "pred_train_rc = rc.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "prediction_train = pd.DataFrame({\n",
    "    'rf': pred_train_rf,\n",
    "    'knc': pred_train_knc,\n",
    "    'sgd': pred_train_sgd,\n",
    "    'abc': pred_train_abc,\n",
    "    'bc': pred_train_bc,\n",
    "    'etc': pred_train_etc,\n",
    "    'gbc': pred_train_gbc,\n",
    "    'pac': pred_train_pac,\n",
    "    'rc': pred_train_rc,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/utils/validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 19 epochs took 2 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    2.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "          penalty='l2', random_state=None, solver='sag', tol=0.0001,\n",
       "          verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#ensemble = RandomForestClassifier(n_estimators=100, verbose=5, n_jobs=-1)\n",
    "ensemble = LogisticRegression(solver='sag', verbose=2, n_jobs=-1)\n",
    "ensemble.fit(prediction_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=36)]: Done  50 out of 100 | elapsed:    0.3s remaining:    0.3s\n",
      "[Parallel(n_jobs=36)]: Done  71 out of 100 | elapsed:    0.4s remaining:    0.2s\n",
      "[Parallel(n_jobs=36)]: Done  92 out of 100 | elapsed:    0.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=36)]: Done 100 out of 100 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=36)]: Done   3 out of  36 | elapsed:    1.0s remaining:   11.3s\n",
      "[Parallel(n_jobs=36)]: Done  22 out of  36 | elapsed:    3.5s remaining:    2.3s\n",
      "[Parallel(n_jobs=36)]: Done  36 out of  36 | elapsed:    5.5s finished\n",
      "[Parallel(n_jobs=36)]: Done  90 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=36)]: Done 200 out of 200 | elapsed:    1.0s finished\n"
     ]
    }
   ],
   "source": [
    "pred_test_rf = rf.predict(X_test)\n",
    "pred_test_knc = knc.predict(X_test)\n",
    "pred_test_sgd = sgd.predict(X_test)\n",
    "pred_test_abc = abc.predict(X_test)\n",
    "pred_test_bc = bc.predict(X_test)\n",
    "pred_test_etc = etc.predict(X_test)\n",
    "pred_test_gbc = gbc.predict(X_test)\n",
    "pred_test_pac = pac.predict(X_test)\n",
    "pred_test_rc = rc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "prediction_test = pd.DataFrame({\n",
    "    'rf': pred_test_rf,\n",
    "    'knc': pred_test_knc,\n",
    "    'sgd': pred_test_sgd,\n",
    "    'abc': pred_test_abc,\n",
    "    'bc': pred_test_bc,\n",
    "    'etc': pred_test_etc,\n",
    "    'gbc': pred_test_gbc,\n",
    "    'pac': pred_test_pac,\n",
    "    'rc': pred_test_rc,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86176534667598248"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble.score(prediction_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fi = list(zip(prediction_test.columns.values, ensemble.feature_importances_))\n",
    "fi = sorted(fi, key=lambda x: -x[1])\n",
    "pd.DataFrame(fi, columns=[\"Feature\",\"Importance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction_train.to_csv('ensemble_train.csv', index=False)\n",
    "prediction_test.to_csv('ensemble_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Observation**:  In RandomForest Ensemble, performance is occupied by the best model. Does not increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Ensemble Model with Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensemble_vote = VotingClassifier(estimators=[\n",
    "    ('rf', rf),\n",
    "    ('knc', knc),\n",
    "    ('sgd', sgd),\n",
    "    ('abc', abc),\n",
    "    ('bc', bc),\n",
    "    ('etc', etc),\n",
    "    ('gbc', gbc),\n",
    "    ('pac', pac),\n",
    "    ('rc', rc)\n",
    "], voting='soft', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/preprocessing/label.py:108: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/preprocessing/label.py:143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 3 of 100\n",
      "building tree 14 of 100\n",
      "building tree 6 of 100\n",
      "building tree 19 of 100\n",
      "building tree 11 of 100\n",
      "building tree 25 of 100\n",
      "building tree 20 of 100\n",
      "building tree 16 of 100\n",
      "building tree 31 of 100\n",
      "building tree 21 of 100\n",
      "building tree 7 of 100\n",
      "building tree 15 of 100\n",
      "building tree 22 of 100\n",
      "building tree 17 of 100\n",
      "building tree 4 of 100\n",
      "building tree 18 of 100\n",
      "building tree 12 of 100\n",
      "building tree 27 of 100\n",
      "building tree 34 of 100\n",
      "building tree 23 of 100\n",
      "building tree 5 of 100\n",
      "building tree 32 of 100\n",
      "building tree 26 of 100\n",
      "building tree 36 of 100\n",
      "building tree 10 of 100\n",
      "building tree 29 of 100\n",
      "building tree 1 of 100\n",
      "building tree 28 of 100\n",
      "building tree 24 of 100\n",
      "building tree 9 of 100\n",
      "building tree 30 of 100\n",
      "building tree 13 of 100\n",
      "building tree 2 of 100\n",
      "building tree 8 of 100\n",
      "building tree 35 of 100\n",
      "building tree 33 of 100\n",
      "-- Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 324954.96, NNZs: 41, Bias: 2406.741789, T: 798642, Avg. loss: 10692641343.489632\n",
      "Norm: 419793.33, NNZs: 41, Bias: 2252.797814, T: 399321, Avg. loss: 19000028890.553448\n",
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 2\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 3\n",
      "building tree 37 of 100\n",
      "Norm: 278557.05, NNZs: 41, Bias: 2465.833951, T: 1197963, Avg. loss: 7595170118.520350\n",
      "Total training time: 0.87 seconds.\n",
      "-- Epoch 4\n",
      "building tree 38 of 100\n",
      "Norm: 251000.74, NNZs: 41, Bias: 2505.691041, T: 1597284, Avg. loss: 5947894317.413574\n",
      "Total training time: 1.16 seconds.\n",
      "-- Epoch 5\n",
      "building tree 39 of 100\n",
      "Norm: 232633.09, NNZs: 41, Bias: 2540.212348, T: 1996605, Avg. loss: 4913409689.636926\n",
      "Total training time: 1.38 seconds.\n",
      "-- Epoch 6\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "Norm: 219106.15, NNZs: 41, Bias: 2564.729045, T: 2395926, Avg. loss: 4199689236.775273\n",
      "Total training time: 1.58 seconds.\n",
      "-- Epoch 7\n",
      "building tree 43 of 100\n",
      "building tree 42 of 100\n",
      "building tree 44 of 100\n",
      "building tree 8 of 200\n",
      "building tree 4 of 200\n",
      "building tree 2 of 200\n",
      "building tree 7 of 200\n",
      "building tree 13 of 200\n",
      "building tree 9 of 200\n",
      "building tree 14 of 200\n",
      "building tree 19 of 200\n",
      "building tree 18 of 200\n",
      "building tree 20 of 200\n",
      "building tree 45 of 100\n",
      "building tree 16 of 200\n",
      "building tree 22 of 200\n",
      "building tree 21 of 200\n",
      "building tree 6 of 200\n",
      "building tree 15 of 200\n",
      "building tree 10 of 200\n",
      "building tree 17 of 200\n",
      "building tree 24 of 200\n",
      "building tree 29 of 200\n",
      "building tree 5 of 200\n",
      "Norm: 207831.59, NNZs: 41, Bias: 2583.619547, T: 2795247, Avg. loss: 3676934025.014419\n",
      "building tree 32 of 200\n",
      "Total training time: 1.81 seconds.\n",
      "building tree 11 of 200\n",
      "building tree 33 of 200\n",
      "-- Epoch 8\n",
      "building tree 23 of 200\n",
      "building tree 12 of 200\n",
      "building tree 1 of 200\n",
      "building tree 30 of 200\n",
      "building tree 28 of 200\n",
      "building tree 31 of 200\n",
      "building tree 35 of 200\n",
      "building tree 34 of 200\n",
      "building tree 25 of 200\n",
      "building tree 46 of 100\n",
      "building tree 3 of 200\n",
      "building tree 26 of 200\n",
      "building tree 36 of 200\n",
      "Norm: 198885.36, NNZs: 41, Bias: 2596.182006, T: 3194568, Avg. loss: 3274794584.587135\n",
      "Total training time: 2.03 seconds.\n",
      "building tree 47 of 100\n",
      "-- Epoch 9\n",
      "building tree 48 of 100\n",
      "building tree 27 of 200\n",
      "building tree 49 of 100\n",
      "Norm: 190441.71, NNZs: 41, Bias: 2604.722364, T: 3593889, Avg. loss: 2955609239.234265\n",
      "Total training time: 2.32 seconds.\n",
      "building tree 50 of 100\n",
      "-- Epoch 10\n",
      "building tree 53 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 54 of 100\n",
      "Norm: 183645.60, NNZs: 41, Bias: 2614.289548, T: 3993210, Avg. loss: 2696425072.526832\n",
      "Total training time: 2.57 seconds.\n",
      "building tree 55 of 100\n",
      "-- Epoch 11\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "Norm: 177012.97, NNZs: 41, Bias: 2619.588873, T: 4392531, Avg. loss: 2480476053.396455\n",
      "Total training time: 2.94 seconds.\n",
      "-- Epoch 12\n",
      "building tree 59 of 100\n",
      "Norm: 172228.65, NNZs: 41, Bias: 2627.332548, T: 4791852, Avg. loss: 2298982293.445588\n",
      "Total training time: 3.16 seconds.\n",
      "-- Epoch 13\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "      Iter       Train Loss   Remaining Time \n",
      "Norm: 167357.39, NNZs: 41, Bias: 2631.061387, T: 5191173, Avg. loss: 2143639808.911852\n",
      "Total training time: 3.45 seconds.\n",
      "-- Epoch 14\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "Norm: 162952.26, NNZs: 41, Bias: 2634.519429, T: 5590494, Avg. loss: 2008131849.283919\n",
      "Total training time: 3.95 seconds.\n",
      "-- Epoch 15\n",
      "building tree 65 of 100\n",
      "-- Epoch 1\n",
      "building tree 66 of 100\n",
      "Norm: 159431.64, NNZs: 41, Bias: 2639.249818, T: 5989815, Avg. loss: 1889737241.484176\n",
      "Total training time: 4.15 seconds.\n",
      "building tree 64 of 100\n",
      "-- Epoch 16\n",
      "building tree 67 of 100\n",
      "Norm: 0.00, NNZs: 41, Bias: -0.000001, T: 399321, Avg. loss: 1.138630\n",
      "Norm: 155980.09, NNZs: 41, Bias: 2642.295464, T: 6389136, Avg. loss: 1785139586.144243\n",
      "Total training time: 0.55 seconds.\n",
      "Total training time: 4.59 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 17\n",
      "Norm: 152870.07, NNZs: 41, Bias: 2644.338623, T: 6788457, Avg. loss: 1692645822.415324\n",
      "Total training time: 4.95 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 0.00, NNZs: 41, Bias: -0.000002, T: 798642, Avg. loss: 1.126902\n",
      "Total training time: 1.06 seconds.\n",
      "-- Epoch 3\n",
      "building tree 68 of 100\n",
      "Norm: 149776.71, NNZs: 41, Bias: 2645.869129, T: 7187778, Avg. loss: 1609604895.443578\n",
      "Total training time: 5.41 seconds.\n",
      "-- Epoch 19\n",
      "building tree 69 of 100\n",
      "Norm: 0.00, NNZs: 41, Bias: -0.000003, T: 1197963, Avg. loss: 1.119265\n",
      "Total training time: 1.44 seconds.\n",
      "-- Epoch 4\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "Norm: 0.00, NNZs: 41, Bias: -0.000004, T: 1597284, Avg. loss: 1.113470\n",
      "building tree 72 of 100\n",
      "Total training time: 1.83 seconds.\n",
      "-- Epoch 5\n",
      "building tree 73 of 100\n",
      "Norm: 146907.06, NNZs: 41, Bias: 2647.454755, T: 7587099, Avg. loss: 1534410074.955326\n",
      "Total training time: 5.99 seconds.\n",
      "-- Epoch 20\n",
      "building tree 74 of 100\n",
      "Norm: 0.00, NNZs: 41, Bias: -0.000006, T: 1996605, Avg. loss: 1.108317\n",
      "Total training time: 2.20 seconds.\n",
      "Norm: 145743.66, NNZs: 41, Bias: 2647.440614, T: 7986420, Avg. loss: 1466488732.379658\n",
      "Total training time: 6.32 seconds.\n",
      "-- Epoch 21\n",
      "building tree 37 of 200\n",
      "building tree 38 of 200\n",
      "Norm: 141635.57, NNZs: 41, Bias: 2647.883127, T: 8385741, Avg. loss: 1404610177.169235\n",
      "Total training time: 6.56 seconds.\n",
      "-- Epoch 22\n",
      "building tree 39 of 200\n",
      "building tree 40 of 200\n",
      "building tree 41 of 200\n",
      "building tree 43 of 200\n",
      "building tree 42 of 200\n",
      "building tree 44 of 200\n",
      "Norm: 139190.13, NNZs: 41, Bias: 2648.125228, T: 8785062, Avg. loss: 1348039099.382280\n",
      "Total training time: 6.97 seconds.\n",
      "-- Epoch 23\n",
      "building tree 75 of 100\n",
      "building tree 46 of 200\n",
      "building tree 47 of 200\n",
      "building tree 45 of 200\n",
      "building tree 48 of 200\n",
      "building tree 76 of 100\n",
      "building tree 49 of 200\n",
      "building tree 50 of 200\n",
      "Norm: 136832.28, NNZs: 41, Bias: 2648.465013, T: 9184383, Avg. loss: 1296050469.928288\n",
      "Total training time: 7.44 seconds.\n",
      "building tree 51 of 200\n",
      "-- Epoch 24\n",
      "building tree 52 of 200\n",
      "building tree 53 of 200\n",
      "building tree 55 of 200\n",
      "building tree 54 of 200\n",
      "building tree 77 of 100\n",
      "building tree 56 of 200\n",
      "building tree 57 of 200\n",
      "building tree 58 of 200\n",
      "Norm: 134802.87, NNZs: 41, Bias: 2649.883368, T: 9583704, Avg. loss: 1248147343.274892\n",
      "Total training time: 7.77 seconds.\n",
      "building tree 60 of 200\n",
      "-- Epoch 25\n",
      "building tree 59 of 200\n",
      "building tree 78 of 100\n",
      "building tree 80 of 100\n",
      "building tree 62 of 200\n",
      "building tree 79 of 100\n",
      "         1           1.0483           76.33m\n",
      "building tree 61 of 200\n",
      "building tree 64 of 200\n",
      "building tree 66 of 200\n",
      "building tree 65 of 200\n",
      "building tree 63 of 200\n",
      "building tree 81 of 100\n",
      "building tree 67 of 200\n",
      "Norm: 132599.91, NNZs: 41, Bias: 2648.814687, T: 9983025, Avg. loss: 1203703947.677973\n",
      "Total training time: 8.18 seconds.\n",
      "-- Epoch 26\n",
      "building tree 82 of 100\n",
      "Norm: 130658.85, NNZs: 41, Bias: 2648.252226, T: 10382346, Avg. loss: 1162541113.319745\n",
      "Total training time: 8.58 seconds.\n",
      "-- Epoch 27\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 69 of 200\n",
      "building tree 68 of 200\n",
      "Norm: 128924.64, NNZs: 41, Bias: 2648.445733, T: 10781667, Avg. loss: 1124386793.045262\n",
      "building tree 70 of 200\n",
      "Total training time: 9.01 seconds.\n",
      "-- Epoch 28\n",
      "building tree 71 of 200\n",
      "building tree 72 of 200\n",
      "building tree 85 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  50 out of 100 | elapsed:   12.0s remaining:   12.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "Norm: 127171.56, NNZs: 41, Bias: 2647.978866, T: 11180988, Avg. loss: 1088695714.101491\n",
      "building tree 88 of 100\n",
      "Total training time: 9.47 seconds.\n",
      "-- Epoch 29\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "Norm: 125482.78, NNZs: 41, Bias: 2647.258658, T: 11580309, Avg. loss: 1055278196.438360\n",
      "building tree 92 of 100\n",
      "Total training time: 9.90 seconds.\n",
      "-- Epoch 30\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n",
      "building tree 95 of 100\n",
      "Norm: 123816.59, NNZs: 41, Bias: 2646.964587, T: 11979630, Avg. loss: 1023988800.163254\n",
      "Total training time: 10.34 seconds.\n",
      "building tree 96 of 100\n",
      "-- Epoch 31\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 73 of 200\n",
      "Norm: 122662.15, NNZs: 41, Bias: 2645.833783, T: 12378951, Avg. loss: 994635320.490451\n",
      "Total training time: 10.75 seconds.\n",
      "-- Epoch 32\n",
      "building tree 74 of 200\n",
      "building tree 76 of 200\n",
      "building tree 75 of 200\n",
      "building tree 77 of 200\n",
      "building tree 100 of 100\n",
      "building tree 78 of 200\n",
      "building tree 79 of 200\n",
      "Norm: 120726.74, NNZs: 41, Bias: 2645.189998, T: 12778272, Avg. loss: 966796539.375204\n",
      "Total training time: 11.20 seconds.\n",
      "-- Epoch 33\n",
      "building tree 80 of 200\n",
      "building tree 81 of 200\n",
      "Norm: 119298.86, NNZs: 41, Bias: 2644.285626, T: 13177593, Avg. loss: 940649439.538415\n",
      "Total training time: 11.61 seconds.\n",
      "-- Epoch 34\n",
      "building tree 83 of 200\n",
      "building tree 82 of 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  71 out of 100 | elapsed:   14.4s remaining:    5.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 84 of 200\n",
      "Norm: 118011.69, NNZs: 41, Bias: 2643.825400, T: 13576914, Avg. loss: 916043736.924312\n",
      "Total training time: 11.98 seconds.\n",
      "-- Epoch 35\n",
      "building tree 85 of 200\n",
      "building tree 86 of 200\n",
      "building tree 87 of 200\n",
      "building tree 88 of 200\n",
      "building tree 89 of 200\n",
      "building tree 91 of 200\n",
      "building tree 90 of 200\n",
      "Norm: 116813.86, NNZs: 41, Bias: 2643.265037, T: 13976235, Avg. loss: 892719157.793785\n",
      "building tree 92 of 200\n",
      "Total training time: 12.43 seconds.\n",
      "-- Epoch 36\n",
      "building tree 94 of 200\n",
      "         2           1.0173           77.41m\n",
      "building tree 93 of 200\n",
      "building tree 95 of 200\n",
      "Norm: 115445.81, NNZs: 41, Bias: 2641.932374, T: 14375556, Avg. loss: 870544458.983413\n",
      "Total training time: 12.88 seconds.\n",
      "-- Epoch 37\n",
      "building tree 96 of 200\n",
      "building tree 97 of 200\n",
      "building tree 98 of 200\n",
      "Norm: 114066.23, NNZs: 41, Bias: 2640.692621, T: 14774877, Avg. loss: 849487664.145281\n",
      "Total training time: 13.30 seconds.\n",
      "-- Epoch 38\n",
      "building tree 99 of 200\n",
      "building tree 100 of 200\n",
      "building tree 101 of 200\n",
      "Norm: 112899.62, NNZs: 41, Bias: 2639.750210, T: 15174198, Avg. loss: 829519721.584826\n",
      "building tree 102 of 200\n",
      "Total training time: 13.71 seconds.\n",
      "building tree 103 of 200\n",
      "-- Epoch 39\n",
      "building tree 104 of 200\n",
      "building tree 105 of 200\n",
      "building tree 106 of 200\n",
      "building tree 107 of 200\n",
      "Norm: 111683.51, NNZs: 41, Bias: 2638.752106, T: 15573519, Avg. loss: 810514424.774380\n",
      "Total training time: 14.21 seconds.\n",
      "-- Epoch 40\n",
      "building tree 108 of 200\n",
      "building tree 109 of 200\n",
      "Norm: 110681.17, NNZs: 41, Bias: 2637.946248, T: 15972840, Avg. loss: 792369644.579161\n",
      "Total training time: 14.64 seconds.\n",
      "-- Epoch 41\n",
      "building tree 110 of 200\n",
      "building tree 111 of 200\n",
      "building tree 112 of 200\n",
      "Norm: 109559.51, NNZs: 41, Bias: 2637.059973, T: 16372161, Avg. loss: 775110103.459170\n",
      "Total training time: 14.95 seconds.\n",
      "-- Epoch 42\n",
      "building tree 113 of 200\n",
      "building tree 114 of 200\n",
      "building tree 115 of 200\n",
      "Norm: 108452.60, NNZs: 41, Bias: 2635.526783, T: 16771482, Avg. loss: 758592896.401264\n",
      "Total training time: 15.26 seconds.\n",
      "building tree 117 of 200\n",
      "-- Epoch 43\n",
      "building tree 116 of 200\n",
      "building tree 118 of 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  92 out of 100 | elapsed:   18.0s remaining:    1.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 119 of 200\n",
      "building tree 120 of 200\n",
      "Norm: 107464.54, NNZs: 41, Bias: 2634.842177, T: 17170803, Avg. loss: 742864282.080491\n",
      "Total training time: 15.56 seconds.\n",
      "-- Epoch 44\n",
      "building tree 121 of 200\n",
      "building tree 122 of 200\n",
      "building tree 123 of 200\n",
      "building tree 124 of 200\n",
      "building tree 125 of 200\n",
      "Norm: 106446.90, NNZs: 41, Bias: 2633.699775, T: 17570124, Avg. loss: 727740886.769196\n",
      "Total training time: 15.81 seconds.\n",
      "-- Epoch 45\n",
      "building tree 127 of 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:   14.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 126 of 200\n",
      "Norm: 105420.79, NNZs: 41, Bias: 2632.439303, T: 17969445, Avg. loss: 713192733.114041\n",
      "Total training time: 16.02 seconds.\n",
      "-- Epoch 46\n",
      "building tree 128 of 200\n",
      "building tree 129 of 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   18.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 104405.64, NNZs: 41, Bias: 2631.528240, T: 18368766, Avg. loss: 699321061.227770\n",
      "Total training time: 16.21 seconds.\n",
      "         3           0.9938           71.47m\n",
      "-- Epoch 47\n",
      "building tree 130 of 200\n",
      "building tree 131 of 200\n",
      "building tree 132 of 200\n",
      "Norm: 103445.97, NNZs: 41, Bias: 2630.536900, T: 18768087, Avg. loss: 686004288.466488\n",
      "Total training time: 16.46 seconds.\n",
      "-- Epoch 48\n",
      "building tree 133 of 200\n",
      "Norm: 102537.25, NNZs: 41, Bias: 2629.526612, T: 19167408, Avg. loss: 673197326.060853\n",
      "Total training time: 16.64 seconds.\n",
      "-- Epoch 49\n",
      "building tree 134 of 200\n",
      "building tree 135 of 200\n",
      "building tree 136 of 200\n",
      "Norm: 101638.55, NNZs: 41, Bias: 2628.975290, T: 19566729, Avg. loss: 660895915.426543\n",
      "Total training time: 16.83 seconds.\n",
      "-- Epoch 50\n",
      "building tree 137 of 200\n",
      "building tree 138 of 200\n",
      "Norm: 100727.29, NNZs: 41, Bias: 2628.054074, T: 19966050, Avg. loss: 649070890.884876\n",
      "Total training time: 17.01 seconds.\n",
      "-- Epoch 51\n",
      "building tree 139 of 200\n",
      "building tree 140 of 200\n",
      "building tree 141 of 200\n",
      "Norm: 99893.14, NNZs: 41, Bias: 2627.208398, T: 20365371, Avg. loss: 637674939.583460\n",
      "Total training time: 17.23 seconds.\n",
      "-- Epoch 52\n",
      "building tree 142 of 200\n",
      "building tree 143 of 200\n",
      "Norm: 99124.14, NNZs: 41, Bias: 2626.805958, T: 20764692, Avg. loss: 626683290.196028\n",
      "Total training time: 17.44 seconds.\n",
      "-- Epoch 53\n",
      "building tree 144 of 200\n",
      "building tree 146 of 200\n",
      "building tree 145 of 200\n",
      "Norm: 98321.35, NNZs: 41, Bias: 2625.639277, T: 21164013, Avg. loss: 616055016.870142\n",
      "Total training time: 17.62 seconds.\n",
      "-- Epoch 54\n",
      "building tree 147 of 200\n",
      "building tree 148 of 200\n",
      "building tree 150 of 200\n",
      "building tree 149 of 200\n",
      "Norm: 97587.41, NNZs: 41, Bias: 2625.713599, T: 21563334, Avg. loss: 605868203.168873\n",
      "Total training time: 17.83 seconds.\n",
      "building tree 151 of 200\n",
      "-- Epoch 55\n",
      "building tree 152 of 200\n",
      "building tree 153 of 200\n",
      "building tree 154 of 200\n",
      "building tree 155 of 200\n",
      "building tree 156 of 200\n",
      "building tree 157 of 200\n",
      "Norm: 96755.52, NNZs: 41, Bias: 2624.378678, T: 21962655, Avg. loss: 595989410.843667\n",
      "-- Epoch 57\n",
      "Total training time: 18.07 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 95991.84, NNZs: 41, Bias: 2623.497826, T: 22361976, Avg. loss: 586446973.677930\n",
      "Total training time: 18.26 seconds.\n",
      "building tree 158 of 200\n",
      "building tree 160 of 200\n",
      "building tree 159 of 200\n",
      "building tree 161 of 200\n",
      "Norm: 95226.02, NNZs: 41, Bias: 2622.752680, T: 22761297, Avg. loss: 577236525.901074\n",
      "-- Epoch 58\n",
      "building tree 162 of 200\n",
      "Total training time: 18.51 seconds.\n",
      "building tree 164 of 200\n",
      "building tree 163 of 200\n",
      "         4           0.9753           63.68m\n",
      "Norm: 94542.46, NNZs: 41, Bias: 2622.246004, T: 23160618, Avg. loss: 568298074.050193\n",
      "Total training time: 18.72 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 93807.22, NNZs: 41, Bias: 2621.002978, T: 23559939, Avg. loss: 559611913.316221\n",
      "Total training time: 18.98 seconds.\n",
      "-- Epoch 60\n",
      "building tree 166 of 200\n",
      "building tree 165 of 200\n",
      "building tree 168 of 200\n",
      "building tree 167 of 200\n",
      "Norm: 93131.32, NNZs: 41, Bias: 2620.055602, T: 23959260, Avg. loss: 551240334.228626\n",
      "building tree 169 of 200\n",
      "Total training time: 19.22 seconds.\n",
      "-- Epoch 61\n",
      "building tree 170 of 200\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "building tree 171 of 200\n",
      "building tree 172 of 200\n",
      "Norm: 92457.10, NNZs: 41, Bias: 2619.376026, T: 24358581, Avg. loss: 543134195.195389\n",
      "building tree 173 of 200\n",
      "Total training time: 19.54 seconds.\n",
      "-- Epoch 62\n",
      "building tree 175 of 200\n",
      "building tree 174 of 200\n",
      "building tree 176 of 200\n",
      "Norm: 91823.83, NNZs: 41, Bias: 2619.043802, T: 24757902, Avg. loss: 535276702.653061\n",
      "Total training time: 19.75 seconds.\n",
      "-- Epoch 63\n",
      "building tree 177 of 200\n",
      "building tree 178 of 200\n",
      "building tree 179 of 200\n",
      "building tree 180 of 200\n",
      "Norm: 91145.76, NNZs: 41, Bias: 2618.058230, T: 25157223, Avg. loss: 527655158.679222\n",
      "Total training time: 19.95 seconds.\n",
      "building tree 182 of 200\n",
      "building tree 181 of 200\n",
      "-- Epoch 64\n",
      "building tree 183 of 200\n",
      "building tree 184 of 200\n",
      "Norm: 90456.97, NNZs: 41, Bias: 2617.323668, T: 25556544, Avg. loss: 520245588.448209\n",
      "Total training time: 20.18 seconds.\n",
      "-- Epoch 65\n",
      "building tree 185 of 200\n",
      "building tree 188 of 200\n",
      "building tree 186 of 200\n",
      "building tree 187 of 200\n",
      "Norm: 89832.78, NNZs: 41, Bias: 2616.265507, T: 25955865, Avg. loss: 513049889.528770\n",
      "Total training time: 20.41 seconds.\n",
      "-- Epoch 66\n",
      "building tree 189 of 200\n",
      "building tree 190 of 200\n",
      "building tree 191 of 200\n",
      "building tree 192 of 200\n",
      "building tree 193 of 200\n",
      "Norm: 89204.45, NNZs: 41, Bias: 2615.636018, T: 26355186, Avg. loss: 506062072.252433\n",
      "Total training time: 20.64 seconds.\n",
      "-- Epoch 67\n",
      "building tree 194 of 200\n",
      "building tree 195 of 200\n",
      "building tree 196 of 200\n",
      "Norm: 88609.57, NNZs: 41, Bias: 2614.811590, T: 26754507, Avg. loss: 499272324.376491\n",
      "Total training time: 20.86 seconds.\n",
      "-- Epoch 68\n",
      "building tree 197 of 200\n",
      "building tree 198 of 200\n",
      "building tree 199 of 200\n",
      "Norm: 88032.70, NNZs: 41, Bias: 2613.981423, T: 27153828, Avg. loss: 492649307.309405\n",
      "Total training time: 21.08 seconds.\n",
      "-- Epoch 69\n",
      "building tree 200 of 200\n",
      "Norm: 87470.96, NNZs: 41, Bias: 2613.775203, T: 27553149, Avg. loss: 486253521.939199\n",
      "Total training time: 21.28 seconds.\n",
      "-- Epoch 70\n",
      "         5           0.9603           59.74m\n",
      "Norm: 86854.35, NNZs: 41, Bias: 2613.020558, T: 27952470, Avg. loss: 480021998.199536\n",
      "Total training time: 21.48 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 86328.09, NNZs: 41, Bias: 2612.724846, T: 28351791, Avg. loss: 473943878.762510\n",
      "Total training time: 21.77 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 85772.63, NNZs: 41, Bias: 2611.789371, T: 28751112, Avg. loss: 468011026.990248\n",
      "Total training time: 21.95 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 85226.50, NNZs: 41, Bias: 2610.885744, T: 29150433, Avg. loss: 462247058.409202\n",
      "Total training time: 22.13 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 84669.46, NNZs: 41, Bias: 2610.309864, T: 29549754, Avg. loss: 456623390.896617\n",
      "Total training time: 22.30 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 84120.56, NNZs: 41, Bias: 2609.571710, T: 29949075, Avg. loss: 451159840.869331\n",
      "Total training time: 22.42 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 83623.80, NNZs: 41, Bias: 2609.077531, T: 30348396, Avg. loss: 445815130.673572\n",
      "Total training time: 22.54 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 83135.36, NNZs: 41, Bias: 2608.299114, T: 30747717, Avg. loss: 440602667.655403\n",
      "Total training time: 22.65 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 82624.98, NNZs: 41, Bias: 2607.514649, T: 31147038, Avg. loss: 435505836.150599\n",
      "Total training time: 22.75 seconds.\n",
      "-- Epoch 79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   21.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 82121.55, NNZs: 41, Bias: 2606.921502, T: 31546359, Avg. loss: 430543612.739707\n",
      "Total training time: 22.90 seconds.\n",
      "-- Epoch 80\n",
      "         6           0.9479           54.06m\n",
      "Norm: 81665.03, NNZs: 41, Bias: 2606.752689, T: 31945680, Avg. loss: 425709911.631912\n",
      "Total training time: 23.02 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 81199.65, NNZs: 41, Bias: 2606.318157, T: 32345001, Avg. loss: 420993145.335640\n",
      "Total training time: 23.12 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 80734.40, NNZs: 41, Bias: 2605.505222, T: 32744322, Avg. loss: 416376764.518218\n",
      "Total training time: 23.27 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 80274.37, NNZs: 41, Bias: 2604.854167, T: 33143643, Avg. loss: 411856065.177952\n",
      "Total training time: 23.37 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 79848.18, NNZs: 41, Bias: 2604.599549, T: 33542964, Avg. loss: 407451860.947677\n",
      "Total training time: 23.48 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 79420.63, NNZs: 41, Bias: 2604.160625, T: 33942285, Avg. loss: 403130252.141788\n",
      "Total training time: 23.58 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 78990.04, NNZs: 41, Bias: 2603.784657, T: 34341606, Avg. loss: 398919505.410165\n",
      "Total training time: 23.68 seconds.\n",
      "-- Epoch 87\n",
      "         7           0.9375           48.18m\n",
      "Norm: 78531.42, NNZs: 41, Bias: 2603.085369, T: 34740927, Avg. loss: 394785945.108660\n",
      "Total training time: 23.78 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 78115.05, NNZs: 41, Bias: 2602.735975, T: 35140248, Avg. loss: 390736074.491678\n",
      "Total training time: 23.89 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 77708.76, NNZs: 41, Bias: 2602.198044, T: 35539569, Avg. loss: 386791187.796699\n",
      "Total training time: 23.99 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 77314.39, NNZs: 41, Bias: 2601.910751, T: 35938890, Avg. loss: 382922666.978222\n",
      "Total training time: 24.09 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 76891.13, NNZs: 41, Bias: 2601.563926, T: 36338211, Avg. loss: 379122528.153665\n",
      "Total training time: 24.19 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 76500.84, NNZs: 41, Bias: 2601.123630, T: 36737532, Avg. loss: 375403922.323566\n",
      "Total training time: 24.35 seconds.\n",
      "-- Epoch 93\n",
      "         8           0.9287           43.67m\n",
      "Norm: 76109.38, NNZs: 41, Bias: 2600.883043, T: 37136853, Avg. loss: 371749378.720269\n",
      "Total training time: 24.50 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 75695.92, NNZs: 41, Bias: 2600.406527, T: 37536174, Avg. loss: 368179957.491623\n",
      "Total training time: 24.63 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 75297.06, NNZs: 41, Bias: 2600.034129, T: 37935495, Avg. loss: 364680964.732174\n",
      "Total training time: 24.79 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 74911.21, NNZs: 41, Bias: 2599.809247, T: 38334816, Avg. loss: 361252284.060793\n",
      "Total training time: 24.92 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 74529.08, NNZs: 41, Bias: 2599.132468, T: 38734137, Avg. loss: 357889579.936076\n",
      "Total training time: 25.05 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 74142.91, NNZs: 41, Bias: 2598.506796, T: 39133458, Avg. loss: 354584575.571491\n",
      "Total training time: 25.18 seconds.\n",
      "-- Epoch 99\n",
      "         9           0.9212           40.16m\n",
      "Norm: 73772.19, NNZs: 41, Bias: 2598.210949, T: 39532779, Avg. loss: 351353828.606199\n",
      "Total training time: 25.29 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 73408.99, NNZs: 41, Bias: 2598.031566, T: 39932100, Avg. loss: 348188136.234723\n",
      "Total training time: 25.40 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 73096.75, NNZs: 41, Bias: 2597.957670, T: 40331421, Avg. loss: 345078486.695272\n",
      "Total training time: 25.55 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 72756.11, NNZs: 41, Bias: 2597.608995, T: 40730742, Avg. loss: 342022625.106420\n",
      "Total training time: 25.70 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 72393.49, NNZs: 41, Bias: 2597.252329, T: 41130063, Avg. loss: 339025704.342409\n",
      "Total training time: 25.86 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 72051.37, NNZs: 41, Bias: 2596.851039, T: 41529384, Avg. loss: 336080338.923479\n",
      "Total training time: 25.98 seconds.\n",
      "-- Epoch 105\n",
      "        10           0.9138           37.57m\n",
      "Norm: 71713.28, NNZs: 41, Bias: 2596.431513, T: 41928705, Avg. loss: 333187037.747220\n",
      "Total training time: 26.12 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 71379.79, NNZs: 41, Bias: 2596.462758, T: 42328026, Avg. loss: 330353210.950520\n",
      "Total training time: 26.23 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 71031.77, NNZs: 41, Bias: 2596.128273, T: 42727347, Avg. loss: 327562557.650498\n",
      "Total training time: 26.38 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 70666.39, NNZs: 41, Bias: 2595.788939, T: 43126668, Avg. loss: 324820323.011200\n",
      "Total training time: 26.54 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 70342.07, NNZs: 41, Bias: 2595.544479, T: 43525989, Avg. loss: 322122741.230981\n",
      "Total training time: 26.67 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 70005.25, NNZs: 41, Bias: 2595.187218, T: 43925310, Avg. loss: 319477768.018784\n",
      "Total training time: 26.80 seconds.\n",
      "-- Epoch 111\n",
      "        11           0.9082           35.33m\n",
      "Norm: 69690.85, NNZs: 41, Bias: 2594.973574, T: 44324631, Avg. loss: 316874309.451790\n",
      "Total training time: 26.94 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 69364.53, NNZs: 41, Bias: 2594.648027, T: 44723952, Avg. loss: 314314405.637909\n",
      "Total training time: 27.05 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 69072.49, NNZs: 41, Bias: 2594.511678, T: 45123273, Avg. loss: 311807960.692342\n",
      "Total training time: 27.20 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 68770.47, NNZs: 41, Bias: 2594.228145, T: 45522594, Avg. loss: 309329850.825028\n",
      "Total training time: 27.33 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 68463.03, NNZs: 41, Bias: 2594.007878, T: 45921915, Avg. loss: 306896313.529282\n",
      "Total training time: 27.47 seconds.\n",
      "-- Epoch 116\n",
      "Norm: 68167.07, NNZs: 41, Bias: 2593.786406, T: 46321236, Avg. loss: 304505200.480247\n",
      "Total training time: 27.59 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 67872.90, NNZs: 41, Bias: 2593.765143, T: 46720557, Avg. loss: 302150653.991087\n",
      "Total training time: 27.69 seconds.\n",
      "-- Epoch 118\n",
      "        12           0.9024           33.53m\n",
      "Norm: 67569.40, NNZs: 41, Bias: 2593.430989, T: 47119878, Avg. loss: 299831396.985362\n",
      "Total training time: 27.80 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 67294.51, NNZs: 41, Bias: 2593.414895, T: 47519199, Avg. loss: 297553741.698582\n",
      "Total training time: 27.94 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 67037.32, NNZs: 41, Bias: 2593.259940, T: 47918520, Avg. loss: 295319908.706423\n",
      "Total training time: 28.10 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 66747.27, NNZs: 41, Bias: 2593.023294, T: 48317841, Avg. loss: 293112158.502909\n",
      "Total training time: 28.25 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 66460.58, NNZs: 41, Bias: 2592.692023, T: 48717162, Avg. loss: 290934021.742034\n",
      "Total training time: 28.37 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 66160.34, NNZs: 41, Bias: 2592.464502, T: 49116483, Avg. loss: 288786737.032593\n",
      "Total training time: 28.48 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 65884.88, NNZs: 41, Bias: 2592.342419, T: 49515804, Avg. loss: 286684416.809894\n",
      "Total training time: 28.58 seconds.\n",
      "-- Epoch 125\n",
      "        13           0.8963           31.97m\n",
      "Norm: 65624.52, NNZs: 41, Bias: 2592.196482, T: 49915125, Avg. loss: 284608931.819488\n",
      "Total training time: 28.69 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 65344.47, NNZs: 41, Bias: 2591.934937, T: 50314446, Avg. loss: 282564935.299685\n",
      "Total training time: 28.79 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 65089.07, NNZs: 41, Bias: 2591.846785, T: 50713767, Avg. loss: 280551592.051402\n",
      "Total training time: 28.90 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 64838.31, NNZs: 41, Bias: 2591.892964, T: 51113088, Avg. loss: 278573125.902680\n",
      "Total training time: 29.00 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 64586.56, NNZs: 41, Bias: 2591.812165, T: 51512409, Avg. loss: 276621130.129398\n",
      "Total training time: 29.14 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 64328.36, NNZs: 41, Bias: 2591.582600, T: 51911730, Avg. loss: 274691054.765029\n",
      "Total training time: 29.25 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 64064.07, NNZs: 41, Bias: 2591.552264, T: 52311051, Avg. loss: 272787080.125254\n",
      "Total training time: 29.38 seconds.\n",
      "-- Epoch 132\n",
      "        14           0.8923           30.60m\n",
      "Norm: 63829.64, NNZs: 41, Bias: 2591.458217, T: 52710372, Avg. loss: 270916486.784511\n",
      "Total training time: 29.49 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 63584.44, NNZs: 41, Bias: 2591.301920, T: 53109693, Avg. loss: 269071954.586380\n",
      "Total training time: 29.60 seconds.\n",
      "-- Epoch 134\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "Norm: 63337.82, NNZs: 41, Bias: 2591.212642, T: 53509014, Avg. loss: 267253278.430352\n",
      "Total training time: 29.76 seconds.\n",
      "-- Epoch 135\n",
      "Norm: 63077.70, NNZs: 41, Bias: 2590.720648, T: 53908335, Avg. loss: 265452102.352147\n",
      "Total training time: 29.90 seconds.\n",
      "-- Epoch 136\n",
      "Norm: 62838.45, NNZs: 41, Bias: 2590.563157, T: 54307656, Avg. loss: 263682528.892805\n",
      "Total training time: 30.04 seconds.\n",
      "-- Epoch 137\n",
      "Norm: 62604.98, NNZs: 41, Bias: 2590.578164, T: 54706977, Avg. loss: 261943442.641856\n",
      "Total training time: 30.17 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 62355.81, NNZs: 41, Bias: 2590.482817, T: 55106298, Avg. loss: 260224679.835468\n",
      "Total training time: 30.28 seconds.\n",
      "-- Epoch 139\n",
      "        15           0.8882           29.52m\n",
      "Norm: 62121.15, NNZs: 41, Bias: 2590.347011, T: 55505619, Avg. loss: 258523084.322642\n",
      "Total training time: 30.38 seconds.\n",
      "-- Epoch 140\n",
      "Norm: 61905.22, NNZs: 41, Bias: 2590.476647, T: 55904940, Avg. loss: 256855700.547531\n",
      "Total training time: 30.49 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 61672.26, NNZs: 41, Bias: 2590.391899, T: 56304261, Avg. loss: 255208693.280940\n",
      "Total training time: 30.63 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 61454.60, NNZs: 41, Bias: 2590.128209, T: 56703582, Avg. loss: 253576124.249302\n",
      "Total training time: 30.77 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 61252.50, NNZs: 41, Bias: 2590.294195, T: 57102903, Avg. loss: 251975619.020523\n",
      "Total training time: 30.90 seconds.\n",
      "-- Epoch 144\n",
      "Norm: 61014.10, NNZs: 41, Bias: 2590.170181, T: 57502224, Avg. loss: 250387639.716272\n",
      "Total training time: 31.01 seconds.\n",
      "-- Epoch 145\n",
      "Norm: 60790.79, NNZs: 41, Bias: 2590.149247, T: 57901545, Avg. loss: 248819427.537917\n",
      "Total training time: 31.12 seconds.\n",
      "-- Epoch 146\n",
      "        16           0.8844           28.59m\n",
      "Norm: 60589.72, NNZs: 41, Bias: 2590.137125, T: 58300866, Avg. loss: 247276902.138744\n",
      "Total training time: 31.28 seconds.\n",
      "-- Epoch 147\n",
      "Norm: 60368.64, NNZs: 41, Bias: 2590.176292, T: 58700187, Avg. loss: 245751768.802218\n",
      "Total training time: 31.42 seconds.\n",
      "-- Epoch 148\n",
      "Norm: 60154.63, NNZs: 41, Bias: 2590.265227, T: 59099508, Avg. loss: 244249799.517586\n",
      "Total training time: 31.55 seconds.\n",
      "-- Epoch 149\n",
      "Norm: 59950.30, NNZs: 41, Bias: 2590.259808, T: 59498829, Avg. loss: 242765475.047153\n",
      "Total training time: 31.68 seconds.\n",
      "-- Epoch 150\n",
      "Norm: 59723.59, NNZs: 41, Bias: 2590.025503, T: 59898150, Avg. loss: 241294999.178293\n",
      "Total training time: 31.82 seconds.\n",
      "-- Epoch 151\n",
      "Norm: 59510.81, NNZs: 41, Bias: 2589.830762, T: 60297471, Avg. loss: 239846011.164581\n",
      "Total training time: 31.93 seconds.\n",
      "-- Epoch 152\n",
      "Norm: 59297.61, NNZs: 41, Bias: 2589.803918, T: 60696792, Avg. loss: 238417374.058899\n",
      "Total training time: 32.03 seconds.\n",
      "-- Epoch 153\n",
      "        17           0.8813           27.77m\n",
      "Norm: 59078.40, NNZs: 41, Bias: 2589.723285, T: 61096113, Avg. loss: 237002558.383381\n",
      "Total training time: 32.20 seconds.\n",
      "-- Epoch 154\n",
      "Norm: 58881.67, NNZs: 41, Bias: 2589.775407, T: 61495434, Avg. loss: 235606638.242534\n",
      "Total training time: 32.34 seconds.\n",
      "-- Epoch 155\n",
      "Norm: 58681.01, NNZs: 41, Bias: 2589.599339, T: 61894755, Avg. loss: 234227046.667100\n",
      "Total training time: 32.48 seconds.\n",
      "-- Epoch 156\n",
      "Norm: 58483.99, NNZs: 41, Bias: 2589.574053, T: 62294076, Avg. loss: 232864206.918329\n",
      "Total training time: 32.61 seconds.\n",
      "-- Epoch 157\n",
      "Norm: 58285.97, NNZs: 41, Bias: 2589.599378, T: 62693397, Avg. loss: 231521073.376769\n",
      "Total training time: 32.72 seconds.\n",
      "-- Epoch 158\n",
      "Norm: 58089.28, NNZs: 41, Bias: 2589.562037, T: 63092718, Avg. loss: 230188896.029202\n",
      "Total training time: 32.82 seconds.\n",
      "-- Epoch 159\n",
      "Norm: 57887.58, NNZs: 41, Bias: 2589.446310, T: 63492039, Avg. loss: 228874346.501666\n",
      "Total training time: 32.93 seconds.\n",
      "-- Epoch 160\n",
      "        18           0.8782           27.02m\n",
      "Norm: 57706.34, NNZs: 41, Bias: 2589.431183, T: 63891360, Avg. loss: 227575961.681841\n",
      "Total training time: 33.06 seconds.\n",
      "-- Epoch 161\n",
      "Norm: 57518.74, NNZs: 41, Bias: 2589.442421, T: 64290681, Avg. loss: 226291706.995593\n",
      "Total training time: 33.16 seconds.\n",
      "-- Epoch 162\n",
      "Norm: 57330.85, NNZs: 41, Bias: 2589.405082, T: 64690002, Avg. loss: 225020870.585068\n",
      "Total training time: 33.26 seconds.\n",
      "-- Epoch 163\n",
      "Norm: 57150.25, NNZs: 41, Bias: 2589.492818, T: 65089323, Avg. loss: 223769977.540813\n",
      "Total training time: 33.36 seconds.\n",
      "-- Epoch 164\n",
      "Norm: 56970.91, NNZs: 41, Bias: 2589.517413, T: 65488644, Avg. loss: 222535507.346433\n",
      "Total training time: 33.46 seconds.\n",
      "-- Epoch 165\n",
      "Norm: 56789.25, NNZs: 41, Bias: 2589.507354, T: 65887965, Avg. loss: 221309869.793065\n",
      "Total training time: 33.57 seconds.\n",
      "-- Epoch 166\n",
      "Norm: 56603.82, NNZs: 41, Bias: 2589.504472, T: 66287286, Avg. loss: 220100926.233505\n",
      "Total training time: 33.67 seconds.\n",
      "-- Epoch 167\n",
      "Norm: 56420.92, NNZs: 41, Bias: 2589.353982, T: 66686607, Avg. loss: 218904756.063642\n",
      "Total training time: 33.77 seconds.\n",
      "-- Epoch 168\n",
      "Norm: 56244.26, NNZs: 41, Bias: 2589.447009, T: 67085928, Avg. loss: 217724046.023526\n",
      "Total training time: 33.87 seconds.\n",
      "-- Epoch 169\n",
      "        19           0.8756           26.34m\n",
      "Norm: 56066.78, NNZs: 41, Bias: 2589.428214, T: 67485249, Avg. loss: 216553146.546974\n",
      "Total training time: 33.97 seconds.\n",
      "-- Epoch 170\n",
      "Norm: 55897.50, NNZs: 41, Bias: 2589.612457, T: 67884570, Avg. loss: 215399587.616737\n",
      "Total training time: 34.08 seconds.\n",
      "-- Epoch 171\n",
      "Norm: 55729.49, NNZs: 41, Bias: 2589.668765, T: 68283891, Avg. loss: 214255913.359375\n",
      "Total training time: 34.18 seconds.\n",
      "-- Epoch 172\n",
      "Norm: 55542.01, NNZs: 41, Bias: 2589.618813, T: 68683212, Avg. loss: 213124123.160152\n",
      "Total training time: 34.28 seconds.\n",
      "-- Epoch 173\n",
      "Norm: 55372.16, NNZs: 41, Bias: 2589.746465, T: 69082533, Avg. loss: 212008269.931102\n",
      "Total training time: 34.38 seconds.\n",
      "-- Epoch 174\n",
      "Norm: 55203.56, NNZs: 41, Bias: 2589.781737, T: 69481854, Avg. loss: 210905497.663323\n",
      "Total training time: 34.48 seconds.\n",
      "-- Epoch 175\n",
      "Norm: 55039.52, NNZs: 41, Bias: 2589.885450, T: 69881175, Avg. loss: 209814828.627852\n",
      "Total training time: 34.59 seconds.\n",
      "-- Epoch 176\n",
      "Norm: 54868.92, NNZs: 41, Bias: 2589.853535, T: 70280496, Avg. loss: 208732595.916806\n",
      "Total training time: 34.69 seconds.\n",
      "-- Epoch 177\n",
      "        20           0.8728           25.62m\n",
      "Norm: 54696.36, NNZs: 41, Bias: 2589.875100, T: 70679817, Avg. loss: 207661549.577562\n",
      "Total training time: 34.79 seconds.\n",
      "-- Epoch 178\n",
      "Norm: 54533.76, NNZs: 41, Bias: 2589.977288, T: 71079138, Avg. loss: 206604607.746928\n",
      "Total training time: 34.90 seconds.\n",
      "-- Epoch 179\n",
      "Norm: 54379.55, NNZs: 41, Bias: 2590.163707, T: 71478459, Avg. loss: 205558637.974515\n",
      "Total training time: 35.00 seconds.\n",
      "-- Epoch 180\n",
      "Norm: 54203.44, NNZs: 41, Bias: 2590.136837, T: 71877780, Avg. loss: 204521066.114666\n",
      "Total training time: 35.10 seconds.\n",
      "-- Epoch 181\n",
      "Norm: 54039.56, NNZs: 41, Bias: 2590.059838, T: 72277101, Avg. loss: 203493467.101159\n",
      "Total training time: 35.21 seconds.\n",
      "-- Epoch 182\n",
      "Norm: 53885.44, NNZs: 41, Bias: 2590.130296, T: 72676422, Avg. loss: 202480363.421232\n",
      "Total training time: 35.31 seconds.\n",
      "-- Epoch 183\n",
      "Norm: 53713.41, NNZs: 41, Bias: 2589.966304, T: 73075743, Avg. loss: 201473145.298072\n",
      "Total training time: 35.41 seconds.\n",
      "-- Epoch 184\n",
      "        21           0.8701           24.93m\n",
      "Norm: 53554.67, NNZs: 41, Bias: 2590.066228, T: 73475064, Avg. loss: 200480674.577630\n",
      "Total training time: 35.51 seconds.\n",
      "-- Epoch 185\n",
      "Norm: 53408.50, NNZs: 41, Bias: 2590.055167, T: 73874385, Avg. loss: 199498821.346643\n",
      "Total training time: 35.62 seconds.\n",
      "-- Epoch 186\n",
      "Norm: 53265.88, NNZs: 41, Bias: 2590.304487, T: 74273706, Avg. loss: 198525991.561567\n",
      "Total training time: 35.72 seconds.\n",
      "-- Epoch 187\n",
      "Norm: 53106.74, NNZs: 41, Bias: 2590.316322, T: 74673027, Avg. loss: 197561918.704240\n",
      "Total training time: 35.82 seconds.\n",
      "-- Epoch 188\n",
      "Norm: 52943.19, NNZs: 41, Bias: 2590.236851, T: 75072348, Avg. loss: 196605132.322880\n",
      "Total training time: 35.92 seconds.\n",
      "-- Epoch 189\n",
      "Norm: 52788.31, NNZs: 41, Bias: 2590.126495, T: 75471669, Avg. loss: 195658445.025116\n",
      "Total training time: 36.03 seconds.\n",
      "-- Epoch 190\n",
      "Norm: 52640.36, NNZs: 41, Bias: 2590.249920, T: 75870990, Avg. loss: 194724266.262204\n",
      "Total training time: 36.13 seconds.\n",
      "-- Epoch 191\n",
      "Norm: 52486.28, NNZs: 41, Bias: 2590.283542, T: 76270311, Avg. loss: 193797471.924371\n",
      "Total training time: 36.23 seconds.\n",
      "-- Epoch 192\n",
      "Norm: 52338.63, NNZs: 41, Bias: 2590.350595, T: 76669632, Avg. loss: 192881966.108472\n",
      "Total training time: 36.33 seconds.\n",
      "-- Epoch 193\n",
      "        22           0.8644           24.45m\n",
      "Norm: 52186.91, NNZs: 41, Bias: 2590.418339, T: 77068953, Avg. loss: 191972435.370225\n",
      "Total training time: 36.44 seconds.\n",
      "-- Epoch 194\n",
      "Norm: 52036.50, NNZs: 41, Bias: 2590.461867, T: 77468274, Avg. loss: 191071732.505650\n",
      "Total training time: 36.54 seconds.\n",
      "-- Epoch 195\n",
      "Norm: 51894.32, NNZs: 41, Bias: 2590.722973, T: 77867595, Avg. loss: 190181813.356866\n",
      "Total training time: 36.64 seconds.\n",
      "-- Epoch 196\n",
      "Norm: 51752.33, NNZs: 41, Bias: 2590.623882, T: 78266916, Avg. loss: 189298256.054378\n",
      "Total training time: 36.74 seconds.\n",
      "-- Epoch 197\n",
      "Norm: 51614.48, NNZs: 41, Bias: 2590.686713, T: 78666237, Avg. loss: 188424888.837487\n",
      "Total training time: 36.84 seconds.\n",
      "-- Epoch 198\n",
      "Norm: 51474.61, NNZs: 41, Bias: 2590.811900, T: 79065558, Avg. loss: 187559849.472201\n",
      "Total training time: 36.95 seconds.\n",
      "-- Epoch 199\n",
      "Norm: 51327.61, NNZs: 41, Bias: 2590.853524, T: 79464879, Avg. loss: 186703078.323832\n",
      "Total training time: 37.05 seconds.\n",
      "-- Epoch 200\n",
      "Norm: 51187.59, NNZs: 41, Bias: 2591.053543, T: 79864200, Avg. loss: 185856002.007262\n",
      "Total training time: 37.15 seconds.\n",
      "-- Epoch 201\n",
      "        23           0.8618           24.00m\n",
      "Norm: 51042.05, NNZs: 41, Bias: 2591.099978, T: 80263521, Avg. loss: 185015912.374720\n",
      "Total training time: 37.25 seconds.\n",
      "-- Epoch 202\n",
      "Norm: 50902.87, NNZs: 41, Bias: 2591.197934, T: 80662842, Avg. loss: 184182918.265576\n",
      "Total training time: 37.36 seconds.\n",
      "-- Epoch 203\n",
      "Norm: 50752.01, NNZs: 41, Bias: 2591.191500, T: 81062163, Avg. loss: 183355421.461524\n",
      "Total training time: 37.46 seconds.\n",
      "-- Epoch 204\n",
      "Norm: 50615.10, NNZs: 41, Bias: 2591.184278, T: 81461484, Avg. loss: 182537596.746211\n",
      "Total training time: 37.56 seconds.\n",
      "-- Epoch 205\n",
      "Norm: 50478.82, NNZs: 41, Bias: 2591.353100, T: 81860805, Avg. loss: 181730762.211349\n",
      "Total training time: 37.66 seconds.\n",
      "-- Epoch 206\n",
      "Norm: 50343.56, NNZs: 41, Bias: 2591.445623, T: 82260126, Avg. loss: 180929911.040067\n",
      "Total training time: 37.76 seconds.\n",
      "-- Epoch 207\n",
      "Norm: 50201.01, NNZs: 41, Bias: 2591.469505, T: 82659447, Avg. loss: 180134687.499493\n",
      "Total training time: 37.87 seconds.\n",
      "-- Epoch 208\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "Norm: 50071.52, NNZs: 41, Bias: 2591.574715, T: 83058768, Avg. loss: 179348434.958756\n",
      "Total training time: 37.97 seconds.\n",
      "-- Epoch 209\n",
      "        24           0.8596           23.53m\n",
      "Norm: 49944.59, NNZs: 41, Bias: 2591.557309, T: 83458089, Avg. loss: 178567034.327390\n",
      "Total training time: 38.07 seconds.\n",
      "-- Epoch 210\n",
      "Norm: 49806.47, NNZs: 41, Bias: 2591.572108, T: 83857410, Avg. loss: 177792513.939528\n",
      "Total training time: 38.18 seconds.\n",
      "-- Epoch 211\n",
      "Norm: 49685.08, NNZs: 41, Bias: 2591.838125, T: 84256731, Avg. loss: 177027744.158521\n",
      "Total training time: 38.28 seconds.\n",
      "-- Epoch 212\n",
      "Norm: 49562.19, NNZs: 41, Bias: 2591.807530, T: 84656052, Avg. loss: 176267989.980885\n",
      "Total training time: 38.38 seconds.\n",
      "-- Epoch 213\n",
      "Norm: 49435.12, NNZs: 41, Bias: 2591.928673, T: 85055373, Avg. loss: 175518405.258241\n",
      "Total training time: 38.48 seconds.\n",
      "-- Epoch 214\n",
      "Norm: 49303.59, NNZs: 41, Bias: 2591.987510, T: 85454694, Avg. loss: 174772115.996201\n",
      "Total training time: 38.59 seconds.\n",
      "-- Epoch 215\n",
      "Norm: 49171.00, NNZs: 41, Bias: 2592.103373, T: 85854015, Avg. loss: 174033119.700107\n",
      "Total training time: 38.69 seconds.\n",
      "-- Epoch 216\n",
      "Norm: 49041.37, NNZs: 41, Bias: 2592.081004, T: 86253336, Avg. loss: 173299874.146296\n",
      "Total training time: 38.79 seconds.\n",
      "-- Epoch 217\n",
      "Norm: 48915.04, NNZs: 41, Bias: 2592.166462, T: 86652657, Avg. loss: 172574479.665292\n",
      "Total training time: 38.89 seconds.\n",
      "-- Epoch 218\n",
      "        25           0.8568           23.13m\n",
      "Norm: 48800.97, NNZs: 41, Bias: 2592.235184, T: 87051978, Avg. loss: 171855013.424357\n",
      "Total training time: 39.00 seconds.\n",
      "-- Epoch 219\n",
      "Norm: 48686.80, NNZs: 41, Bias: 2592.442556, T: 87451299, Avg. loss: 171143262.435008\n",
      "Total training time: 39.10 seconds.\n",
      "-- Epoch 220\n",
      "Norm: 48570.01, NNZs: 41, Bias: 2592.486197, T: 87850620, Avg. loss: 170435594.749560\n",
      "Total training time: 39.20 seconds.\n",
      "-- Epoch 221\n",
      "Norm: 48440.30, NNZs: 41, Bias: 2592.534645, T: 88249941, Avg. loss: 169731846.982939\n",
      "Total training time: 39.30 seconds.\n",
      "-- Epoch 222\n",
      "Norm: 48320.45, NNZs: 41, Bias: 2592.615014, T: 88649262, Avg. loss: 169035941.219551\n",
      "Total training time: 39.41 seconds.\n",
      "-- Epoch 223\n",
      "Norm: 48194.00, NNZs: 41, Bias: 2592.635715, T: 89048583, Avg. loss: 168346046.163876\n",
      "Total training time: 39.51 seconds.\n",
      "-- Epoch 224\n",
      "Norm: 48071.97, NNZs: 41, Bias: 2592.763508, T: 89447904, Avg. loss: 167662407.961283\n",
      "Total training time: 39.61 seconds.\n",
      "-- Epoch 225\n",
      "Norm: 47955.24, NNZs: 41, Bias: 2592.957238, T: 89847225, Avg. loss: 166985180.171722\n",
      "Total training time: 39.71 seconds.\n",
      "-- Epoch 226\n",
      "Norm: 47836.77, NNZs: 41, Bias: 2593.009280, T: 90246546, Avg. loss: 166313057.978206\n",
      "Total training time: 39.81 seconds.\n",
      "-- Epoch 227\n",
      "        26           0.8549           22.79m\n",
      "Norm: 47719.90, NNZs: 41, Bias: 2593.149579, T: 90645867, Avg. loss: 165647266.100190\n",
      "Total training time: 39.92 seconds.\n",
      "-- Epoch 228\n",
      "Norm: 47603.13, NNZs: 41, Bias: 2593.237633, T: 91045188, Avg. loss: 164987425.325246\n",
      "Total training time: 40.02 seconds.\n",
      "-- Epoch 229\n",
      "Norm: 47490.57, NNZs: 41, Bias: 2593.412012, T: 91444509, Avg. loss: 164332137.943489\n",
      "Total training time: 40.12 seconds.\n",
      "-- Epoch 230\n",
      "Norm: 47375.63, NNZs: 41, Bias: 2593.519680, T: 91843830, Avg. loss: 163682972.742319\n",
      "Total training time: 40.22 seconds.\n",
      "-- Epoch 231\n",
      "Norm: 47263.69, NNZs: 41, Bias: 2593.569347, T: 92243151, Avg. loss: 163036996.460548\n",
      "Total training time: 40.33 seconds.\n",
      "-- Epoch 232\n",
      "Norm: 47141.66, NNZs: 41, Bias: 2593.615160, T: 92642472, Avg. loss: 162397978.187167\n",
      "Total training time: 40.43 seconds.\n",
      "-- Epoch 233\n",
      "Norm: 47029.56, NNZs: 41, Bias: 2593.679885, T: 93041793, Avg. loss: 161764620.606487\n",
      "Total training time: 40.53 seconds.\n",
      "-- Epoch 234\n",
      "Norm: 46919.09, NNZs: 41, Bias: 2593.760550, T: 93441114, Avg. loss: 161135548.853112\n",
      "Total training time: 40.63 seconds.\n",
      "-- Epoch 235\n",
      "        27           0.8534           22.42m\n",
      "Norm: 46808.63, NNZs: 41, Bias: 2593.953621, T: 93840435, Avg. loss: 160512623.555283\n",
      "Total training time: 40.73 seconds.\n",
      "-- Epoch 236\n",
      "Norm: 46702.95, NNZs: 41, Bias: 2593.989358, T: 94239756, Avg. loss: 159892546.465139\n",
      "Total training time: 40.84 seconds.\n",
      "-- Epoch 237\n",
      "Norm: 46596.13, NNZs: 41, Bias: 2594.104928, T: 94639077, Avg. loss: 159279028.000094\n",
      "Total training time: 40.94 seconds.\n",
      "-- Epoch 238\n",
      "Norm: 46497.77, NNZs: 41, Bias: 2594.431689, T: 95038398, Avg. loss: 158671167.483851\n",
      "Total training time: 41.04 seconds.\n",
      "-- Epoch 239\n",
      "Norm: 46380.19, NNZs: 41, Bias: 2594.450814, T: 95437719, Avg. loss: 158066276.518607\n",
      "Total training time: 41.14 seconds.\n",
      "-- Epoch 240\n",
      "Norm: 46272.70, NNZs: 41, Bias: 2594.536224, T: 95837040, Avg. loss: 157466815.190684\n",
      "Total training time: 41.24 seconds.\n",
      "-- Epoch 241\n",
      "Norm: 46161.46, NNZs: 41, Bias: 2594.550458, T: 96236361, Avg. loss: 156871487.023156\n",
      "Total training time: 41.35 seconds.\n",
      "-- Epoch 242\n",
      "Norm: 46059.43, NNZs: 41, Bias: 2594.719274, T: 96635682, Avg. loss: 156281190.468461\n",
      "Total training time: 41.45 seconds.\n",
      "-- Epoch 243\n",
      "        28           0.8501           22.10m\n",
      "Norm: 45954.92, NNZs: 41, Bias: 2594.816411, T: 97035003, Avg. loss: 155696763.136350\n",
      "Total training time: 41.55 seconds.\n",
      "-- Epoch 244\n",
      "Norm: 45847.01, NNZs: 41, Bias: 2594.944867, T: 97434324, Avg. loss: 155116902.282468\n",
      "Total training time: 41.65 seconds.\n",
      "-- Epoch 245\n",
      "Norm: 45746.60, NNZs: 41, Bias: 2594.899743, T: 97833645, Avg. loss: 154539706.876309\n",
      "Total training time: 41.76 seconds.\n",
      "-- Epoch 246\n",
      "Norm: 45637.54, NNZs: 41, Bias: 2595.063794, T: 98232966, Avg. loss: 153967493.057206\n",
      "Total training time: 41.86 seconds.\n",
      "-- Epoch 247\n",
      "Norm: 45534.59, NNZs: 41, Bias: 2595.188640, T: 98632287, Avg. loss: 153399183.833717\n",
      "Total training time: 41.96 seconds.\n",
      "-- Epoch 248\n",
      "Norm: 45426.06, NNZs: 41, Bias: 2595.287489, T: 99031608, Avg. loss: 152835166.389636\n",
      "Total training time: 42.06 seconds.\n",
      "-- Epoch 249\n",
      "Norm: 45325.41, NNZs: 41, Bias: 2595.418740, T: 99430929, Avg. loss: 152276334.424763\n",
      "Total training time: 42.16 seconds.\n",
      "-- Epoch 250\n",
      "Norm: 45229.10, NNZs: 41, Bias: 2595.608545, T: 99830250, Avg. loss: 151722960.784222\n",
      "Total training time: 42.27 seconds.\n",
      "-- Epoch 251\n",
      "        29           0.8484           21.73m\n",
      "Norm: 45131.56, NNZs: 41, Bias: 2595.593377, T: 100229571, Avg. loss: 151171860.596454\n",
      "Total training time: 42.37 seconds.\n",
      "-- Epoch 252\n",
      "Norm: 45031.01, NNZs: 41, Bias: 2595.759243, T: 100628892, Avg. loss: 150625871.619772\n",
      "Total training time: 42.47 seconds.\n",
      "-- Epoch 253\n",
      "Norm: 44924.06, NNZs: 41, Bias: 2595.757495, T: 101028213, Avg. loss: 150082760.143559\n",
      "Total training time: 42.57 seconds.\n",
      "-- Epoch 254\n",
      "Norm: 44815.46, NNZs: 41, Bias: 2595.817205, T: 101427534, Avg. loss: 149542475.332189\n",
      "Total training time: 42.67 seconds.\n",
      "-- Epoch 255\n",
      "Norm: 44717.39, NNZs: 41, Bias: 2595.978180, T: 101826855, Avg. loss: 149008745.350685\n",
      "Total training time: 42.78 seconds.\n",
      "-- Epoch 256\n",
      "Norm: 44621.71, NNZs: 41, Bias: 2596.098097, T: 102226176, Avg. loss: 148478024.889571\n",
      "Total training time: 42.88 seconds.\n",
      "-- Epoch 257\n",
      "Norm: 44518.58, NNZs: 41, Bias: 2596.169470, T: 102625497, Avg. loss: 147951352.916496\n",
      "Total training time: 42.98 seconds.\n",
      "-- Epoch 258\n",
      "Norm: 44422.11, NNZs: 41, Bias: 2596.397623, T: 103024818, Avg. loss: 147428841.933276\n",
      "Total training time: 43.08 seconds.\n",
      "-- Epoch 259\n",
      "        30           0.8469           21.43m\n",
      "Norm: 44321.31, NNZs: 41, Bias: 2596.472394, T: 103424139, Avg. loss: 146910875.894961\n",
      "Total training time: 43.19 seconds.\n",
      "-- Epoch 260\n",
      "Norm: 44228.02, NNZs: 41, Bias: 2596.504918, T: 103823460, Avg. loss: 146395717.005640\n",
      "Total training time: 43.29 seconds.\n",
      "-- Epoch 261\n",
      "Norm: 44132.56, NNZs: 41, Bias: 2596.668967, T: 104222781, Avg. loss: 145885146.261243\n",
      "Total training time: 43.39 seconds.\n",
      "-- Epoch 262\n",
      "Norm: 44033.25, NNZs: 41, Bias: 2596.776334, T: 104622102, Avg. loss: 145377456.158766\n",
      "Total training time: 43.49 seconds.\n",
      "-- Epoch 263\n",
      "Norm: 43944.58, NNZs: 41, Bias: 2596.886701, T: 105021423, Avg. loss: 144874885.415599\n",
      "Total training time: 43.59 seconds.\n",
      "-- Epoch 264\n",
      "Norm: 43846.67, NNZs: 41, Bias: 2597.022508, T: 105420744, Avg. loss: 144374734.429861\n",
      "Total training time: 43.70 seconds.\n",
      "-- Epoch 265\n",
      "Norm: 43755.38, NNZs: 41, Bias: 2597.056545, T: 105820065, Avg. loss: 143878227.596507\n",
      "Total training time: 43.80 seconds.\n",
      "-- Epoch 266\n",
      "Norm: 43659.96, NNZs: 41, Bias: 2597.194065, T: 106219386, Avg. loss: 143385201.202332\n",
      "Total training time: 43.90 seconds.\n",
      "-- Epoch 267\n",
      "        31           0.8440           21.18m\n",
      "Norm: 43565.76, NNZs: 41, Bias: 2597.285362, T: 106618707, Avg. loss: 142895885.445400\n",
      "Total training time: 44.00 seconds.\n",
      "-- Epoch 268\n",
      "Norm: 43475.94, NNZs: 41, Bias: 2597.386422, T: 107018028, Avg. loss: 142409909.269414\n",
      "Total training time: 44.11 seconds.\n",
      "-- Epoch 269\n",
      "Norm: 43381.48, NNZs: 41, Bias: 2597.373258, T: 107417349, Avg. loss: 141925985.223789\n",
      "Total training time: 44.21 seconds.\n",
      "-- Epoch 270\n",
      "Norm: 43304.47, NNZs: 41, Bias: 2597.567520, T: 107816670, Avg. loss: 141448221.586960\n",
      "Total training time: 44.31 seconds.\n",
      "-- Epoch 271\n",
      "Norm: 43207.10, NNZs: 41, Bias: 2597.626317, T: 108215991, Avg. loss: 140973118.409523\n",
      "Total training time: 44.41 seconds.\n",
      "-- Epoch 272\n",
      "Norm: 43120.11, NNZs: 41, Bias: 2597.805151, T: 108615312, Avg. loss: 140501881.258849\n",
      "Total training time: 44.52 seconds.\n",
      "-- Epoch 273\n",
      "Norm: 43035.01, NNZs: 41, Bias: 2597.901838, T: 109014633, Avg. loss: 140033841.724849\n",
      "Total training time: 44.62 seconds.\n",
      "-- Epoch 274\n",
      "Norm: 42945.51, NNZs: 41, Bias: 2597.978022, T: 109413954, Avg. loss: 139567786.138303\n",
      "Total training time: 44.72 seconds.\n",
      "-- Epoch 275\n",
      "        32           0.8423           20.89m\n",
      "Norm: 42853.63, NNZs: 41, Bias: 2597.998548, T: 109813275, Avg. loss: 139103567.805648\n",
      "Total training time: 44.82 seconds.\n",
      "-- Epoch 276\n",
      "Norm: 42774.31, NNZs: 41, Bias: 2598.197179, T: 110212596, Avg. loss: 138645237.208795\n",
      "Total training time: 44.93 seconds.\n",
      "-- Epoch 277\n",
      "Norm: 42686.44, NNZs: 41, Bias: 2598.302557, T: 110611917, Avg. loss: 138188674.502043\n",
      "Total training time: 45.03 seconds.\n",
      "-- Epoch 278\n",
      "Norm: 42595.67, NNZs: 41, Bias: 2598.432366, T: 111011238, Avg. loss: 137735240.491932\n",
      "Total training time: 45.13 seconds.\n",
      "-- Epoch 279\n",
      "Norm: 42511.00, NNZs: 41, Bias: 2598.572514, T: 111410559, Avg. loss: 137284712.102334\n",
      "Total training time: 45.23 seconds.\n",
      "-- Epoch 280\n",
      "Norm: 42431.76, NNZs: 41, Bias: 2598.579412, T: 111809880, Avg. loss: 136838737.997968\n",
      "Total training time: 45.33 seconds.\n",
      "-- Epoch 281\n",
      "Norm: 42340.79, NNZs: 41, Bias: 2598.532618, T: 112209201, Avg. loss: 136392906.614477\n",
      "Total training time: 45.44 seconds.\n",
      "-- Epoch 282\n",
      "Norm: 42255.81, NNZs: 41, Bias: 2598.614216, T: 112608522, Avg. loss: 135951459.915208\n",
      "Total training time: 45.53 seconds.\n",
      "-- Epoch 283\n",
      "        33           0.8410           20.63m\n",
      "Norm: 42177.82, NNZs: 41, Bias: 2598.844593, T: 113007843, Avg. loss: 135513820.841589\n",
      "Total training time: 45.64 seconds.\n",
      "-- Epoch 284\n",
      "Norm: 42096.73, NNZs: 41, Bias: 2599.005472, T: 113407164, Avg. loss: 135079122.803064\n",
      "Total training time: 45.74 seconds.\n",
      "-- Epoch 285\n",
      "Norm: 42013.53, NNZs: 41, Bias: 2599.096633, T: 113806485, Avg. loss: 134647684.748186\n",
      "Total training time: 45.84 seconds.\n",
      "-- Epoch 286\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "Norm: 41930.32, NNZs: 41, Bias: 2599.252371, T: 114205806, Avg. loss: 134218447.511899\n",
      "Total training time: 45.94 seconds.\n",
      "-- Epoch 287\n",
      "Norm: 41848.64, NNZs: 41, Bias: 2599.326922, T: 114605127, Avg. loss: 133792120.571030\n",
      "Total training time: 46.05 seconds.\n",
      "-- Epoch 288\n",
      "Norm: 41768.27, NNZs: 41, Bias: 2599.401567, T: 115004448, Avg. loss: 133368432.858298\n",
      "Total training time: 46.15 seconds.\n",
      "-- Epoch 289\n",
      "Norm: 41689.79, NNZs: 41, Bias: 2599.484519, T: 115403769, Avg. loss: 132947571.954740\n",
      "Total training time: 46.30 seconds.\n",
      "-- Epoch 290\n",
      "Norm: 41612.97, NNZs: 41, Bias: 2599.607704, T: 115803090, Avg. loss: 132530650.682350\n",
      "Total training time: 46.43 seconds.\n",
      "-- Epoch 291\n",
      "        34           0.8398           20.41m\n",
      "Norm: 41539.07, NNZs: 41, Bias: 2599.772337, T: 116202411, Avg. loss: 132117067.431402\n",
      "Total training time: 46.56 seconds.\n",
      "-- Epoch 292\n",
      "Norm: 41461.26, NNZs: 41, Bias: 2599.912765, T: 116601732, Avg. loss: 131704404.978172\n",
      "Total training time: 46.67 seconds.\n",
      "-- Epoch 293\n",
      "Norm: 41380.59, NNZs: 41, Bias: 2600.000348, T: 117001053, Avg. loss: 131294251.400711\n",
      "Total training time: 46.77 seconds.\n",
      "-- Epoch 294\n",
      "Norm: 41307.20, NNZs: 41, Bias: 2600.107828, T: 117400374, Avg. loss: 130887606.224951\n",
      "Total training time: 46.87 seconds.\n",
      "-- Epoch 295\n",
      "Norm: 41233.68, NNZs: 41, Bias: 2600.238676, T: 117799695, Avg. loss: 130483614.662750\n",
      "Total training time: 46.97 seconds.\n",
      "-- Epoch 296\n",
      "Norm: 41159.15, NNZs: 41, Bias: 2600.372565, T: 118199016, Avg. loss: 130082480.792728\n",
      "Total training time: 47.12 seconds.\n",
      "-- Epoch 297\n",
      "        35           0.8386           20.12m\n",
      "Norm: 41078.43, NNZs: 41, Bias: 2600.464122, T: 118598337, Avg. loss: 129682414.666953\n",
      "Total training time: 47.25 seconds.\n",
      "-- Epoch 298\n",
      "Norm: 41007.47, NNZs: 41, Bias: 2600.645549, T: 118997658, Avg. loss: 129286506.789987\n",
      "Total training time: 47.38 seconds.\n",
      "-- Epoch 299\n",
      "Norm: 40926.75, NNZs: 41, Bias: 2600.675016, T: 119396979, Avg. loss: 128891378.331912\n",
      "Total training time: 47.51 seconds.\n",
      "-- Epoch 300\n",
      "Norm: 40849.01, NNZs: 41, Bias: 2600.713758, T: 119796300, Avg. loss: 128498918.207952\n",
      "Total training time: 47.62 seconds.\n",
      "-- Epoch 301\n",
      "Norm: 40768.60, NNZs: 41, Bias: 2600.814265, T: 120195621, Avg. loss: 128108633.659094\n",
      "Total training time: 47.75 seconds.\n",
      "-- Epoch 302\n",
      "Norm: 40695.67, NNZs: 41, Bias: 2600.949799, T: 120594942, Avg. loss: 127722084.671734\n",
      "Total training time: 47.86 seconds.\n",
      "-- Epoch 303\n",
      "        36           0.8373           19.92m\n",
      "Norm: 40620.31, NNZs: 41, Bias: 2601.060714, T: 120994263, Avg. loss: 127337131.377167\n",
      "Total training time: 48.01 seconds.\n",
      "-- Epoch 304\n",
      "Norm: 40554.18, NNZs: 41, Bias: 2601.271357, T: 121393584, Avg. loss: 126955745.596961\n",
      "Total training time: 48.13 seconds.\n",
      "-- Epoch 305\n",
      "Norm: 40476.12, NNZs: 41, Bias: 2601.330180, T: 121792905, Avg. loss: 126575255.190604\n",
      "Total training time: 48.25 seconds.\n",
      "-- Epoch 306\n",
      "Norm: 40406.06, NNZs: 41, Bias: 2601.459990, T: 122192226, Avg. loss: 126197479.336466\n",
      "Total training time: 48.37 seconds.\n",
      "-- Epoch 307\n",
      "Norm: 40335.05, NNZs: 41, Bias: 2601.568676, T: 122591547, Avg. loss: 125822775.710124\n",
      "Total training time: 48.50 seconds.\n",
      "-- Epoch 308\n",
      "Norm: 40264.40, NNZs: 41, Bias: 2601.691217, T: 122990868, Avg. loss: 125450535.887061\n",
      "Total training time: 48.63 seconds.\n",
      "-- Epoch 309\n",
      "        37           0.8362           19.69m\n",
      "Norm: 40195.25, NNZs: 41, Bias: 2601.788602, T: 123390189, Avg. loss: 125080748.000429\n",
      "Total training time: 48.74 seconds.\n",
      "-- Epoch 310\n",
      "Norm: 40126.75, NNZs: 41, Bias: 2601.940819, T: 123789510, Avg. loss: 124713434.536748\n",
      "Total training time: 48.89 seconds.\n",
      "-- Epoch 311\n",
      "Norm: 40053.73, NNZs: 41, Bias: 2602.031828, T: 124188831, Avg. loss: 124348145.823533\n",
      "Total training time: 49.01 seconds.\n",
      "-- Epoch 312\n",
      "Norm: 39981.53, NNZs: 41, Bias: 2602.172679, T: 124588152, Avg. loss: 123984121.776768\n",
      "Total training time: 49.13 seconds.\n",
      "-- Epoch 313\n",
      "Norm: 39910.77, NNZs: 41, Bias: 2602.346989, T: 124987473, Avg. loss: 123623497.960233\n",
      "Total training time: 49.24 seconds.\n",
      "-- Epoch 314\n",
      "Norm: 39843.40, NNZs: 41, Bias: 2602.413736, T: 125386794, Avg. loss: 123264768.998641\n",
      "Total training time: 49.39 seconds.\n",
      "-- Epoch 315\n",
      "        38           0.8352           19.43m\n",
      "Norm: 39765.38, NNZs: 41, Bias: 2602.447157, T: 125786115, Avg. loss: 122906382.978832\n",
      "Total training time: 49.53 seconds.\n",
      "-- Epoch 316\n",
      "Norm: 39696.78, NNZs: 41, Bias: 2602.477713, T: 126185436, Avg. loss: 122551309.681613\n",
      "Total training time: 49.66 seconds.\n",
      "-- Epoch 317\n",
      "Norm: 39624.07, NNZs: 41, Bias: 2602.606261, T: 126584757, Avg. loss: 122198449.253790\n",
      "Total training time: 49.78 seconds.\n",
      "-- Epoch 318\n",
      "Norm: 39552.97, NNZs: 41, Bias: 2602.692117, T: 126984078, Avg. loss: 121848000.173438\n",
      "Total training time: 49.90 seconds.\n",
      "-- Epoch 319\n",
      "Norm: 39485.47, NNZs: 41, Bias: 2602.816462, T: 127383399, Avg. loss: 121499976.455713\n",
      "Total training time: 50.03 seconds.\n",
      "-- Epoch 320\n",
      "        39           0.8341           19.21m\n",
      "Norm: 39422.89, NNZs: 41, Bias: 2602.937781, T: 127782720, Avg. loss: 121153766.374802\n",
      "Total training time: 50.16 seconds.\n",
      "-- Epoch 321\n",
      "Norm: 39357.00, NNZs: 41, Bias: 2603.090516, T: 128182041, Avg. loss: 120809570.559276\n",
      "Total training time: 50.31 seconds.\n",
      "-- Epoch 322\n",
      "Norm: 39283.25, NNZs: 41, Bias: 2603.186686, T: 128581362, Avg. loss: 120466508.792615\n",
      "Total training time: 50.44 seconds.\n",
      "-- Epoch 323\n",
      "Norm: 39216.89, NNZs: 41, Bias: 2603.338155, T: 128980683, Avg. loss: 120126127.695729\n",
      "Total training time: 50.57 seconds.\n",
      "-- Epoch 324\n",
      "Norm: 39148.44, NNZs: 41, Bias: 2603.469036, T: 129380004, Avg. loss: 119787786.041122\n",
      "Total training time: 50.68 seconds.\n",
      "-- Epoch 325\n",
      "Norm: 39079.25, NNZs: 41, Bias: 2603.533275, T: 129779325, Avg. loss: 119450870.189237\n",
      "Total training time: 50.83 seconds.\n",
      "-- Epoch 326\n",
      "Norm: 39010.55, NNZs: 41, Bias: 2603.730589, T: 130178646, Avg. loss: 119116736.472282\n",
      "Total training time: 50.98 seconds.\n",
      "-- Epoch 327\n",
      "        40           0.8313           19.07m\n",
      "Norm: 38944.33, NNZs: 41, Bias: 2603.864428, T: 130577967, Avg. loss: 118784426.000620\n",
      "Total training time: 51.11 seconds.\n",
      "-- Epoch 328\n",
      "Norm: 38880.12, NNZs: 41, Bias: 2604.005373, T: 130977288, Avg. loss: 118454535.950044\n",
      "Total training time: 51.22 seconds.\n",
      "-- Epoch 329\n",
      "Norm: 38812.46, NNZs: 41, Bias: 2604.132272, T: 131376609, Avg. loss: 118126261.417033\n",
      "Total training time: 51.36 seconds.\n",
      "-- Epoch 330\n",
      "Norm: 38742.95, NNZs: 41, Bias: 2604.142958, T: 131775930, Avg. loss: 117798791.799195\n",
      "Total training time: 51.49 seconds.\n",
      "-- Epoch 331\n",
      "Norm: 38711.07, NNZs: 41, Bias: 2604.298949, T: 132175251, Avg. loss: 117474028.451225\n",
      "Total training time: 51.60 seconds.\n",
      "-- Epoch 332\n",
      "Norm: 38617.88, NNZs: 41, Bias: 2604.390165, T: 132574572, Avg. loss: 117151456.364847\n",
      "Total training time: 51.75 seconds.\n",
      "-- Epoch 333\n",
      "        41           0.8291           18.88m\n",
      "Norm: 38547.86, NNZs: 41, Bias: 2604.475069, T: 132973893, Avg. loss: 116829596.785376\n",
      "Total training time: 51.87 seconds.\n",
      "-- Epoch 334\n",
      "Norm: 38479.44, NNZs: 41, Bias: 2604.566688, T: 133373214, Avg. loss: 116510266.912803\n",
      "Total training time: 52.00 seconds.\n",
      "-- Epoch 335\n",
      "Norm: 38414.40, NNZs: 41, Bias: 2604.706502, T: 133772535, Avg. loss: 116192931.635805\n",
      "Total training time: 52.11 seconds.\n",
      "-- Epoch 336\n",
      "Norm: 38347.63, NNZs: 41, Bias: 2604.800498, T: 134171856, Avg. loss: 115877268.663025\n",
      "Total training time: 52.21 seconds.\n",
      "-- Epoch 337\n",
      "Norm: 38283.82, NNZs: 41, Bias: 2604.945371, T: 134571177, Avg. loss: 115563772.202693\n",
      "Total training time: 52.33 seconds.\n",
      "-- Epoch 338\n",
      "Norm: 38223.49, NNZs: 41, Bias: 2605.084689, T: 134970498, Avg. loss: 115251575.272480\n",
      "Total training time: 52.43 seconds.\n",
      "-- Epoch 339\n",
      "        42           0.8282           18.70m\n",
      "Norm: 38161.13, NNZs: 41, Bias: 2605.282158, T: 135369819, Avg. loss: 114941862.611803\n",
      "Total training time: 52.58 seconds.\n",
      "-- Epoch 340\n",
      "Norm: 38099.39, NNZs: 41, Bias: 2605.376576, T: 135769140, Avg. loss: 114633049.447691\n",
      "Total training time: 52.70 seconds.\n",
      "-- Epoch 341\n",
      "Norm: 38033.99, NNZs: 41, Bias: 2605.473345, T: 136168461, Avg. loss: 114326315.956442\n",
      "Total training time: 52.83 seconds.\n",
      "-- Epoch 342\n",
      "Norm: 37976.22, NNZs: 41, Bias: 2605.641522, T: 136567782, Avg. loss: 114021550.551689\n",
      "Total training time: 52.96 seconds.\n",
      "-- Epoch 343\n",
      "Norm: 37915.44, NNZs: 41, Bias: 2605.753332, T: 136967103, Avg. loss: 113718424.779631\n",
      "Total training time: 53.07 seconds.\n",
      "-- Epoch 344\n",
      "Norm: 37854.07, NNZs: 41, Bias: 2605.874233, T: 137366424, Avg. loss: 113416010.636415\n",
      "Total training time: 53.22 seconds.\n",
      "-- Epoch 345\n",
      "        43           0.8257           18.53m\n",
      "Norm: 37797.88, NNZs: 41, Bias: 2606.068455, T: 137765745, Avg. loss: 113116047.223854\n",
      "Total training time: 53.34 seconds.\n",
      "-- Epoch 346\n",
      "Norm: 37733.82, NNZs: 41, Bias: 2606.124726, T: 138165066, Avg. loss: 112817225.633200\n",
      "Total training time: 53.46 seconds.\n",
      "-- Epoch 347\n",
      "Norm: 37672.20, NNZs: 41, Bias: 2606.197867, T: 138564387, Avg. loss: 112520624.082976\n",
      "Total training time: 53.57 seconds.\n",
      "-- Epoch 348\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "Norm: 37614.17, NNZs: 41, Bias: 2606.348414, T: 138963708, Avg. loss: 112225027.532209\n",
      "Total training time: 53.68 seconds.\n",
      "-- Epoch 349\n",
      "Norm: 37554.05, NNZs: 41, Bias: 2606.362219, T: 139363029, Avg. loss: 111931509.840449\n",
      "Total training time: 53.83 seconds.\n",
      "-- Epoch 350\n",
      "Norm: 37501.72, NNZs: 41, Bias: 2606.544785, T: 139762350, Avg. loss: 111639971.043003\n",
      "Total training time: 53.96 seconds.\n",
      "-- Epoch 351\n",
      "Norm: 37441.97, NNZs: 41, Bias: 2606.653940, T: 140161671, Avg. loss: 111349698.432163\n",
      "Total training time: 54.08 seconds.\n",
      "-- Epoch 352\n",
      "        44           0.8248           18.39m\n",
      "Norm: 37383.94, NNZs: 41, Bias: 2606.745689, T: 140560992, Avg. loss: 111060654.745327\n",
      "Total training time: 54.21 seconds.\n",
      "-- Epoch 353\n",
      "Norm: 37325.05, NNZs: 41, Bias: 2606.816129, T: 140960313, Avg. loss: 110773345.456184\n",
      "Total training time: 54.32 seconds.\n",
      "-- Epoch 354\n",
      "Norm: 37266.36, NNZs: 41, Bias: 2606.946861, T: 141359634, Avg. loss: 110488039.449944\n",
      "Total training time: 54.45 seconds.\n",
      "-- Epoch 355\n",
      "Norm: 37206.70, NNZs: 41, Bias: 2607.061902, T: 141758955, Avg. loss: 110203897.115922\n",
      "Total training time: 54.56 seconds.\n",
      "-- Epoch 356\n",
      "Norm: 37155.65, NNZs: 41, Bias: 2607.234663, T: 142158276, Avg. loss: 109922457.361075\n",
      "Total training time: 54.67 seconds.\n",
      "-- Epoch 357\n",
      "Norm: 37098.00, NNZs: 41, Bias: 2607.268129, T: 142557597, Avg. loss: 109641684.671356\n",
      "Total training time: 54.77 seconds.\n",
      "-- Epoch 358\n",
      "Norm: 37042.94, NNZs: 41, Bias: 2607.424332, T: 142956918, Avg. loss: 109362682.203857\n",
      "Total training time: 54.92 seconds.\n",
      "-- Epoch 359\n",
      "        45           0.8240           18.27m\n",
      "Norm: 36985.34, NNZs: 41, Bias: 2607.487232, T: 143356239, Avg. loss: 109084294.950469\n",
      "Total training time: 55.05 seconds.\n",
      "-- Epoch 360\n",
      "Norm: 36934.08, NNZs: 41, Bias: 2607.664669, T: 143755560, Avg. loss: 108807959.642925\n",
      "Total training time: 55.17 seconds.\n",
      "-- Epoch 361\n",
      "Norm: 36878.41, NNZs: 41, Bias: 2607.701826, T: 144154881, Avg. loss: 108532198.520507\n",
      "Total training time: 55.30 seconds.\n",
      "-- Epoch 362\n",
      "Norm: 36824.22, NNZs: 41, Bias: 2607.759703, T: 144554202, Avg. loss: 108258564.742185\n",
      "Total training time: 55.41 seconds.\n",
      "-- Epoch 363\n",
      "Norm: 36772.29, NNZs: 41, Bias: 2607.899028, T: 144953523, Avg. loss: 107986340.693150\n",
      "Total training time: 55.51 seconds.\n",
      "-- Epoch 364\n",
      "Norm: 36714.60, NNZs: 41, Bias: 2607.945648, T: 145352844, Avg. loss: 107715238.895074\n",
      "Total training time: 55.62 seconds.\n",
      "-- Epoch 365\n",
      "Norm: 36661.04, NNZs: 41, Bias: 2608.113269, T: 145752165, Avg. loss: 107446483.519117\n",
      "Total training time: 55.72 seconds.\n",
      "-- Epoch 366\n",
      "        46           0.8231           18.13m\n",
      "Norm: 36612.30, NNZs: 41, Bias: 2608.252939, T: 146151486, Avg. loss: 107178734.836496\n",
      "Total training time: 55.87 seconds.\n",
      "-- Epoch 367\n",
      "Norm: 36552.41, NNZs: 41, Bias: 2608.285711, T: 146550807, Avg. loss: 106911245.346676\n",
      "Total training time: 55.99 seconds.\n",
      "-- Epoch 368\n",
      "Norm: 36500.15, NNZs: 41, Bias: 2608.427187, T: 146950128, Avg. loss: 106646082.990899\n",
      "Total training time: 56.12 seconds.\n",
      "-- Epoch 369\n",
      "Norm: 36446.52, NNZs: 41, Bias: 2608.526221, T: 147349449, Avg. loss: 106381784.596749\n",
      "Total training time: 56.23 seconds.\n",
      "-- Epoch 370\n",
      "Norm: 36391.82, NNZs: 41, Bias: 2608.610062, T: 147748770, Avg. loss: 106119127.936009\n",
      "Total training time: 56.34 seconds.\n",
      "-- Epoch 371\n",
      "Norm: 36340.26, NNZs: 41, Bias: 2608.741435, T: 148148091, Avg. loss: 105858160.821460\n",
      "Total training time: 56.48 seconds.\n",
      "-- Epoch 372\n",
      "        47           0.8205           18.00m\n",
      "Norm: 36282.57, NNZs: 41, Bias: 2608.748707, T: 148547412, Avg. loss: 105597357.031009\n",
      "Total training time: 56.62 seconds.\n",
      "-- Epoch 373\n",
      "Norm: 36224.49, NNZs: 41, Bias: 2608.854928, T: 148946733, Avg. loss: 105338535.539636\n",
      "Total training time: 56.73 seconds.\n",
      "-- Epoch 374\n",
      "Norm: 36172.77, NNZs: 41, Bias: 2608.997091, T: 149346054, Avg. loss: 105081921.060571\n",
      "Total training time: 56.88 seconds.\n",
      "-- Epoch 375\n",
      "Norm: 36124.87, NNZs: 41, Bias: 2609.126917, T: 149745375, Avg. loss: 104826842.428335\n",
      "Total training time: 57.03 seconds.\n",
      "-- Epoch 376\n",
      "Norm: 36138.56, NNZs: 41, Bias: 2609.260506, T: 150144696, Avg. loss: 104572137.841206\n",
      "Total training time: 57.15 seconds.\n",
      "-- Epoch 377\n",
      "Norm: 36020.22, NNZs: 41, Bias: 2609.333158, T: 150544017, Avg. loss: 104318320.557691\n",
      "Total training time: 57.28 seconds.\n",
      "-- Epoch 378\n",
      "        48           0.8197           17.85m\n",
      "Norm: 35969.25, NNZs: 41, Bias: 2609.434577, T: 150943338, Avg. loss: 104066173.705453\n",
      "Total training time: 57.39 seconds.\n",
      "-- Epoch 379\n",
      "Norm: 35917.23, NNZs: 41, Bias: 2609.549102, T: 151342659, Avg. loss: 103815004.171586\n",
      "Total training time: 57.49 seconds.\n",
      "-- Epoch 380\n",
      "Norm: 35863.22, NNZs: 41, Bias: 2609.627799, T: 151741980, Avg. loss: 103565166.563050\n",
      "Total training time: 57.64 seconds.\n",
      "-- Epoch 381\n",
      "Norm: 35810.36, NNZs: 41, Bias: 2609.697899, T: 152141301, Avg. loss: 103316553.785957\n",
      "Total training time: 57.78 seconds.\n",
      "-- Epoch 382\n",
      "Norm: 35762.80, NNZs: 41, Bias: 2609.853325, T: 152540622, Avg. loss: 103070046.275740\n",
      "Total training time: 57.91 seconds.\n",
      "-- Epoch 383\n",
      "        49           0.8188           17.70m\n",
      "Norm: 35715.13, NNZs: 41, Bias: 2609.972026, T: 152939943, Avg. loss: 102824549.350891\n",
      "Total training time: 58.06 seconds.\n",
      "-- Epoch 384\n",
      "Norm: 35662.80, NNZs: 41, Bias: 2610.091661, T: 153339264, Avg. loss: 102579638.237234\n",
      "Total training time: 58.19 seconds.\n",
      "-- Epoch 385\n",
      "Norm: 35611.73, NNZs: 41, Bias: 2610.217937, T: 153738585, Avg. loss: 102335876.998330\n",
      "Total training time: 58.30 seconds.\n",
      "-- Epoch 386\n",
      "Norm: 35560.07, NNZs: 41, Bias: 2610.321854, T: 154137906, Avg. loss: 102093609.167360\n",
      "Total training time: 58.44 seconds.\n",
      "-- Epoch 387\n",
      "Norm: 35508.46, NNZs: 41, Bias: 2610.373198, T: 154537227, Avg. loss: 101852180.344689\n",
      "Total training time: 58.56 seconds.\n",
      "-- Epoch 388\n",
      "Norm: 35461.33, NNZs: 41, Bias: 2610.517395, T: 154936548, Avg. loss: 101612358.514922\n",
      "Total training time: 58.68 seconds.\n",
      "-- Epoch 389\n",
      "        50           0.8178           17.53m\n",
      "Norm: 35411.76, NNZs: 41, Bias: 2610.670328, T: 155335869, Avg. loss: 101373268.234576\n",
      "Total training time: 58.82 seconds.\n",
      "-- Epoch 390\n",
      "Norm: 35360.52, NNZs: 41, Bias: 2610.724254, T: 155735190, Avg. loss: 101135191.782760\n",
      "Total training time: 58.95 seconds.\n",
      "-- Epoch 391\n",
      "Norm: 35312.54, NNZs: 41, Bias: 2610.839462, T: 156134511, Avg. loss: 100898595.344982\n",
      "Total training time: 59.08 seconds.\n",
      "-- Epoch 392\n",
      "Norm: 35265.06, NNZs: 41, Bias: 2610.884431, T: 156533832, Avg. loss: 100663483.848862\n",
      "Total training time: 59.19 seconds.\n",
      "-- Epoch 393\n",
      "Norm: 35213.44, NNZs: 41, Bias: 2610.982649, T: 156933153, Avg. loss: 100429652.407322\n",
      "Total training time: 59.29 seconds.\n",
      "-- Epoch 394\n",
      "Norm: 35171.47, NNZs: 41, Bias: 2611.127230, T: 157332474, Avg. loss: 100197479.893382\n",
      "Total training time: 59.44 seconds.\n",
      "-- Epoch 395\n",
      "        51           0.8172           17.41m\n",
      "Norm: 35123.51, NNZs: 41, Bias: 2611.160221, T: 157731795, Avg. loss: 99965313.935991\n",
      "Total training time: 59.59 seconds.\n",
      "-- Epoch 396\n",
      "Norm: 35072.26, NNZs: 41, Bias: 2611.186323, T: 158131116, Avg. loss: 99734429.380919\n",
      "Total training time: 59.71 seconds.\n",
      "-- Epoch 397\n",
      "Norm: 35029.43, NNZs: 41, Bias: 2611.260910, T: 158530437, Avg. loss: 99505133.758044\n",
      "Total training time: 59.83 seconds.\n",
      "-- Epoch 398\n",
      "Norm: 34979.34, NNZs: 41, Bias: 2611.386669, T: 158929758, Avg. loss: 99276702.347339\n",
      "Total training time: 59.96 seconds.\n",
      "-- Epoch 399\n",
      "Norm: 34928.97, NNZs: 41, Bias: 2611.486182, T: 159329079, Avg. loss: 99048822.335645\n",
      "Total training time: 60.09 seconds.\n",
      "-- Epoch 400\n",
      "Norm: 34877.30, NNZs: 41, Bias: 2611.606083, T: 159728400, Avg. loss: 98822437.329363\n",
      "Total training time: 60.20 seconds.\n",
      "-- Epoch 401\n",
      "        52           0.8156           17.29m\n",
      "Norm: 34826.60, NNZs: 41, Bias: 2611.667253, T: 160127721, Avg. loss: 98596702.450334\n",
      "Total training time: 60.33 seconds.\n",
      "-- Epoch 402\n",
      "Norm: 34781.71, NNZs: 41, Bias: 2611.743106, T: 160527042, Avg. loss: 98372533.996105\n",
      "Total training time: 60.48 seconds.\n",
      "-- Epoch 403\n",
      "Norm: 34733.81, NNZs: 41, Bias: 2611.882261, T: 160926363, Avg. loss: 98149387.216694\n",
      "Total training time: 60.61 seconds.\n",
      "-- Epoch 404\n",
      "Norm: 34687.82, NNZs: 41, Bias: 2612.011815, T: 161325684, Avg. loss: 97927108.562576\n",
      "Total training time: 60.74 seconds.\n",
      "-- Epoch 405\n",
      "Norm: 34642.02, NNZs: 41, Bias: 2612.089821, T: 161725005, Avg. loss: 97705817.152738\n",
      "Total training time: 60.85 seconds.\n",
      "-- Epoch 406\n",
      "Norm: 34594.37, NNZs: 41, Bias: 2612.148408, T: 162124326, Avg. loss: 97485514.706518\n",
      "Total training time: 61.00 seconds.\n",
      "-- Epoch 407\n",
      "        53           0.8147           17.17m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=36)]: Done   1 out of   1 | elapsed:  1.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "Norm: 34546.66, NNZs: 41, Bias: 2612.246486, T: 162523647, Avg. loss: 97266751.210106\n",
      "Total training time: 61.15 seconds.\n",
      "-- Epoch 408\n",
      "Norm: 34499.16, NNZs: 41, Bias: 2612.365277, T: 162922968, Avg. loss: 97048184.558636\n",
      "Total training time: 61.28 seconds.\n",
      "-- Epoch 409\n",
      "Norm: 34454.02, NNZs: 41, Bias: 2612.380476, T: 163322289, Avg. loss: 96831117.531498\n",
      "Total training time: 61.41 seconds.\n",
      "-- Epoch 410\n",
      "Norm: 34406.62, NNZs: 41, Bias: 2612.495935, T: 163721610, Avg. loss: 96615053.810508\n",
      "Total training time: 61.52 seconds.\n",
      "-- Epoch 411\n",
      "Norm: 34363.67, NNZs: 41, Bias: 2612.595764, T: 164120931, Avg. loss: 96400129.319902\n",
      "Total training time: 61.63 seconds.\n",
      "-- Epoch 412\n",
      "        54           0.8138           17.04m\n",
      "Norm: 34320.98, NNZs: 41, Bias: 2612.675562, T: 164520252, Avg. loss: 96186243.604745\n",
      "Total training time: 61.76 seconds.\n",
      "-- Epoch 413\n",
      "Norm: 34277.83, NNZs: 41, Bias: 2612.699266, T: 164919573, Avg. loss: 95973367.364558\n",
      "Total training time: 61.87 seconds.\n",
      "-- Epoch 414\n",
      "Norm: 34235.23, NNZs: 41, Bias: 2612.801743, T: 165318894, Avg. loss: 95761537.308090\n",
      "Total training time: 62.02 seconds.\n",
      "-- Epoch 415\n",
      "Norm: 34189.22, NNZs: 41, Bias: 2612.890167, T: 165718215, Avg. loss: 95550346.353207\n",
      "Total training time: 62.17 seconds.\n",
      "-- Epoch 416\n",
      "Norm: 34147.89, NNZs: 41, Bias: 2612.942940, T: 166117536, Avg. loss: 95340353.568084\n",
      "Total training time: 62.29 seconds.\n",
      "-- Epoch 417\n",
      "Norm: 34104.35, NNZs: 41, Bias: 2613.022797, T: 166516857, Avg. loss: 95131378.622579\n",
      "Total training time: 62.42 seconds.\n",
      "-- Epoch 418\n",
      "        55           0.8129           16.94m\n",
      "Norm: 34059.86, NNZs: 41, Bias: 2613.136018, T: 166916178, Avg. loss: 94922775.628723\n",
      "Total training time: 62.53 seconds.\n",
      "-- Epoch 419\n",
      "Norm: 34016.15, NNZs: 41, Bias: 2613.219765, T: 167315499, Avg. loss: 94715591.852835\n",
      "Total training time: 62.67 seconds.\n",
      "-- Epoch 420\n",
      "Norm: 33974.52, NNZs: 41, Bias: 2613.353014, T: 167714820, Avg. loss: 94509209.693226\n",
      "Total training time: 62.80 seconds.\n",
      "-- Epoch 421\n",
      "Norm: 33930.15, NNZs: 41, Bias: 2613.385674, T: 168114141, Avg. loss: 94303619.071467\n",
      "Total training time: 62.93 seconds.\n",
      "-- Epoch 422\n",
      "Norm: 33885.61, NNZs: 41, Bias: 2613.457887, T: 168513462, Avg. loss: 94098931.760725\n",
      "Total training time: 63.08 seconds.\n",
      "-- Epoch 423\n",
      "Norm: 33846.30, NNZs: 41, Bias: 2613.622626, T: 168912783, Avg. loss: 93896151.402354\n",
      "Total training time: 63.23 seconds.\n",
      "-- Epoch 424\n",
      "        56           0.8120           16.84m\n",
      "Norm: 33801.65, NNZs: 41, Bias: 2613.702116, T: 169312104, Avg. loss: 93693313.352756\n",
      "Total training time: 63.35 seconds.\n",
      "-- Epoch 425\n",
      "Norm: 33762.20, NNZs: 41, Bias: 2613.865710, T: 169711425, Avg. loss: 93491608.656874\n",
      "Total training time: 63.48 seconds.\n",
      "-- Epoch 426\n",
      "Norm: 33722.90, NNZs: 41, Bias: 2613.945066, T: 170110746, Avg. loss: 93290677.916517\n",
      "Total training time: 63.60 seconds.\n",
      "-- Epoch 427\n",
      "Norm: 33678.34, NNZs: 41, Bias: 2614.048179, T: 170510067, Avg. loss: 93090660.722813\n",
      "Total training time: 63.71 seconds.\n",
      "-- Epoch 428\n",
      "Norm: 33635.85, NNZs: 41, Bias: 2614.156191, T: 170909388, Avg. loss: 92891902.878811\n",
      "Total training time: 63.81 seconds.\n",
      "-- Epoch 429\n",
      "Norm: 33590.45, NNZs: 41, Bias: 2614.172939, T: 171308709, Avg. loss: 92693487.237491\n",
      "Total training time: 63.92 seconds.\n",
      "-- Epoch 430\n",
      "Norm: 33545.77, NNZs: 41, Bias: 2614.217481, T: 171708030, Avg. loss: 92495563.715399\n",
      "Total training time: 64.02 seconds.\n",
      "-- Epoch 431\n",
      "Norm: 33501.16, NNZs: 41, Bias: 2614.317778, T: 172107351, Avg. loss: 92299084.363763\n",
      "Total training time: 64.12 seconds.\n",
      "-- Epoch 432\n",
      "        57           0.8113           16.76m\n",
      "Norm: 33457.01, NNZs: 41, Bias: 2614.391828, T: 172506672, Avg. loss: 92103247.173677\n",
      "Total training time: 64.27 seconds.\n",
      "-- Epoch 433\n",
      "Norm: 33411.81, NNZs: 41, Bias: 2614.436113, T: 172905993, Avg. loss: 91908309.528443\n",
      "Total training time: 64.40 seconds.\n",
      "-- Epoch 434\n",
      "Norm: 33370.35, NNZs: 41, Bias: 2614.553029, T: 173305314, Avg. loss: 91714508.509786\n",
      "Total training time: 64.52 seconds.\n",
      "-- Epoch 435\n",
      "Norm: 33330.76, NNZs: 41, Bias: 2614.710435, T: 173704635, Avg. loss: 91521767.396079\n",
      "Total training time: 64.64 seconds.\n",
      "-- Epoch 436\n",
      "Norm: 33290.51, NNZs: 41, Bias: 2614.823622, T: 174103956, Avg. loss: 91329748.636440\n",
      "Total training time: 64.74 seconds.\n",
      "-- Epoch 437\n",
      "        58           0.8093           16.65m\n",
      "Norm: 33246.99, NNZs: 41, Bias: 2614.895636, T: 174503277, Avg. loss: 91138140.733689\n",
      "Total training time: 64.89 seconds.\n",
      "-- Epoch 438\n",
      "Norm: 33211.10, NNZs: 41, Bias: 2615.106266, T: 174902598, Avg. loss: 90948530.098450\n",
      "Total training time: 65.02 seconds.\n",
      "-- Epoch 439\n",
      "Norm: 33171.68, NNZs: 41, Bias: 2615.165139, T: 175301919, Avg. loss: 90758669.574071\n",
      "Total training time: 65.14 seconds.\n",
      "-- Epoch 440\n",
      "Norm: 33130.10, NNZs: 41, Bias: 2615.202345, T: 175701240, Avg. loss: 90569494.195008\n",
      "Total training time: 65.24 seconds.\n",
      "-- Epoch 441\n",
      "Norm: 33085.91, NNZs: 41, Bias: 2615.303067, T: 176100561, Avg. loss: 90381391.758116\n",
      "Total training time: 65.34 seconds.\n",
      "-- Epoch 442\n",
      "Norm: 33045.29, NNZs: 41, Bias: 2615.427362, T: 176499882, Avg. loss: 90194159.125356\n",
      "Total training time: 65.49 seconds.\n",
      "-- Epoch 443\n",
      "Norm: 33008.61, NNZs: 41, Bias: 2615.545279, T: 176899203, Avg. loss: 90007825.950492\n",
      "Total training time: 65.62 seconds.\n",
      "-- Epoch 444\n",
      "        59           0.8085           16.56m\n",
      "Norm: 32970.34, NNZs: 41, Bias: 2615.604937, T: 177298524, Avg. loss: 89822474.234644\n",
      "Total training time: 65.76 seconds.\n",
      "-- Epoch 445\n",
      "Norm: 32927.47, NNZs: 41, Bias: 2615.640774, T: 177697845, Avg. loss: 89637011.654765\n",
      "Total training time: 65.89 seconds.\n",
      "-- Epoch 446\n",
      "Norm: 32888.92, NNZs: 41, Bias: 2615.673628, T: 178097166, Avg. loss: 89453032.118257\n",
      "Total training time: 66.00 seconds.\n",
      "-- Epoch 447\n",
      "Norm: 32849.88, NNZs: 41, Bias: 2615.724974, T: 178496487, Avg. loss: 89269212.536260\n",
      "Total training time: 66.12 seconds.\n",
      "-- Epoch 448\n",
      "Norm: 32812.63, NNZs: 41, Bias: 2615.860181, T: 178895808, Avg. loss: 89087008.594698\n",
      "Total training time: 66.23 seconds.\n",
      "-- Epoch 449\n",
      "Norm: 32774.69, NNZs: 41, Bias: 2615.951951, T: 179295129, Avg. loss: 88905623.983169\n",
      "Total training time: 66.38 seconds.\n",
      "-- Epoch 450\n",
      "        60           0.8077           16.47m\n",
      "Norm: 32740.58, NNZs: 41, Bias: 2616.070903, T: 179694450, Avg. loss: 88724840.367092\n",
      "Total training time: 66.50 seconds.\n",
      "-- Epoch 451\n",
      "Norm: 32699.68, NNZs: 41, Bias: 2616.164503, T: 180093771, Avg. loss: 88544497.495029\n",
      "Total training time: 66.62 seconds.\n",
      "-- Epoch 452\n",
      "Norm: 32661.42, NNZs: 41, Bias: 2616.255283, T: 180493092, Avg. loss: 88365278.532616\n",
      "Total training time: 66.76 seconds.\n",
      "-- Epoch 453\n",
      "Norm: 32625.64, NNZs: 41, Bias: 2616.369231, T: 180892413, Avg. loss: 88187073.120468\n",
      "Total training time: 66.87 seconds.\n",
      "-- Epoch 454\n",
      "Norm: 32585.58, NNZs: 41, Bias: 2616.496441, T: 181291734, Avg. loss: 88009480.660403\n",
      "Total training time: 67.01 seconds.\n",
      "-- Epoch 455\n",
      "        61           0.8071           16.36m\n",
      "Norm: 32547.29, NNZs: 41, Bias: 2616.545147, T: 181691055, Avg. loss: 87832385.601248\n",
      "Total training time: 67.16 seconds.\n",
      "-- Epoch 456\n",
      "Norm: 32505.77, NNZs: 41, Bias: 2616.642081, T: 182090376, Avg. loss: 87655888.962708\n",
      "Total training time: 67.29 seconds.\n",
      "-- Epoch 457\n",
      "Norm: 32469.86, NNZs: 41, Bias: 2616.720664, T: 182489697, Avg. loss: 87479912.394858\n",
      "Total training time: 67.42 seconds.\n",
      "-- Epoch 458\n",
      "Norm: 32430.14, NNZs: 41, Bias: 2616.773636, T: 182889018, Avg. loss: 87304743.732606\n",
      "Total training time: 67.55 seconds.\n",
      "-- Epoch 459\n",
      "Norm: 32390.20, NNZs: 41, Bias: 2616.839560, T: 183288339, Avg. loss: 87130437.628204\n",
      "Total training time: 67.65 seconds.\n",
      "-- Epoch 460\n",
      "Norm: 32352.72, NNZs: 41, Bias: 2616.938336, T: 183687660, Avg. loss: 86956948.651685\n",
      "Total training time: 67.77 seconds.\n",
      "-- Epoch 461\n",
      "        62           0.8040           16.26m\n",
      "Norm: 32316.73, NNZs: 41, Bias: 2617.067998, T: 184086981, Avg. loss: 86784200.869425\n",
      "Total training time: 67.88 seconds.\n",
      "-- Epoch 462\n",
      "Norm: 32279.52, NNZs: 41, Bias: 2617.161574, T: 184486302, Avg. loss: 86612290.036684\n",
      "Total training time: 68.03 seconds.\n",
      "-- Epoch 463\n",
      "Norm: 32240.93, NNZs: 41, Bias: 2617.209689, T: 184885623, Avg. loss: 86440960.937187\n",
      "Total training time: 68.15 seconds.\n",
      "-- Epoch 464\n",
      "Norm: 32203.20, NNZs: 41, Bias: 2617.245775, T: 185284944, Avg. loss: 86270476.781871\n",
      "Total training time: 68.29 seconds.\n",
      "-- Epoch 465\n",
      "Norm: 32171.14, NNZs: 41, Bias: 2617.411181, T: 185684265, Avg. loss: 86101256.081241\n",
      "Total training time: 68.39 seconds.\n",
      "-- Epoch 466\n",
      "Norm: 32135.74, NNZs: 41, Bias: 2617.489035, T: 186083586, Avg. loss: 85932191.328374\n",
      "Total training time: 68.54 seconds.\n",
      "-- Epoch 467\n",
      "        63           0.8028           16.18m\n",
      "Norm: 32098.25, NNZs: 41, Bias: 2617.545306, T: 186482907, Avg. loss: 85763824.770501\n",
      "Total training time: 68.66 seconds.\n",
      "-- Epoch 468\n",
      "Norm: 32058.85, NNZs: 41, Bias: 2617.627998, T: 186882228, Avg. loss: 85595896.040459\n",
      "Total training time: 68.80 seconds.\n",
      "-- Epoch 469\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "Norm: 32023.60, NNZs: 41, Bias: 2617.712262, T: 187281549, Avg. loss: 85428646.626021\n",
      "Total training time: 68.91 seconds.\n",
      "-- Epoch 470\n",
      "Norm: 31987.95, NNZs: 41, Bias: 2617.785203, T: 187680870, Avg. loss: 85262279.493738\n",
      "Total training time: 69.02 seconds.\n",
      "-- Epoch 471\n",
      "Norm: 31946.45, NNZs: 41, Bias: 2617.847856, T: 188080191, Avg. loss: 85096156.047065\n",
      "Total training time: 69.16 seconds.\n",
      "-- Epoch 472\n",
      "Norm: 31910.40, NNZs: 41, Bias: 2617.932008, T: 188479512, Avg. loss: 84930934.962555\n",
      "Total training time: 69.28 seconds.\n",
      "-- Epoch 473\n",
      "        64           0.8016           16.08m\n",
      "Norm: 31876.72, NNZs: 41, Bias: 2618.028072, T: 188878833, Avg. loss: 84766774.900182\n",
      "Total training time: 69.39 seconds.\n",
      "-- Epoch 474\n",
      "Norm: 31875.04, NNZs: 41, Bias: 2618.108684, T: 189278154, Avg. loss: 84603085.491117\n",
      "Total training time: 69.49 seconds.\n",
      "-- Epoch 475\n",
      "Norm: 31806.80, NNZs: 41, Bias: 2618.229024, T: 189677475, Avg. loss: 84440283.256778\n",
      "Total training time: 69.63 seconds.\n",
      "-- Epoch 476\n",
      "Norm: 31771.62, NNZs: 41, Bias: 2618.322210, T: 190076796, Avg. loss: 84277948.356398\n",
      "Total training time: 69.75 seconds.\n",
      "-- Epoch 477\n",
      "Norm: 31735.79, NNZs: 41, Bias: 2618.415942, T: 190476117, Avg. loss: 84116198.735781\n",
      "Total training time: 69.88 seconds.\n",
      "-- Epoch 478\n",
      "Norm: 31697.38, NNZs: 41, Bias: 2618.467556, T: 190875438, Avg. loss: 83954770.897606\n",
      "Total training time: 70.01 seconds.\n",
      "-- Epoch 479\n",
      "        65           0.8009           15.99m\n",
      "Norm: 31663.73, NNZs: 41, Bias: 2618.559891, T: 191274759, Avg. loss: 83794217.264722\n",
      "Total training time: 70.11 seconds.\n",
      "-- Epoch 480\n",
      "Norm: 31626.53, NNZs: 41, Bias: 2618.640942, T: 191674080, Avg. loss: 83634288.432709\n",
      "Total training time: 70.23 seconds.\n",
      "-- Epoch 481\n",
      "Norm: 31593.28, NNZs: 41, Bias: 2618.767052, T: 192073401, Avg. loss: 83475542.305813\n",
      "Total training time: 70.37 seconds.\n",
      "-- Epoch 482\n",
      "Norm: 31559.35, NNZs: 41, Bias: 2618.847375, T: 192472722, Avg. loss: 83316814.094374\n",
      "Total training time: 70.49 seconds.\n",
      "-- Epoch 483\n",
      "Norm: 31523.30, NNZs: 41, Bias: 2618.940198, T: 192872043, Avg. loss: 83158783.096866\n",
      "Total training time: 70.62 seconds.\n",
      "-- Epoch 484\n",
      "Norm: 31492.76, NNZs: 41, Bias: 2619.088779, T: 193271364, Avg. loss: 83002101.960745\n",
      "Total training time: 70.74 seconds.\n",
      "-- Epoch 485\n",
      "        66           0.8001           15.92m\n",
      "Norm: 31458.45, NNZs: 41, Bias: 2619.154757, T: 193670685, Avg. loss: 82845421.330339\n",
      "Total training time: 70.87 seconds.\n",
      "-- Epoch 486\n",
      "Norm: 31424.43, NNZs: 41, Bias: 2619.249913, T: 194070006, Avg. loss: 82689285.084208\n",
      "Total training time: 70.98 seconds.\n",
      "-- Epoch 487\n",
      "Norm: 31390.76, NNZs: 41, Bias: 2619.378976, T: 194469327, Avg. loss: 82534185.916098\n",
      "Total training time: 71.08 seconds.\n",
      "-- Epoch 488\n",
      "Norm: 31355.92, NNZs: 41, Bias: 2619.449480, T: 194868648, Avg. loss: 82379265.807785\n",
      "Total training time: 71.23 seconds.\n",
      "-- Epoch 489\n",
      "Norm: 31321.87, NNZs: 41, Bias: 2619.547931, T: 195267969, Avg. loss: 82224838.344629\n",
      "Total training time: 71.37 seconds.\n",
      "-- Epoch 490\n",
      "Norm: 31285.86, NNZs: 41, Bias: 2619.589108, T: 195667290, Avg. loss: 82071135.195269\n",
      "Total training time: 71.49 seconds.\n",
      "-- Epoch 491\n",
      "Norm: 31254.72, NNZs: 41, Bias: 2619.673257, T: 196066611, Avg. loss: 81918316.956018\n",
      "Total training time: 71.62 seconds.\n",
      "-- Epoch 492\n",
      "        67           0.7994           15.85m\n",
      "Norm: 31220.32, NNZs: 41, Bias: 2619.729521, T: 196465932, Avg. loss: 81765584.626042\n",
      "Total training time: 71.76 seconds.\n",
      "-- Epoch 493\n",
      "Norm: 31187.03, NNZs: 41, Bias: 2619.824064, T: 196865253, Avg. loss: 81613864.726168\n",
      "Total training time: 71.87 seconds.\n",
      "-- Epoch 494\n",
      "Norm: 31156.11, NNZs: 41, Bias: 2619.915371, T: 197264574, Avg. loss: 81462792.444506\n",
      "Total training time: 71.97 seconds.\n",
      "-- Epoch 495\n",
      "Norm: 31124.17, NNZs: 41, Bias: 2619.997379, T: 197663895, Avg. loss: 81312302.553559\n",
      "Total training time: 72.12 seconds.\n",
      "-- Epoch 496\n",
      "Norm: 31088.89, NNZs: 41, Bias: 2620.027288, T: 198063216, Avg. loss: 81161959.936400\n",
      "Total training time: 72.24 seconds.\n",
      "-- Epoch 497\n",
      "        68           0.7982           15.75m\n",
      "Norm: 31053.74, NNZs: 41, Bias: 2620.078330, T: 198462537, Avg. loss: 81012518.147695\n",
      "Total training time: 72.37 seconds.\n",
      "-- Epoch 498\n",
      "Norm: 31020.74, NNZs: 41, Bias: 2620.186423, T: 198861858, Avg. loss: 80863547.787025\n",
      "Total training time: 72.48 seconds.\n",
      "-- Epoch 499\n",
      "Norm: 30989.35, NNZs: 41, Bias: 2620.328435, T: 199261179, Avg. loss: 80715206.407870\n",
      "Total training time: 72.62 seconds.\n",
      "-- Epoch 500\n",
      "Norm: 30955.87, NNZs: 41, Bias: 2620.383149, T: 199660500, Avg. loss: 80567648.640291\n",
      "Total training time: 72.74 seconds.\n",
      "-- Epoch 501\n",
      "Norm: 30922.83, NNZs: 41, Bias: 2620.443017, T: 200059821, Avg. loss: 80420354.855385\n",
      "Total training time: 72.87 seconds.\n",
      "-- Epoch 502\n",
      "Norm: 30887.65, NNZs: 41, Bias: 2620.458619, T: 200459142, Avg. loss: 80273343.923076\n",
      "Total training time: 72.98 seconds.\n",
      "-- Epoch 503\n",
      "        69           0.7964           15.68m\n",
      "Norm: 30856.24, NNZs: 41, Bias: 2620.532795, T: 200858463, Avg. loss: 80127221.509968\n",
      "Total training time: 73.08 seconds.\n",
      "-- Epoch 504\n",
      "Norm: 30826.28, NNZs: 41, Bias: 2620.581358, T: 201257784, Avg. loss: 79981389.387740\n",
      "Total training time: 73.21 seconds.\n",
      "-- Epoch 505\n",
      "Norm: 30793.69, NNZs: 41, Bias: 2620.673670, T: 201657105, Avg. loss: 79836504.103545\n",
      "Total training time: 73.33 seconds.\n",
      "-- Epoch 506\n",
      "Norm: 30758.06, NNZs: 41, Bias: 2620.765210, T: 202056426, Avg. loss: 79691591.744116\n",
      "Total training time: 73.46 seconds.\n",
      "-- Epoch 507\n",
      "Norm: 30729.48, NNZs: 41, Bias: 2620.837659, T: 202455747, Avg. loss: 79547678.424644\n",
      "Total training time: 73.56 seconds.\n",
      "-- Epoch 508\n",
      "Norm: 30696.64, NNZs: 41, Bias: 2620.933348, T: 202855068, Avg. loss: 79403880.961742\n",
      "Total training time: 73.72 seconds.\n",
      "-- Epoch 509\n",
      "        70           0.7945           15.61m\n",
      "Norm: 30667.40, NNZs: 41, Bias: 2621.067568, T: 203254389, Avg. loss: 79261371.949951\n",
      "Total training time: 73.87 seconds.\n",
      "-- Epoch 510\n",
      "Norm: 30636.24, NNZs: 41, Bias: 2621.188496, T: 203653710, Avg. loss: 79119334.256159\n",
      "Total training time: 73.99 seconds.\n",
      "-- Epoch 511\n",
      "Norm: 30604.61, NNZs: 41, Bias: 2621.296606, T: 204053031, Avg. loss: 78977533.113201\n",
      "Total training time: 74.12 seconds.\n",
      "-- Epoch 512\n",
      "Norm: 30571.62, NNZs: 41, Bias: 2621.317025, T: 204452352, Avg. loss: 78835996.938000\n",
      "Total training time: 74.22 seconds.\n",
      "-- Epoch 513\n",
      "Norm: 30538.35, NNZs: 41, Bias: 2621.407929, T: 204851673, Avg. loss: 78695256.809826\n",
      "Total training time: 74.37 seconds.\n",
      "-- Epoch 514\n",
      "Norm: 30508.81, NNZs: 41, Bias: 2621.506073, T: 205250994, Avg. loss: 78554984.786213\n",
      "Total training time: 74.50 seconds.\n",
      "-- Epoch 515\n",
      "        71           0.7937           15.54m\n",
      "Norm: 30478.66, NNZs: 41, Bias: 2621.588615, T: 205650315, Avg. loss: 78415155.009293\n",
      "Total training time: 74.62 seconds.\n",
      "-- Epoch 516\n",
      "Norm: 30446.48, NNZs: 41, Bias: 2621.673914, T: 206049636, Avg. loss: 78276005.726896\n",
      "Total training time: 74.73 seconds.\n",
      "-- Epoch 517\n",
      "Norm: 30416.88, NNZs: 41, Bias: 2621.790300, T: 206448957, Avg. loss: 78137532.320582\n",
      "Total training time: 74.83 seconds.\n",
      "-- Epoch 518\n",
      "Norm: 30383.21, NNZs: 41, Bias: 2621.844118, T: 206848278, Avg. loss: 77998953.288865\n",
      "Total training time: 74.98 seconds.\n",
      "-- Epoch 519\n",
      "Norm: 30354.31, NNZs: 41, Bias: 2621.872299, T: 207247599, Avg. loss: 77861196.739083\n",
      "Total training time: 75.11 seconds.\n",
      "-- Epoch 520\n",
      "Norm: 30323.96, NNZs: 41, Bias: 2621.880806, T: 207646920, Avg. loss: 77723958.398781\n",
      "Total training time: 75.24 seconds.\n",
      "-- Epoch 521\n",
      "Norm: 30292.77, NNZs: 41, Bias: 2621.980863, T: 208046241, Avg. loss: 77587305.336539\n",
      "Total training time: 75.37 seconds.\n",
      "-- Epoch 522\n",
      "        72           0.7883           15.49m\n",
      "Norm: 30264.96, NNZs: 41, Bias: 2622.072125, T: 208445562, Avg. loss: 77451043.277124\n",
      "Total training time: 75.51 seconds.\n",
      "-- Epoch 523\n",
      "Norm: 30237.07, NNZs: 41, Bias: 2622.150342, T: 208844883, Avg. loss: 77315283.215626\n",
      "Total training time: 75.63 seconds.\n",
      "-- Epoch 524\n",
      "Norm: 30208.71, NNZs: 41, Bias: 2622.258849, T: 209244204, Avg. loss: 77180151.867829\n",
      "Total training time: 75.77 seconds.\n",
      "-- Epoch 525\n",
      "Norm: 30181.44, NNZs: 41, Bias: 2622.399426, T: 209643525, Avg. loss: 77045714.906376\n",
      "Total training time: 75.92 seconds.\n",
      "-- Epoch 526\n",
      "Norm: 30148.83, NNZs: 41, Bias: 2622.417137, T: 210042846, Avg. loss: 76911070.348343\n",
      "Total training time: 76.04 seconds.\n",
      "-- Epoch 527\n",
      "        73           0.7864           15.41m\n",
      "Norm: 30118.51, NNZs: 41, Bias: 2622.505982, T: 210442167, Avg. loss: 76777232.120939\n",
      "Total training time: 76.17 seconds.\n",
      "-- Epoch 528\n",
      "Norm: 30089.46, NNZs: 41, Bias: 2622.582499, T: 210841488, Avg. loss: 76643690.101968\n",
      "Total training time: 76.30 seconds.\n",
      "-- Epoch 529\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "Norm: 30058.46, NNZs: 41, Bias: 2622.633671, T: 211240809, Avg. loss: 76510793.272680\n",
      "Total training time: 76.45 seconds.\n",
      "-- Epoch 530\n",
      "Norm: 30028.91, NNZs: 41, Bias: 2622.704646, T: 211640130, Avg. loss: 76378548.765563\n",
      "Total training time: 76.58 seconds.\n",
      "-- Epoch 531\n",
      "Norm: 29999.91, NNZs: 41, Bias: 2622.807355, T: 212039451, Avg. loss: 76246892.843804\n",
      "Total training time: 76.71 seconds.\n",
      "-- Epoch 532\n",
      "Norm: 29971.16, NNZs: 41, Bias: 2622.876389, T: 212438772, Avg. loss: 76115456.339372\n",
      "Total training time: 76.84 seconds.\n",
      "-- Epoch 533\n",
      "Norm: 29944.90, NNZs: 41, Bias: 2623.020286, T: 212838093, Avg. loss: 75984760.824963\n",
      "Total training time: 76.95 seconds.\n",
      "-- Epoch 534\n",
      "        74           0.7849           15.36m\n",
      "Norm: 29914.61, NNZs: 41, Bias: 2623.074202, T: 213237414, Avg. loss: 75854320.567556\n",
      "Total training time: 77.08 seconds.\n",
      "-- Epoch 535\n",
      "Norm: 29882.99, NNZs: 41, Bias: 2623.093693, T: 213636735, Avg. loss: 75724362.272344\n",
      "Total training time: 77.23 seconds.\n",
      "-- Epoch 536\n",
      "Norm: 29852.38, NNZs: 41, Bias: 2623.127965, T: 214036056, Avg. loss: 75594532.691832\n",
      "Total training time: 77.36 seconds.\n",
      "-- Epoch 537\n",
      "Norm: 29825.21, NNZs: 41, Bias: 2623.250845, T: 214435377, Avg. loss: 75465534.490969\n",
      "Total training time: 77.51 seconds.\n",
      "-- Epoch 538\n",
      "Norm: 29796.83, NNZs: 41, Bias: 2623.336973, T: 214834698, Avg. loss: 75337155.225717\n",
      "Total training time: 77.63 seconds.\n",
      "-- Epoch 539\n",
      "Norm: 29768.14, NNZs: 41, Bias: 2623.393670, T: 215234019, Avg. loss: 75209040.927363\n",
      "Total training time: 77.76 seconds.\n",
      "-- Epoch 540\n",
      "        75           0.7835           15.30m\n",
      "Norm: 29739.53, NNZs: 41, Bias: 2623.471086, T: 215633340, Avg. loss: 75081255.529576\n",
      "Total training time: 77.87 seconds.\n",
      "-- Epoch 541\n",
      "Norm: 29709.98, NNZs: 41, Bias: 2623.532957, T: 216032661, Avg. loss: 74953989.142185\n",
      "Total training time: 78.02 seconds.\n",
      "-- Epoch 542\n",
      "Norm: 29680.54, NNZs: 41, Bias: 2623.543476, T: 216431982, Avg. loss: 74826772.761987\n",
      "Total training time: 78.14 seconds.\n",
      "-- Epoch 543\n",
      "Norm: 29649.95, NNZs: 41, Bias: 2623.617153, T: 216831303, Avg. loss: 74700228.221529\n",
      "Total training time: 78.26 seconds.\n",
      "-- Epoch 544\n",
      "Norm: 29617.67, NNZs: 41, Bias: 2623.687583, T: 217230624, Avg. loss: 74574282.693701\n",
      "Total training time: 78.37 seconds.\n",
      "-- Epoch 545\n",
      "Norm: 29588.51, NNZs: 41, Bias: 2623.788258, T: 217629945, Avg. loss: 74448619.444731\n",
      "Total training time: 78.47 seconds.\n",
      "-- Epoch 546\n",
      "        76           0.7822           15.24m\n",
      "Norm: 29560.91, NNZs: 41, Bias: 2623.922958, T: 218029266, Avg. loss: 74323884.243358\n",
      "Total training time: 78.62 seconds.\n",
      "-- Epoch 547\n",
      "Norm: 29533.55, NNZs: 41, Bias: 2623.961645, T: 218428587, Avg. loss: 74199125.567685\n",
      "Total training time: 78.74 seconds.\n",
      "-- Epoch 548\n",
      "Norm: 29505.68, NNZs: 41, Bias: 2624.048174, T: 218827908, Avg. loss: 74074926.115556\n",
      "Total training time: 78.87 seconds.\n",
      "-- Epoch 549\n",
      "Norm: 29477.08, NNZs: 41, Bias: 2624.083026, T: 219227229, Avg. loss: 73951092.125790\n",
      "Total training time: 79.01 seconds.\n",
      "-- Epoch 550\n",
      "Norm: 29448.61, NNZs: 41, Bias: 2624.146263, T: 219626550, Avg. loss: 73827790.729748\n",
      "Total training time: 79.13 seconds.\n",
      "-- Epoch 551\n",
      "Norm: 29423.26, NNZs: 41, Bias: 2624.232292, T: 220025871, Avg. loss: 73705045.997284\n",
      "Total training time: 79.26 seconds.\n",
      "-- Epoch 552\n",
      "        77           0.7805           15.19m\n",
      "Norm: 29397.34, NNZs: 41, Bias: 2624.336400, T: 220425192, Avg. loss: 73582832.675056\n",
      "Total training time: 79.37 seconds.\n",
      "-- Epoch 553\n",
      "Norm: 29371.20, NNZs: 41, Bias: 2624.384250, T: 220824513, Avg. loss: 73461085.312672\n",
      "Total training time: 79.47 seconds.\n",
      "-- Epoch 554\n",
      "Norm: 29343.86, NNZs: 41, Bias: 2624.438218, T: 221223834, Avg. loss: 73339594.685438\n",
      "Total training time: 79.62 seconds.\n",
      "-- Epoch 555\n",
      "Norm: 29318.81, NNZs: 41, Bias: 2624.582180, T: 221623155, Avg. loss: 73218863.791627\n",
      "Total training time: 79.74 seconds.\n",
      "-- Epoch 556\n",
      "Norm: 29289.18, NNZs: 41, Bias: 2624.620013, T: 222022476, Avg. loss: 73097882.480544\n",
      "Total training time: 79.86 seconds.\n",
      "-- Epoch 557\n",
      "Norm: 29260.43, NNZs: 41, Bias: 2624.707679, T: 222421797, Avg. loss: 72977534.358151\n",
      "Total training time: 79.99 seconds.\n",
      "-- Epoch 558\n",
      "Norm: 29235.78, NNZs: 41, Bias: 2624.820884, T: 222821118, Avg. loss: 72858123.214857\n",
      "Total training time: 80.10 seconds.\n",
      "-- Epoch 559\n",
      "        78           0.7795           15.13m\n",
      "Norm: 29207.32, NNZs: 41, Bias: 2624.882982, T: 223220439, Avg. loss: 72738627.678872\n",
      "Total training time: 80.20 seconds.\n",
      "-- Epoch 560\n",
      "Norm: 29182.04, NNZs: 41, Bias: 2625.006858, T: 223619760, Avg. loss: 72619774.680187\n",
      "Total training time: 80.35 seconds.\n",
      "-- Epoch 561\n",
      "Norm: 29157.47, NNZs: 41, Bias: 2625.087089, T: 224019081, Avg. loss: 72501376.939531\n",
      "Total training time: 80.47 seconds.\n",
      "-- Epoch 562\n",
      "Norm: 29133.29, NNZs: 41, Bias: 2625.214801, T: 224418402, Avg. loss: 72383352.434865\n",
      "Total training time: 80.59 seconds.\n",
      "-- Epoch 563\n",
      "Norm: 29109.42, NNZs: 41, Bias: 2625.280115, T: 224817723, Avg. loss: 72265821.925813\n",
      "Total training time: 80.72 seconds.\n",
      "-- Epoch 564\n",
      "Norm: 29082.24, NNZs: 41, Bias: 2625.344630, T: 225217044, Avg. loss: 72148356.569266\n",
      "Total training time: 80.85 seconds.\n",
      "-- Epoch 565\n",
      "Norm: 29057.69, NNZs: 41, Bias: 2625.440119, T: 225616365, Avg. loss: 72031604.232605\n",
      "Total training time: 80.98 seconds.\n",
      "-- Epoch 566\n",
      "        79           0.7781           15.11m\n",
      "Norm: 29031.66, NNZs: 41, Bias: 2625.521236, T: 226015686, Avg. loss: 71915087.220092\n",
      "Total training time: 81.09 seconds.\n",
      "-- Epoch 567\n",
      "Norm: 29004.23, NNZs: 41, Bias: 2625.583837, T: 226415007, Avg. loss: 71798596.143499\n",
      "Total training time: 81.24 seconds.\n",
      "-- Epoch 568\n",
      "Norm: 28978.40, NNZs: 41, Bias: 2625.626928, T: 226814328, Avg. loss: 71682763.934852\n",
      "Total training time: 81.37 seconds.\n",
      "-- Epoch 569\n",
      "Norm: 28952.06, NNZs: 41, Bias: 2625.684918, T: 227213649, Avg. loss: 71567093.979825\n",
      "Total training time: 81.51 seconds.\n",
      "-- Epoch 570\n",
      "Norm: 28925.20, NNZs: 41, Bias: 2625.743635, T: 227612970, Avg. loss: 71452116.467233\n",
      "Total training time: 81.64 seconds.\n",
      "-- Epoch 571\n",
      "Norm: 28899.20, NNZs: 41, Bias: 2625.789440, T: 228012291, Avg. loss: 71337538.586673\n",
      "Total training time: 81.77 seconds.\n",
      "-- Epoch 572\n",
      "        80           0.7770           15.03m\n",
      "Norm: 28870.71, NNZs: 41, Bias: 2625.799088, T: 228411612, Avg. loss: 71222843.539637\n",
      "Total training time: 81.90 seconds.\n",
      "-- Epoch 573\n",
      "Norm: 28842.89, NNZs: 41, Bias: 2625.867136, T: 228810933, Avg. loss: 71108730.323324\n",
      "Total training time: 82.01 seconds.\n",
      "-- Epoch 574\n",
      "Norm: 28817.32, NNZs: 41, Bias: 2625.974901, T: 229210254, Avg. loss: 70995131.069602\n",
      "Total training time: 82.15 seconds.\n",
      "-- Epoch 575\n",
      "Norm: 28793.34, NNZs: 41, Bias: 2626.076747, T: 229609575, Avg. loss: 70882216.624966\n",
      "Total training time: 82.28 seconds.\n",
      "-- Epoch 576\n",
      "Norm: 28768.00, NNZs: 41, Bias: 2626.122525, T: 230008896, Avg. loss: 70769339.139490\n",
      "Total training time: 82.41 seconds.\n",
      "-- Epoch 577\n",
      "Norm: 28743.15, NNZs: 41, Bias: 2626.212511, T: 230408217, Avg. loss: 70656923.545231\n",
      "Total training time: 82.54 seconds.\n",
      "-- Epoch 578\n",
      "        81           0.7761           14.98m\n",
      "Norm: 28718.34, NNZs: 41, Bias: 2626.287780, T: 230807538, Avg. loss: 70544959.927053\n",
      "Total training time: 82.64 seconds.\n",
      "-- Epoch 579\n",
      "Norm: 28703.33, NNZs: 41, Bias: 2626.308522, T: 231206859, Avg. loss: 70433066.550640\n",
      "Total training time: 82.79 seconds.\n",
      "-- Epoch 580\n",
      "Norm: 28667.02, NNZs: 41, Bias: 2626.356912, T: 231606180, Avg. loss: 70321666.276929\n",
      "Total training time: 82.92 seconds.\n",
      "-- Epoch 581\n",
      "Norm: 28641.10, NNZs: 41, Bias: 2626.418149, T: 232005501, Avg. loss: 70210545.757556\n",
      "Total training time: 83.05 seconds.\n",
      "-- Epoch 582\n",
      "Norm: 28613.65, NNZs: 41, Bias: 2626.461921, T: 232404822, Avg. loss: 70099791.248520\n",
      "Total training time: 83.15 seconds.\n",
      "-- Epoch 583\n",
      "Norm: 28589.15, NNZs: 41, Bias: 2626.531368, T: 232804143, Avg. loss: 69989553.594145\n",
      "Total training time: 83.30 seconds.\n",
      "-- Epoch 584\n",
      "Norm: 28567.19, NNZs: 41, Bias: 2626.631261, T: 233203464, Avg. loss: 69880197.480502\n",
      "Total training time: 83.43 seconds.\n",
      "-- Epoch 585\n",
      "        82           0.7754           14.95m\n",
      "Norm: 28542.13, NNZs: 41, Bias: 2626.691069, T: 233602785, Avg. loss: 69770958.719745\n",
      "Total training time: 83.56 seconds.\n",
      "-- Epoch 586\n",
      "Norm: 28516.12, NNZs: 41, Bias: 2626.728736, T: 234002106, Avg. loss: 69661575.280969\n",
      "Total training time: 83.69 seconds.\n",
      "-- Epoch 587\n",
      "Norm: 28491.73, NNZs: 41, Bias: 2626.801994, T: 234401427, Avg. loss: 69552747.808144\n",
      "Total training time: 83.80 seconds.\n",
      "-- Epoch 588\n",
      "Norm: 28464.82, NNZs: 41, Bias: 2626.855202, T: 234800748, Avg. loss: 69444005.096119\n",
      "Total training time: 83.95 seconds.\n",
      "-- Epoch 589\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "Norm: 28443.24, NNZs: 41, Bias: 2626.968952, T: 235200069, Avg. loss: 69336321.120393\n",
      "Total training time: 84.08 seconds.\n",
      "-- Epoch 590\n",
      "Norm: 28418.77, NNZs: 41, Bias: 2627.026360, T: 235599390, Avg. loss: 69228624.011239\n",
      "Total training time: 84.21 seconds.\n",
      "-- Epoch 591\n",
      "Norm: 28393.10, NNZs: 41, Bias: 2627.053003, T: 235998711, Avg. loss: 69121052.305265\n",
      "Total training time: 84.31 seconds.\n",
      "-- Epoch 592\n",
      "        83           0.7747           14.92m\n",
      "Norm: 28367.69, NNZs: 41, Bias: 2627.128731, T: 236398032, Avg. loss: 69014117.703518\n",
      "Total training time: 84.45 seconds.\n",
      "-- Epoch 593\n",
      "Norm: 28344.32, NNZs: 41, Bias: 2627.201591, T: 236797353, Avg. loss: 68907482.773648\n",
      "Total training time: 84.55 seconds.\n",
      "-- Epoch 594\n",
      "Norm: 28320.31, NNZs: 41, Bias: 2627.249017, T: 237196674, Avg. loss: 68801256.221068\n",
      "Total training time: 84.66 seconds.\n",
      "-- Epoch 595\n",
      "Norm: 28296.65, NNZs: 41, Bias: 2627.309845, T: 237595995, Avg. loss: 68695213.212853\n",
      "Total training time: 84.80 seconds.\n",
      "-- Epoch 596\n",
      "Norm: 28271.07, NNZs: 41, Bias: 2627.354243, T: 237995316, Avg. loss: 68589309.893657\n",
      "Total training time: 84.93 seconds.\n",
      "-- Epoch 597\n",
      "Norm: 28246.07, NNZs: 41, Bias: 2627.415866, T: 238394637, Avg. loss: 68483846.181169\n",
      "Total training time: 85.05 seconds.\n",
      "-- Epoch 598\n",
      "        84           0.7741           14.87m\n",
      "Norm: 28220.12, NNZs: 41, Bias: 2627.517117, T: 238793958, Avg. loss: 68378800.379817\n",
      "Total training time: 85.17 seconds.\n",
      "-- Epoch 599\n",
      "Norm: 28196.02, NNZs: 41, Bias: 2627.587953, T: 239193279, Avg. loss: 68274278.412778\n",
      "Total training time: 85.30 seconds.\n",
      "-- Epoch 600\n",
      "Norm: 28172.60, NNZs: 41, Bias: 2627.690893, T: 239592600, Avg. loss: 68170019.532023\n",
      "Total training time: 85.40 seconds.\n",
      "-- Epoch 601\n",
      "Norm: 28149.52, NNZs: 41, Bias: 2627.734918, T: 239991921, Avg. loss: 68066188.464502\n",
      "Total training time: 85.51 seconds.\n",
      "-- Epoch 602\n",
      "Norm: 28124.22, NNZs: 41, Bias: 2627.743575, T: 240391242, Avg. loss: 67962377.010690\n",
      "Total training time: 85.66 seconds.\n",
      "-- Epoch 603\n",
      "Norm: 28101.67, NNZs: 41, Bias: 2627.825082, T: 240790563, Avg. loss: 67859141.998557\n",
      "Total training time: 85.81 seconds.\n",
      "-- Epoch 604\n",
      "        85           0.7737           14.81m\n",
      "Norm: 28077.49, NNZs: 41, Bias: 2627.888150, T: 241189884, Avg. loss: 67756065.472203\n",
      "Total training time: 85.93 seconds.\n",
      "-- Epoch 605\n",
      "Norm: 28053.26, NNZs: 41, Bias: 2627.998026, T: 241589205, Avg. loss: 67653163.190550\n",
      "Total training time: 86.05 seconds.\n",
      "-- Epoch 606\n",
      "Norm: 28028.02, NNZs: 41, Bias: 2628.055608, T: 241988526, Avg. loss: 67550629.720929\n",
      "Total training time: 86.18 seconds.\n",
      "-- Epoch 607\n",
      "Norm: 28006.06, NNZs: 41, Bias: 2628.121845, T: 242387847, Avg. loss: 67448824.411736\n",
      "Total training time: 86.31 seconds.\n",
      "-- Epoch 608\n",
      "Norm: 27981.73, NNZs: 41, Bias: 2628.192939, T: 242787168, Avg. loss: 67347213.411649\n",
      "Total training time: 86.42 seconds.\n",
      "-- Epoch 609\n",
      "Norm: 27957.32, NNZs: 41, Bias: 2628.261729, T: 243186489, Avg. loss: 67245881.132581\n",
      "Total training time: 86.57 seconds.\n",
      "-- Epoch 610\n",
      "        86           0.7732           14.76m\n",
      "Norm: 27933.55, NNZs: 41, Bias: 2628.326549, T: 243585810, Avg. loss: 67144755.233075\n",
      "Total training time: 86.72 seconds.\n",
      "-- Epoch 611\n",
      "Norm: 27908.99, NNZs: 41, Bias: 2628.405667, T: 243985131, Avg. loss: 67043940.211243\n",
      "Total training time: 86.84 seconds.\n",
      "-- Epoch 612\n",
      "Norm: 27883.89, NNZs: 41, Bias: 2628.441679, T: 244384452, Avg. loss: 66943478.481270\n",
      "Total training time: 86.97 seconds.\n",
      "-- Epoch 613\n",
      "Norm: 27860.12, NNZs: 41, Bias: 2628.531322, T: 244783773, Avg. loss: 66843213.913408\n",
      "Total training time: 87.10 seconds.\n",
      "-- Epoch 614\n",
      "Norm: 27836.70, NNZs: 41, Bias: 2628.567732, T: 245183094, Avg. loss: 66743393.181654\n",
      "Total training time: 87.21 seconds.\n",
      "-- Epoch 615\n",
      "        87           0.7726           14.69m\n",
      "Norm: 27817.45, NNZs: 41, Bias: 2628.684783, T: 245582415, Avg. loss: 66644138.262334\n",
      "Total training time: 87.36 seconds.\n",
      "-- Epoch 616\n",
      "Norm: 27794.33, NNZs: 41, Bias: 2628.740417, T: 245981736, Avg. loss: 66544920.517496\n",
      "Total training time: 87.48 seconds.\n",
      "-- Epoch 617\n",
      "Norm: 27771.82, NNZs: 41, Bias: 2628.800191, T: 246381057, Avg. loss: 66445911.373669\n",
      "Total training time: 87.61 seconds.\n",
      "-- Epoch 618\n",
      "Norm: 27787.08, NNZs: 41, Bias: 2628.818698, T: 246780378, Avg. loss: 66347269.456213\n",
      "Total training time: 87.72 seconds.\n",
      "-- Epoch 619\n",
      "Norm: 27727.76, NNZs: 41, Bias: 2628.906505, T: 247179699, Avg. loss: 66249220.910417\n",
      "Total training time: 87.87 seconds.\n",
      "-- Epoch 620\n",
      "Norm: 27705.46, NNZs: 41, Bias: 2628.967797, T: 247579020, Avg. loss: 66151356.825665\n",
      "Total training time: 87.99 seconds.\n",
      "-- Epoch 621\n",
      "        88           0.7713           14.63m\n",
      "Norm: 27680.73, NNZs: 41, Bias: 2628.957153, T: 247978341, Avg. loss: 66053414.886098\n",
      "Total training time: 88.12 seconds.\n",
      "-- Epoch 622\n",
      "Norm: 27659.60, NNZs: 41, Bias: 2629.063359, T: 248377662, Avg. loss: 65956311.535137\n",
      "Total training time: 88.23 seconds.\n",
      "-- Epoch 623\n",
      "Norm: 27637.20, NNZs: 41, Bias: 2629.100666, T: 248776983, Avg. loss: 65859306.416142\n",
      "Total training time: 88.36 seconds.\n",
      "-- Epoch 624\n",
      "Norm: 27612.42, NNZs: 41, Bias: 2629.124941, T: 249176304, Avg. loss: 65762391.332974\n",
      "Total training time: 88.49 seconds.\n",
      "-- Epoch 625\n",
      "Norm: 27590.38, NNZs: 41, Bias: 2629.169552, T: 249575625, Avg. loss: 65665610.068305\n",
      "Total training time: 88.61 seconds.\n",
      "-- Epoch 626\n",
      "Norm: 27569.34, NNZs: 41, Bias: 2629.246416, T: 249974946, Avg. loss: 65569376.267292\n",
      "Total training time: 88.72 seconds.\n",
      "-- Epoch 627\n",
      "        89           0.7703           14.57m\n",
      "Norm: 27545.05, NNZs: 41, Bias: 2629.272341, T: 250374267, Avg. loss: 65473372.345884\n",
      "Total training time: 88.87 seconds.\n",
      "-- Epoch 628\n",
      "Norm: 27524.43, NNZs: 41, Bias: 2629.358555, T: 250773588, Avg. loss: 65377937.192012\n",
      "Total training time: 88.99 seconds.\n",
      "-- Epoch 629\n",
      "Norm: 27500.87, NNZs: 41, Bias: 2629.391718, T: 251172909, Avg. loss: 65282409.242853\n",
      "Total training time: 89.12 seconds.\n",
      "-- Epoch 630\n",
      "Norm: 27477.57, NNZs: 41, Bias: 2629.459976, T: 251572230, Avg. loss: 65187240.083315\n",
      "Total training time: 89.23 seconds.\n",
      "-- Epoch 631\n",
      "Norm: 27456.15, NNZs: 41, Bias: 2629.533898, T: 251971551, Avg. loss: 65092673.485392\n",
      "Total training time: 89.34 seconds.\n",
      "-- Epoch 632\n",
      "Norm: 27433.71, NNZs: 41, Bias: 2629.625932, T: 252370872, Avg. loss: 64998131.691301\n",
      "Total training time: 89.49 seconds.\n",
      "-- Epoch 633\n",
      "        90           0.7670           14.52m\n",
      "Norm: 27411.25, NNZs: 41, Bias: 2629.673945, T: 252770193, Avg. loss: 64903944.523495\n",
      "Total training time: 89.61 seconds.\n",
      "-- Epoch 634\n",
      "Norm: 27388.60, NNZs: 41, Bias: 2629.741601, T: 253169514, Avg. loss: 64810149.796393\n",
      "Total training time: 89.73 seconds.\n",
      "-- Epoch 635\n",
      "Norm: 27365.99, NNZs: 41, Bias: 2629.788968, T: 253568835, Avg. loss: 64716407.470130\n",
      "Total training time: 89.85 seconds.\n",
      "-- Epoch 636\n",
      "Norm: 27344.63, NNZs: 41, Bias: 2629.829344, T: 253968156, Avg. loss: 64623085.778836\n",
      "Total training time: 89.97 seconds.\n",
      "-- Epoch 637\n",
      "Norm: 27324.86, NNZs: 41, Bias: 2629.898890, T: 254367477, Avg. loss: 64530135.841119\n",
      "Total training time: 90.08 seconds.\n",
      "-- Epoch 638\n",
      "Norm: 27304.95, NNZs: 41, Bias: 2629.993547, T: 254766798, Avg. loss: 64437398.416038\n",
      "Total training time: 90.21 seconds.\n",
      "-- Epoch 639\n",
      "        91           0.7662           14.47m\n",
      "Norm: 27284.88, NNZs: 41, Bias: 2630.069801, T: 255166119, Avg. loss: 64344938.026485\n",
      "Total training time: 90.32 seconds.\n",
      "-- Epoch 640\n",
      "Norm: 27263.41, NNZs: 41, Bias: 2630.145313, T: 255565440, Avg. loss: 64252691.208983\n",
      "Total training time: 90.46 seconds.\n",
      "-- Epoch 641\n",
      "Norm: 27241.24, NNZs: 41, Bias: 2630.191915, T: 255964761, Avg. loss: 64160528.089911\n",
      "Total training time: 90.61 seconds.\n",
      "-- Epoch 642\n",
      "Norm: 27221.35, NNZs: 41, Bias: 2630.236475, T: 256364082, Avg. loss: 64068953.745137\n",
      "Total training time: 90.74 seconds.\n",
      "-- Epoch 643\n",
      "Norm: 27199.61, NNZs: 41, Bias: 2630.277152, T: 256763403, Avg. loss: 63977409.110524\n",
      "Total training time: 90.87 seconds.\n",
      "-- Epoch 644\n",
      "Norm: 27177.64, NNZs: 41, Bias: 2630.368091, T: 257162724, Avg. loss: 63886248.533260\n",
      "Total training time: 90.98 seconds.\n",
      "-- Epoch 645\n",
      "Norm: 27157.41, NNZs: 41, Bias: 2630.458237, T: 257562045, Avg. loss: 63795388.721269\n",
      "Total training time: 91.08 seconds.\n",
      "-- Epoch 646\n",
      "        92           0.7632           14.44m\n",
      "Norm: 27134.15, NNZs: 41, Bias: 2630.452350, T: 257961366, Avg. loss: 63704574.475697\n",
      "Total training time: 91.23 seconds.\n",
      "-- Epoch 647\n",
      "Norm: 27112.27, NNZs: 41, Bias: 2630.527008, T: 258360687, Avg. loss: 63614217.433547\n",
      "Total training time: 91.35 seconds.\n",
      "-- Epoch 648\n",
      "Norm: 27090.69, NNZs: 41, Bias: 2630.571676, T: 258760008, Avg. loss: 63524040.429477\n",
      "Total training time: 91.48 seconds.\n",
      "-- Epoch 649\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "Norm: 27070.36, NNZs: 41, Bias: 2630.622320, T: 259159329, Avg. loss: 63434142.271206\n",
      "Total training time: 91.61 seconds.\n",
      "-- Epoch 650\n",
      "Norm: 27050.41, NNZs: 41, Bias: 2630.659638, T: 259558650, Avg. loss: 63344558.616350\n",
      "Total training time: 91.72 seconds.\n",
      "-- Epoch 651\n",
      "Norm: 27028.87, NNZs: 41, Bias: 2630.665353, T: 259957971, Avg. loss: 63255110.086149\n",
      "Total training time: 91.82 seconds.\n",
      "-- Epoch 652\n",
      "        93           0.7624           14.39m\n",
      "Norm: 27007.18, NNZs: 41, Bias: 2630.728524, T: 260357292, Avg. loss: 63165967.799614\n",
      "Total training time: 91.92 seconds.\n",
      "-- Epoch 653\n",
      "Norm: 26984.34, NNZs: 41, Bias: 2630.758001, T: 260756613, Avg. loss: 63076776.655070\n",
      "Total training time: 92.06 seconds.\n",
      "-- Epoch 654\n",
      "Norm: 26977.26, NNZs: 41, Bias: 2630.820541, T: 261155934, Avg. loss: 62988263.099268\n",
      "Total training time: 92.21 seconds.\n",
      "-- Epoch 655\n",
      "Norm: 26944.86, NNZs: 41, Bias: 2630.917588, T: 261555255, Avg. loss: 62900147.345439\n",
      "Total training time: 92.33 seconds.\n",
      "-- Epoch 656\n",
      "Norm: 26923.91, NNZs: 41, Bias: 2630.949209, T: 261954576, Avg. loss: 62812033.376698\n",
      "Total training time: 92.44 seconds.\n",
      "-- Epoch 657\n",
      "Norm: 26905.28, NNZs: 41, Bias: 2631.048058, T: 262353897, Avg. loss: 62724519.219658\n",
      "Total training time: 92.57 seconds.\n",
      "-- Epoch 658\n",
      "Norm: 26883.99, NNZs: 41, Bias: 2631.089048, T: 262753218, Avg. loss: 62636939.350963\n",
      "Total training time: 92.72 seconds.\n",
      "-- Epoch 659\n",
      "        94           0.7591           14.36m\n",
      "Norm: 26863.36, NNZs: 41, Bias: 2631.129959, T: 263152539, Avg. loss: 62549477.825950\n",
      "Total training time: 92.84 seconds.\n",
      "-- Epoch 660\n",
      "Norm: 26842.65, NNZs: 41, Bias: 2631.224260, T: 263551860, Avg. loss: 62462460.928465\n",
      "Total training time: 92.96 seconds.\n",
      "-- Epoch 661\n",
      "Norm: 26821.62, NNZs: 41, Bias: 2631.263688, T: 263951181, Avg. loss: 62375759.906271\n",
      "Total training time: 93.10 seconds.\n",
      "-- Epoch 662\n",
      "Norm: 26801.79, NNZs: 41, Bias: 2631.343199, T: 264350502, Avg. loss: 62289373.469071\n",
      "Total training time: 93.20 seconds.\n",
      "-- Epoch 663\n",
      "Norm: 26781.91, NNZs: 41, Bias: 2631.396122, T: 264749823, Avg. loss: 62203244.262818\n",
      "Total training time: 93.36 seconds.\n",
      "-- Epoch 664\n",
      "Norm: 26762.66, NNZs: 41, Bias: 2631.429178, T: 265149144, Avg. loss: 62117231.676555\n",
      "Total training time: 93.48 seconds.\n",
      "-- Epoch 665\n",
      "        95           0.7563           14.33m\n",
      "Norm: 26740.73, NNZs: 41, Bias: 2631.469151, T: 265548465, Avg. loss: 62031517.524554\n",
      "Total training time: 93.60 seconds.\n",
      "-- Epoch 666\n",
      "Norm: 26721.85, NNZs: 41, Bias: 2631.546631, T: 265947786, Avg. loss: 61946114.363242\n",
      "Total training time: 93.71 seconds.\n",
      "-- Epoch 667\n",
      "Norm: 26702.04, NNZs: 41, Bias: 2631.598927, T: 266347107, Avg. loss: 61860765.288615\n",
      "Total training time: 93.86 seconds.\n",
      "-- Epoch 668\n",
      "Norm: 26680.02, NNZs: 41, Bias: 2631.644856, T: 266746428, Avg. loss: 61775402.557665\n",
      "Total training time: 93.99 seconds.\n",
      "-- Epoch 669\n",
      "Norm: 26659.59, NNZs: 41, Bias: 2631.672607, T: 267145749, Avg. loss: 61690592.843222\n",
      "Total training time: 94.10 seconds.\n",
      "-- Epoch 670\n",
      "Norm: 26641.01, NNZs: 41, Bias: 2631.780010, T: 267545070, Avg. loss: 61606245.711949\n",
      "Total training time: 94.25 seconds.\n",
      "-- Epoch 671\n",
      "        96           0.7548           14.28m\n",
      "Norm: 26620.24, NNZs: 41, Bias: 2631.815130, T: 267944391, Avg. loss: 61521831.015883\n",
      "Total training time: 94.37 seconds.\n",
      "-- Epoch 672\n",
      "Norm: 26600.43, NNZs: 41, Bias: 2631.868783, T: 268343712, Avg. loss: 61437567.447611\n",
      "Total training time: 94.50 seconds.\n",
      "-- Epoch 673\n",
      "Norm: 26580.68, NNZs: 41, Bias: 2631.935085, T: 268743033, Avg. loss: 61353764.440236\n",
      "Total training time: 94.63 seconds.\n",
      "-- Epoch 674\n",
      "Norm: 26559.74, NNZs: 41, Bias: 2631.966764, T: 269142354, Avg. loss: 61270028.633539\n",
      "Total training time: 94.74 seconds.\n",
      "-- Epoch 675\n",
      "Norm: 26541.75, NNZs: 41, Bias: 2632.043828, T: 269541675, Avg. loss: 61186717.286159\n",
      "Total training time: 94.88 seconds.\n",
      "-- Epoch 676\n",
      "        97           0.7533           14.22m\n",
      "Norm: 26523.60, NNZs: 41, Bias: 2632.113230, T: 269940996, Avg. loss: 61103810.908298\n",
      "Total training time: 95.03 seconds.\n",
      "-- Epoch 677\n",
      "Norm: 26504.88, NNZs: 41, Bias: 2632.168739, T: 270340317, Avg. loss: 61020913.216501\n",
      "Total training time: 95.15 seconds.\n",
      "-- Epoch 678\n",
      "Norm: 26486.64, NNZs: 41, Bias: 2632.236158, T: 270739638, Avg. loss: 60938330.305488\n",
      "Total training time: 95.29 seconds.\n",
      "-- Epoch 679\n",
      "Norm: 26468.61, NNZs: 41, Bias: 2632.294621, T: 271138959, Avg. loss: 60856007.155067\n",
      "Total training time: 95.39 seconds.\n",
      "-- Epoch 680\n",
      "Norm: 26448.76, NNZs: 41, Bias: 2632.334119, T: 271538280, Avg. loss: 60773816.196034\n",
      "Total training time: 95.54 seconds.\n",
      "-- Epoch 681\n",
      "Norm: 26430.40, NNZs: 41, Bias: 2632.411398, T: 271937601, Avg. loss: 60691827.705127\n",
      "Total training time: 95.66 seconds.\n",
      "-- Epoch 682\n",
      "        98           0.7520           14.17m\n",
      "Norm: 26410.85, NNZs: 41, Bias: 2632.470205, T: 272336922, Avg. loss: 60610016.990174\n",
      "Total training time: 95.79 seconds.\n",
      "-- Epoch 683\n",
      "Norm: 26392.05, NNZs: 41, Bias: 2632.547834, T: 272736243, Avg. loss: 60528518.183188\n",
      "Total training time: 95.90 seconds.\n",
      "-- Epoch 684\n",
      "Norm: 26371.92, NNZs: 41, Bias: 2632.626217, T: 273135564, Avg. loss: 60447374.900489\n",
      "Total training time: 96.04 seconds.\n",
      "-- Epoch 685\n",
      "Norm: 26353.34, NNZs: 41, Bias: 2632.692077, T: 273534885, Avg. loss: 60366500.726455\n",
      "Total training time: 96.17 seconds.\n",
      "-- Epoch 686\n",
      "Norm: 26334.32, NNZs: 41, Bias: 2632.738545, T: 273934206, Avg. loss: 60285570.198992\n",
      "Total training time: 96.30 seconds.\n",
      "-- Epoch 687\n",
      "Norm: 26313.33, NNZs: 41, Bias: 2632.786260, T: 274333527, Avg. loss: 60204928.306596\n",
      "Total training time: 96.43 seconds.\n",
      "-- Epoch 688\n",
      "Norm: 26292.81, NNZs: 41, Bias: 2632.853458, T: 274732848, Avg. loss: 60124691.211525\n",
      "Total training time: 96.56 seconds.\n",
      "-- Epoch 689\n",
      "        99           0.7511           14.14m\n",
      "Norm: 26273.04, NNZs: 41, Bias: 2632.903364, T: 275132169, Avg. loss: 60044513.176000\n",
      "Total training time: 96.67 seconds.\n",
      "-- Epoch 690\n",
      "Norm: 26255.10, NNZs: 41, Bias: 2632.916723, T: 275531490, Avg. loss: 59964435.116419\n",
      "Total training time: 96.80 seconds.\n",
      "-- Epoch 691\n",
      "Norm: 26236.22, NNZs: 41, Bias: 2632.983451, T: 275930811, Avg. loss: 59884718.316185\n",
      "Total training time: 96.91 seconds.\n",
      "-- Epoch 692\n",
      "Norm: 26219.26, NNZs: 41, Bias: 2633.060364, T: 276330132, Avg. loss: 59805247.201076\n",
      "Total training time: 97.06 seconds.\n",
      "-- Epoch 693\n",
      "Norm: 26201.13, NNZs: 41, Bias: 2633.115475, T: 276729453, Avg. loss: 59726017.310134\n",
      "Total training time: 97.20 seconds.\n",
      "-- Epoch 694\n",
      "Norm: 26184.26, NNZs: 41, Bias: 2633.219030, T: 277128774, Avg. loss: 59647150.668041\n",
      "Total training time: 97.34 seconds.\n",
      "-- Epoch 695\n",
      "       100           0.7499           14.10m\n",
      "Norm: 26164.32, NNZs: 41, Bias: 2633.265163, T: 277528095, Avg. loss: 59568297.993698\n",
      "Total training time: 97.47 seconds.\n",
      "-- Epoch 696\n",
      "Norm: 26144.79, NNZs: 41, Bias: 2633.268618, T: 277927416, Avg. loss: 59489524.757626\n",
      "Total training time: 97.58 seconds.\n",
      "-- Epoch 697\n",
      "Norm: 26125.89, NNZs: 41, Bias: 2633.321111, T: 278326737, Avg. loss: 59411139.248703\n",
      "Total training time: 97.68 seconds.\n",
      "-- Epoch 698\n",
      "Norm: 26109.10, NNZs: 41, Bias: 2633.399818, T: 278726058, Avg. loss: 59333114.361178\n",
      "Total training time: 97.78 seconds.\n",
      "-- Epoch 699\n",
      "Norm: 26090.60, NNZs: 41, Bias: 2633.460911, T: 279125379, Avg. loss: 59255137.186102\n",
      "Total training time: 97.93 seconds.\n",
      "-- Epoch 700\n",
      "Norm: 26072.94, NNZs: 41, Bias: 2633.470804, T: 279524700, Avg. loss: 59177357.823626\n",
      "Total training time: 98.08 seconds.\n",
      "-- Epoch 701\n",
      "Norm: 26055.72, NNZs: 41, Bias: 2633.538025, T: 279924021, Avg. loss: 59099991.054311\n",
      "Total training time: 98.20 seconds.\n",
      "-- Epoch 702\n",
      "       101           0.7485           14.07m\n",
      "Norm: 26035.62, NNZs: 41, Bias: 2633.550566, T: 280323342, Avg. loss: 59022499.830280\n",
      "Total training time: 98.33 seconds.\n",
      "-- Epoch 703\n",
      "Norm: 26018.11, NNZs: 41, Bias: 2633.606321, T: 280722663, Avg. loss: 58945429.130844\n",
      "Total training time: 98.46 seconds.\n",
      "-- Epoch 704\n",
      "Norm: 26000.82, NNZs: 41, Bias: 2633.706270, T: 281121984, Avg. loss: 58868653.597650\n",
      "-- Epoch 705\n",
      "Total training time: 98.57 seconds.\n",
      "Norm: 25982.75, NNZs: 41, Bias: 2633.764249, T: 281521305, Avg. loss: 58791963.215619\n",
      "Total training time: 98.67 seconds.\n",
      "-- Epoch 706\n",
      "Norm: 25963.61, NNZs: 41, Bias: 2633.812501, T: 281920626, Avg. loss: 58715462.599488\n",
      "Total training time: 98.78 seconds.\n",
      "-- Epoch 707\n",
      "Norm: 25946.94, NNZs: 41, Bias: 2633.878174, T: 282319947, Avg. loss: 58639380.490982\n",
      "Total training time: 98.88 seconds.\n",
      "-- Epoch 708\n",
      "Norm: 25927.91, NNZs: 41, Bias: 2633.930437, T: 282719268, Avg. loss: 58563241.466050\n",
      "Total training time: 98.97 seconds.\n",
      "-- Epoch 709\n",
      "Norm: 25911.56, NNZs: 41, Bias: 2633.990419, T: 283118589, Avg. loss: 58487509.738998\n",
      "Total training time: 99.06 seconds.\n",
      "-- Epoch 710\n",
      "       102           0.7479           14.05m\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "Norm: 25894.96, NNZs: 41, Bias: 2634.031349, T: 283517910, Avg. loss: 58411898.975905\n",
      "Total training time: 99.20 seconds.\n",
      "-- Epoch 711\n",
      "Norm: 25875.24, NNZs: 41, Bias: 2634.053501, T: 283917231, Avg. loss: 58336301.003179\n",
      "Total training time: 99.34 seconds.\n",
      "-- Epoch 712\n",
      "Norm: 25857.26, NNZs: 41, Bias: 2634.122615, T: 284316552, Avg. loss: 58260921.711062\n",
      "Total training time: 99.44 seconds.\n",
      "-- Epoch 713\n",
      "Norm: 25838.64, NNZs: 41, Bias: 2634.145254, T: 284715873, Avg. loss: 58185778.074265\n",
      "Total training time: 99.59 seconds.\n",
      "-- Epoch 714\n",
      "Norm: 25820.44, NNZs: 41, Bias: 2634.216611, T: 285115194, Avg. loss: 58111008.045904\n",
      "Total training time: 99.72 seconds.\n",
      "-- Epoch 715\n",
      "Norm: 25802.96, NNZs: 41, Bias: 2634.283773, T: 285514515, Avg. loss: 58036460.864657\n",
      "Total training time: 99.84 seconds.\n",
      "-- Epoch 716\n",
      "       103           0.7464           14.01m\n",
      "Norm: 25786.74, NNZs: 41, Bias: 2634.347198, T: 285913836, Avg. loss: 57962200.835598\n",
      "Total training time: 99.95 seconds.\n",
      "-- Epoch 717\n",
      "Norm: 25770.09, NNZs: 41, Bias: 2634.396549, T: 286313157, Avg. loss: 57888187.459650\n",
      "Total training time: 100.10 seconds.\n",
      "-- Epoch 718\n",
      "Norm: 25752.35, NNZs: 41, Bias: 2634.447528, T: 286712478, Avg. loss: 57814136.323382\n",
      "Total training time: 100.22 seconds.\n",
      "-- Epoch 719\n",
      "Norm: 25735.12, NNZs: 41, Bias: 2634.489769, T: 287111799, Avg. loss: 57740269.821090\n",
      "Total training time: 100.35 seconds.\n",
      "-- Epoch 720\n",
      "Norm: 25717.34, NNZs: 41, Bias: 2634.530837, T: 287511120, Avg. loss: 57666601.400737\n",
      "Total training time: 100.47 seconds.\n",
      "-- Epoch 721\n",
      "       104           0.7458           13.96m\n",
      "Norm: 25700.16, NNZs: 41, Bias: 2634.593526, T: 287910441, Avg. loss: 57593131.740312\n",
      "Total training time: 100.58 seconds.\n",
      "-- Epoch 722\n",
      "Norm: 25682.53, NNZs: 41, Bias: 2634.631855, T: 288309762, Avg. loss: 57519872.123192\n",
      "Total training time: 100.73 seconds.\n",
      "-- Epoch 723\n",
      "Norm: 25664.40, NNZs: 41, Bias: 2634.647653, T: 288709083, Avg. loss: 57446673.912048\n",
      "Total training time: 100.88 seconds.\n",
      "-- Epoch 724\n",
      "Norm: 25648.54, NNZs: 41, Bias: 2634.702044, T: 289108404, Avg. loss: 57373897.072641\n",
      "Total training time: 101.00 seconds.\n",
      "-- Epoch 725\n",
      "Norm: 25629.89, NNZs: 41, Bias: 2634.729145, T: 289507725, Avg. loss: 57301114.042333\n",
      "Total training time: 101.15 seconds.\n",
      "-- Epoch 726\n",
      "Norm: 25612.89, NNZs: 41, Bias: 2634.753717, T: 289907046, Avg. loss: 57228558.454989\n",
      "Total training time: 101.27 seconds.\n",
      "-- Epoch 727\n",
      "Norm: 25594.51, NNZs: 41, Bias: 2634.805306, T: 290306367, Avg. loss: 57156263.064692\n",
      "Total training time: 101.40 seconds.\n",
      "-- Epoch 728\n",
      "       105           0.7427           13.93m\n",
      "Norm: 25577.50, NNZs: 41, Bias: 2634.871553, T: 290705688, Avg. loss: 57084171.396533\n",
      "Total training time: 101.51 seconds.\n",
      "-- Epoch 729\n",
      "Norm: 25561.94, NNZs: 41, Bias: 2634.953763, T: 291105009, Avg. loss: 57012398.808215\n",
      "Total training time: 101.66 seconds.\n",
      "-- Epoch 730\n",
      "Norm: 25544.92, NNZs: 41, Bias: 2634.999034, T: 291504330, Avg. loss: 56940574.179728\n",
      "Total training time: 101.79 seconds.\n",
      "-- Epoch 731\n",
      "Norm: 25528.27, NNZs: 41, Bias: 2635.034849, T: 291903651, Avg. loss: 56868924.748880\n",
      "Total training time: 101.90 seconds.\n",
      "-- Epoch 732\n",
      "Norm: 25511.55, NNZs: 41, Bias: 2635.081257, T: 292302972, Avg. loss: 56797596.247687\n",
      "Total training time: 102.04 seconds.\n",
      "-- Epoch 733\n",
      "Norm: 25495.39, NNZs: 41, Bias: 2635.124042, T: 292702293, Avg. loss: 56726526.358113\n",
      "Total training time: 102.17 seconds.\n",
      "-- Epoch 734\n",
      "       106           0.7421           13.90m\n",
      "Norm: 25480.13, NNZs: 41, Bias: 2635.189731, T: 293101614, Avg. loss: 56655599.850517\n",
      "Total training time: 102.28 seconds.\n",
      "-- Epoch 735\n",
      "Norm: 25460.49, NNZs: 41, Bias: 2635.211194, T: 293500935, Avg. loss: 56584601.456824\n",
      "Total training time: 102.41 seconds.\n",
      "-- Epoch 736\n",
      "Norm: 25444.00, NNZs: 41, Bias: 2635.261580, T: 293900256, Avg. loss: 56513784.072092\n",
      "Total training time: 102.52 seconds.\n",
      "-- Epoch 737\n",
      "Norm: 25428.56, NNZs: 41, Bias: 2635.326704, T: 294299577, Avg. loss: 56443421.913401\n",
      "Total training time: 102.62 seconds.\n",
      "-- Epoch 738\n",
      "Norm: 25412.31, NNZs: 41, Bias: 2635.348579, T: 294698898, Avg. loss: 56373054.290431\n",
      "Total training time: 102.73 seconds.\n",
      "-- Epoch 739\n",
      "Norm: 25396.61, NNZs: 41, Bias: 2635.416669, T: 295098219, Avg. loss: 56303018.135897\n",
      "Total training time: 102.88 seconds.\n",
      "-- Epoch 740\n",
      "Norm: 25378.44, NNZs: 41, Bias: 2635.465831, T: 295497540, Avg. loss: 56233118.060606\n",
      "Total training time: 103.00 seconds.\n",
      "-- Epoch 741\n",
      "       107           0.7390           13.87m\n",
      "Norm: 25363.40, NNZs: 41, Bias: 2635.528749, T: 295896861, Avg. loss: 56163625.135511\n",
      "Total training time: 103.13 seconds.\n",
      "-- Epoch 742\n",
      "Norm: 25345.61, NNZs: 41, Bias: 2635.581166, T: 296296182, Avg. loss: 56093956.612142\n",
      "Total training time: 103.24 seconds.\n",
      "-- Epoch 743\n",
      "Norm: 25328.36, NNZs: 41, Bias: 2635.646734, T: 296695503, Avg. loss: 56024592.813319\n",
      "Total training time: 103.38 seconds.\n",
      "-- Epoch 744\n",
      "Norm: 25311.07, NNZs: 41, Bias: 2635.694702, T: 297094824, Avg. loss: 55955353.608739\n",
      "Total training time: 103.51 seconds.\n",
      "-- Epoch 745\n",
      "Norm: 25294.96, NNZs: 41, Bias: 2635.745018, T: 297494145, Avg. loss: 55886324.861618\n",
      "Total training time: 103.64 seconds.\n",
      "-- Epoch 746\n",
      "Norm: 25278.77, NNZs: 41, Bias: 2635.799715, T: 297893466, Avg. loss: 55817581.658107\n",
      "Total training time: 103.77 seconds.\n",
      "-- Epoch 747\n",
      "Norm: 25262.25, NNZs: 41, Bias: 2635.863874, T: 298292787, Avg. loss: 55749065.699159\n",
      "Total training time: 103.88 seconds.\n",
      "-- Epoch 748\n",
      "       108           0.7381           13.84m\n",
      "Norm: 25246.51, NNZs: 41, Bias: 2635.924961, T: 298692108, Avg. loss: 55680587.605384\n",
      "Total training time: 104.03 seconds.\n",
      "-- Epoch 749\n",
      "Norm: 25229.08, NNZs: 41, Bias: 2635.922269, T: 299091429, Avg. loss: 55612168.575276\n",
      "Total training time: 104.15 seconds.\n",
      "-- Epoch 750\n",
      "Norm: 25212.13, NNZs: 41, Bias: 2635.966563, T: 299490750, Avg. loss: 55543946.725231\n",
      "Total training time: 104.28 seconds.\n",
      "-- Epoch 751\n",
      "Norm: 25196.40, NNZs: 41, Bias: 2636.031020, T: 299890071, Avg. loss: 55476157.603849\n",
      "Total training time: 104.38 seconds.\n",
      "-- Epoch 752\n",
      "Norm: 25181.38, NNZs: 41, Bias: 2636.080289, T: 300289392, Avg. loss: 55408579.225046\n",
      "Total training time: 104.53 seconds.\n",
      "-- Epoch 753\n",
      "Norm: 25164.35, NNZs: 41, Bias: 2636.126997, T: 300688713, Avg. loss: 55340863.699751\n",
      "Total training time: 104.68 seconds.\n",
      "-- Epoch 754\n",
      "       109           0.7359           13.82m\n",
      "Norm: 25148.74, NNZs: 41, Bias: 2636.197873, T: 301088034, Avg. loss: 55273371.683639\n",
      "Total training time: 104.80 seconds.\n",
      "-- Epoch 755\n",
      "Norm: 25132.02, NNZs: 41, Bias: 2636.256686, T: 301487355, Avg. loss: 55206049.119523\n",
      "Total training time: 104.92 seconds.\n",
      "-- Epoch 756\n",
      "Norm: 25115.05, NNZs: 41, Bias: 2636.279212, T: 301886676, Avg. loss: 55138953.209747\n",
      "Total training time: 105.06 seconds.\n",
      "-- Epoch 757\n",
      "Norm: 25099.88, NNZs: 41, Bias: 2636.314959, T: 302285997, Avg. loss: 55071937.691896\n",
      "Total training time: 105.19 seconds.\n",
      "-- Epoch 758\n",
      "Norm: 25081.96, NNZs: 41, Bias: 2636.342142, T: 302685318, Avg. loss: 55005052.072777\n",
      "Total training time: 105.30 seconds.\n",
      "-- Epoch 759\n",
      "Norm: 25064.67, NNZs: 41, Bias: 2636.348373, T: 303084639, Avg. loss: 54938405.043414\n",
      "Total training time: 105.40 seconds.\n",
      "-- Epoch 760\n",
      "Norm: 25046.99, NNZs: 41, Bias: 2636.389794, T: 303483960, Avg. loss: 54872005.865448\n",
      "Total training time: 105.49 seconds.\n",
      "-- Epoch 761\n",
      "Norm: 25030.46, NNZs: 41, Bias: 2636.456569, T: 303883281, Avg. loss: 54805694.881913\n",
      "Total training time: 105.58 seconds.\n",
      "-- Epoch 762\n",
      "       110           0.7342           13.79m\n",
      "Norm: 25015.00, NNZs: 41, Bias: 2636.502894, T: 304282602, Avg. loss: 54739533.417941\n",
      "Total training time: 105.67 seconds.\n",
      "-- Epoch 763\n",
      "Norm: 24997.65, NNZs: 41, Bias: 2636.540467, T: 304681923, Avg. loss: 54673527.573666\n",
      "Total training time: 105.78 seconds.\n",
      "-- Epoch 764\n",
      "Norm: 24981.76, NNZs: 41, Bias: 2636.603173, T: 305081244, Avg. loss: 54607751.369414\n",
      "Total training time: 105.93 seconds.\n",
      "-- Epoch 765\n",
      "Norm: 24964.82, NNZs: 41, Bias: 2636.644309, T: 305480565, Avg. loss: 54542181.865705\n",
      "Total training time: 106.05 seconds.\n",
      "-- Epoch 766\n",
      "Norm: 24948.20, NNZs: 41, Bias: 2636.710659, T: 305879886, Avg. loss: 54476782.312435\n",
      "Total training time: 106.16 seconds.\n",
      "-- Epoch 767\n",
      "Norm: 24933.20, NNZs: 41, Bias: 2636.773883, T: 306279207, Avg. loss: 54411687.445217\n",
      "Total training time: 106.31 seconds.\n",
      "-- Epoch 768\n",
      "Norm: 24918.78, NNZs: 41, Bias: 2636.811854, T: 306678528, Avg. loss: 54346605.004029\n",
      "Total training time: 106.43 seconds.\n",
      "-- Epoch 769\n",
      "Norm: 24903.41, NNZs: 41, Bias: 2636.883630, T: 307077849, Avg. loss: 54281805.037380\n",
      "Total training time: 106.56 seconds.\n",
      "-- Epoch 770\n",
      "       111           0.7332           13.78m\n",
      "Norm: 24887.04, NNZs: 41, Bias: 2636.910718, T: 307477170, Avg. loss: 54216934.919229\n",
      "Total training time: 106.70 seconds.\n",
      "-- Epoch 771\n",
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "Norm: 24870.54, NNZs: 41, Bias: 2636.936987, T: 307876491, Avg. loss: 54152209.130397\n",
      "Total training time: 106.81 seconds.\n",
      "-- Epoch 772\n",
      "Norm: 24855.51, NNZs: 41, Bias: 2636.993482, T: 308275812, Avg. loss: 54087669.108869\n",
      "Total training time: 106.94 seconds.\n",
      "-- Epoch 773\n",
      "Norm: 24840.28, NNZs: 41, Bias: 2637.031209, T: 308675133, Avg. loss: 54023213.961157\n",
      "Total training time: 107.07 seconds.\n",
      "-- Epoch 774\n",
      "Norm: 24825.77, NNZs: 41, Bias: 2637.090390, T: 309074454, Avg. loss: 53959164.878402\n",
      "Total training time: 107.21 seconds.\n",
      "-- Epoch 775\n",
      "       112           0.7324           13.74m\n",
      "Norm: 24809.58, NNZs: 41, Bias: 2637.106700, T: 309473775, Avg. loss: 53895146.686667\n",
      "Total training time: 107.33 seconds.\n",
      "-- Epoch 776\n",
      "Norm: 24795.53, NNZs: 41, Bias: 2637.168319, T: 309873096, Avg. loss: 53831355.595381\n",
      "Total training time: 107.46 seconds.\n",
      "-- Epoch 777\n",
      "Norm: 24779.48, NNZs: 41, Bias: 2637.215526, T: 310272417, Avg. loss: 53767627.012385\n",
      "Total training time: 107.57 seconds.\n",
      "-- Epoch 778\n",
      "Norm: 24762.85, NNZs: 41, Bias: 2637.266942, T: 310671738, Avg. loss: 53704104.103043\n",
      "Total training time: 107.70 seconds.\n",
      "-- Epoch 779\n",
      "Norm: 24748.51, NNZs: 41, Bias: 2637.302828, T: 311071059, Avg. loss: 53640682.960571\n",
      "Total training time: 107.83 seconds.\n",
      "-- Epoch 780\n",
      "Norm: 24732.59, NNZs: 41, Bias: 2637.311707, T: 311470380, Avg. loss: 53577408.798379\n",
      "Total training time: 107.97 seconds.\n",
      "-- Epoch 781\n",
      "Norm: 24717.94, NNZs: 41, Bias: 2637.382807, T: 311869701, Avg. loss: 53514228.772786\n",
      "Total training time: 108.08 seconds.\n",
      "-- Epoch 782\n",
      "       113           0.7309           13.72m\n",
      "Norm: 24704.69, NNZs: 41, Bias: 2637.452661, T: 312269022, Avg. loss: 53451357.389540\n",
      "Total training time: 108.23 seconds.\n",
      "-- Epoch 783\n",
      "Norm: 24689.08, NNZs: 41, Bias: 2637.489839, T: 312668343, Avg. loss: 53388584.766063\n",
      "Total training time: 108.36 seconds.\n",
      "-- Epoch 784\n",
      "Norm: 24673.87, NNZs: 41, Bias: 2637.533920, T: 313067664, Avg. loss: 53325964.889270\n",
      "Total training time: 108.48 seconds.\n",
      "-- Epoch 785\n",
      "Norm: 24658.91, NNZs: 41, Bias: 2637.573893, T: 313466985, Avg. loss: 53263543.460052\n",
      "Total training time: 108.61 seconds.\n",
      "-- Epoch 786\n",
      "Norm: 24643.21, NNZs: 41, Bias: 2637.634598, T: 313866306, Avg. loss: 53201369.189952\n",
      "Total training time: 108.74 seconds.\n",
      "-- Epoch 787\n",
      "Norm: 24628.23, NNZs: 41, Bias: 2637.671924, T: 314265627, Avg. loss: 53139291.310668\n",
      "Total training time: 108.85 seconds.\n",
      "-- Epoch 788\n",
      "       114           0.7306           13.67m\n",
      "Norm: 24612.80, NNZs: 41, Bias: 2637.694432, T: 314664948, Avg. loss: 53077372.066189\n",
      "Total training time: 108.98 seconds.\n",
      "-- Epoch 789\n",
      "Norm: 24598.47, NNZs: 41, Bias: 2637.749431, T: 315064269, Avg. loss: 53015427.922116\n",
      "Total training time: 109.09 seconds.\n",
      "-- Epoch 790\n",
      "Norm: 24583.06, NNZs: 41, Bias: 2637.763380, T: 315463590, Avg. loss: 52953722.470696\n",
      "Total training time: 109.24 seconds.\n",
      "-- Epoch 791\n",
      "Norm: 24567.44, NNZs: 41, Bias: 2637.820665, T: 315862911, Avg. loss: 52892249.919792\n",
      "Total training time: 109.37 seconds.\n",
      "-- Epoch 792\n",
      "Norm: 24553.51, NNZs: 41, Bias: 2637.903421, T: 316262232, Avg. loss: 52830797.967934\n",
      "Total training time: 109.50 seconds.\n",
      "-- Epoch 793\n",
      "Norm: 24538.03, NNZs: 41, Bias: 2637.948393, T: 316661553, Avg. loss: 52769568.132169\n",
      "Total training time: 109.65 seconds.\n",
      "-- Epoch 794\n",
      "       115           0.7302           13.64m\n",
      "Norm: 24523.25, NNZs: 41, Bias: 2637.979200, T: 317060874, Avg. loss: 52708465.247913\n",
      "Total training time: 109.80 seconds.\n",
      "-- Epoch 795\n",
      "Norm: 24508.48, NNZs: 41, Bias: 2638.034696, T: 317460195, Avg. loss: 52647634.977676\n",
      "Total training time: 109.92 seconds.\n",
      "-- Epoch 796\n",
      "Norm: 24495.15, NNZs: 41, Bias: 2638.111248, T: 317859516, Avg. loss: 52587009.424374\n",
      "Total training time: 110.05 seconds.\n",
      "-- Epoch 797\n",
      "Norm: 24478.27, NNZs: 41, Bias: 2638.116525, T: 318258837, Avg. loss: 52526121.986326\n",
      "Total training time: 110.18 seconds.\n",
      "-- Epoch 798\n",
      "Norm: 24462.10, NNZs: 41, Bias: 2638.115535, T: 318658158, Avg. loss: 52465531.486406\n",
      "Total training time: 110.31 seconds.\n",
      "-- Epoch 799\n",
      "       116           0.7297           13.60m\n",
      "Norm: 24447.06, NNZs: 41, Bias: 2638.139125, T: 319057479, Avg. loss: 52404962.155914\n",
      "Total training time: 110.42 seconds.\n",
      "-- Epoch 800\n",
      "Norm: 24431.76, NNZs: 41, Bias: 2638.148137, T: 319456800, Avg. loss: 52344595.764202\n",
      "Total training time: 110.56 seconds.\n",
      "-- Epoch 801\n",
      "Norm: 24417.05, NNZs: 41, Bias: 2638.185934, T: 319856121, Avg. loss: 52284506.877284\n",
      "Total training time: 110.69 seconds.\n",
      "-- Epoch 802\n",
      "Norm: 24402.93, NNZs: 41, Bias: 2638.249544, T: 320255442, Avg. loss: 52224617.458599\n",
      "Total training time: 110.82 seconds.\n",
      "-- Epoch 803\n",
      "Norm: 24387.72, NNZs: 41, Bias: 2638.285851, T: 320654763, Avg. loss: 52164831.149019\n",
      "Total training time: 110.92 seconds.\n",
      "-- Epoch 804\n",
      "Norm: 24374.11, NNZs: 41, Bias: 2638.324733, T: 321054084, Avg. loss: 52105033.352486\n",
      "Total training time: 111.07 seconds.\n",
      "-- Epoch 805\n",
      "Norm: 24359.31, NNZs: 41, Bias: 2638.357488, T: 321453405, Avg. loss: 52045456.715578\n",
      "Total training time: 111.20 seconds.\n",
      "-- Epoch 806\n",
      "       117           0.7286           13.57m\n",
      "Norm: 24343.54, NNZs: 41, Bias: 2638.417054, T: 321852726, Avg. loss: 51986000.071488\n",
      "Total training time: 111.32 seconds.\n",
      "-- Epoch 807\n",
      "Norm: 24329.02, NNZs: 41, Bias: 2638.444260, T: 322252047, Avg. loss: 51926747.373931\n",
      "Total training time: 111.45 seconds.\n",
      "-- Epoch 808\n",
      "Norm: 24313.25, NNZs: 41, Bias: 2638.466218, T: 322651368, Avg. loss: 51867506.705090\n",
      "Total training time: 111.56 seconds.\n",
      "-- Epoch 809\n",
      "Norm: 24298.47, NNZs: 41, Bias: 2638.492247, T: 323050689, Avg. loss: 51808405.082101\n",
      "Total training time: 111.71 seconds.\n",
      "-- Epoch 810\n",
      "Norm: 24283.53, NNZs: 41, Bias: 2638.543464, T: 323450010, Avg. loss: 51749506.833026\n",
      "Total training time: 111.83 seconds.\n",
      "-- Epoch 811\n",
      "Norm: 24271.17, NNZs: 41, Bias: 2638.609590, T: 323849331, Avg. loss: 51691014.232725\n",
      "Total training time: 111.97 seconds.\n",
      "-- Epoch 812\n",
      "Norm: 24257.07, NNZs: 41, Bias: 2638.608092, T: 324248652, Avg. loss: 51632441.287356\n",
      "Total training time: 112.08 seconds.\n",
      "-- Epoch 813\n",
      "       118           0.7282           13.55m\n",
      "Norm: 24241.79, NNZs: 41, Bias: 2638.648763, T: 324647973, Avg. loss: 51574000.322568\n",
      "Total training time: 112.21 seconds.\n",
      "-- Epoch 814\n",
      "Norm: 24226.09, NNZs: 41, Bias: 2638.681757, T: 325047294, Avg. loss: 51515742.364948\n",
      "Total training time: 112.34 seconds.\n",
      "-- Epoch 815\n",
      "Norm: 24212.11, NNZs: 41, Bias: 2638.729850, T: 325446615, Avg. loss: 51457606.280404\n",
      "Total training time: 112.44 seconds.\n",
      "-- Epoch 816\n",
      "Norm: 24199.78, NNZs: 41, Bias: 2638.788284, T: 325845936, Avg. loss: 51399799.749482\n",
      "Total training time: 112.59 seconds.\n",
      "-- Epoch 817\n",
      "Norm: 24184.84, NNZs: 41, Bias: 2638.841271, T: 326245257, Avg. loss: 51341915.106200\n",
      "Total training time: 112.71 seconds.\n",
      "-- Epoch 818\n",
      "       119           0.7276           13.51m\n",
      "Norm: 24172.38, NNZs: 41, Bias: 2638.913445, T: 326644578, Avg. loss: 51284122.678365\n",
      "Total training time: 112.84 seconds.\n",
      "-- Epoch 819\n",
      "Norm: 24156.72, NNZs: 41, Bias: 2638.920923, T: 327043899, Avg. loss: 51226292.717133\n",
      "Total training time: 112.95 seconds.\n",
      "-- Epoch 820\n",
      "Norm: 24142.37, NNZs: 41, Bias: 2638.947703, T: 327443220, Avg. loss: 51168831.032605\n",
      "Total training time: 113.05 seconds.\n",
      "-- Epoch 821\n",
      "Norm: 24128.85, NNZs: 41, Bias: 2638.998625, T: 327842541, Avg. loss: 51111527.696543\n",
      "Total training time: 113.20 seconds.\n",
      "-- Epoch 822\n",
      "Norm: 24114.99, NNZs: 41, Bias: 2639.052661, T: 328241862, Avg. loss: 51054248.910099\n",
      "Total training time: 113.33 seconds.\n",
      "-- Epoch 823\n",
      "Norm: 24100.88, NNZs: 41, Bias: 2639.088822, T: 328641183, Avg. loss: 50997221.030083\n",
      "Total training time: 113.45 seconds.\n",
      "-- Epoch 824\n",
      "       120           0.7271           13.47m\n",
      "Norm: 24086.43, NNZs: 41, Bias: 2639.129770, T: 329040504, Avg. loss: 50940313.542011\n",
      "Total training time: 113.56 seconds.\n",
      "-- Epoch 825\n",
      "Norm: 24072.38, NNZs: 41, Bias: 2639.155038, T: 329439825, Avg. loss: 50883514.489633\n",
      "Total training time: 113.71 seconds.\n",
      "-- Epoch 826\n",
      "Norm: 24059.40, NNZs: 41, Bias: 2639.238856, T: 329839146, Avg. loss: 50826963.340224\n",
      "Total training time: 113.84 seconds.\n",
      "-- Epoch 827\n",
      "Norm: 24046.53, NNZs: 41, Bias: 2639.264691, T: 330238467, Avg. loss: 50770495.292219\n",
      "Total training time: 113.95 seconds.\n",
      "-- Epoch 828\n",
      "Norm: 24031.37, NNZs: 41, Bias: 2639.270259, T: 330637788, Avg. loss: 50713985.066445\n",
      "Total training time: 114.09 seconds.\n",
      "-- Epoch 829\n",
      "Norm: 24018.59, NNZs: 41, Bias: 2639.315950, T: 331037109, Avg. loss: 50657847.251602\n",
      "Total training time: 114.22 seconds.\n",
      "-- Epoch 830\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "Norm: 24003.57, NNZs: 41, Bias: 2639.352661, T: 331436430, Avg. loss: 50601658.075868\n",
      "Total training time: 114.34 seconds.\n",
      "-- Epoch 831\n",
      "       121           0.7268           13.44m\n",
      "Norm: 23989.00, NNZs: 41, Bias: 2639.422601, T: 331835751, Avg. loss: 50545634.488403\n",
      "Total training time: 114.45 seconds.\n",
      "-- Epoch 832\n",
      "Norm: 23974.61, NNZs: 41, Bias: 2639.432837, T: 332235072, Avg. loss: 50489730.314092\n",
      "Total training time: 114.59 seconds.\n",
      "-- Epoch 833\n",
      "Norm: 23960.63, NNZs: 41, Bias: 2639.460045, T: 332634393, Avg. loss: 50433983.080353\n",
      "Total training time: 114.69 seconds.\n",
      "-- Epoch 834\n",
      "Norm: 23951.50, NNZs: 41, Bias: 2639.485161, T: 333033714, Avg. loss: 50378283.498118\n",
      "Total training time: 114.84 seconds.\n",
      "-- Epoch 835\n",
      "Norm: 23934.92, NNZs: 41, Bias: 2639.562108, T: 333433035, Avg. loss: 50322921.666071\n",
      "Total training time: 114.99 seconds.\n",
      "-- Epoch 836\n",
      "Norm: 23921.48, NNZs: 41, Bias: 2639.599532, T: 333832356, Avg. loss: 50267532.584224\n",
      "Total training time: 115.14 seconds.\n",
      "-- Epoch 837\n",
      "       122           0.7263           13.41m\n",
      "Norm: 23908.37, NNZs: 41, Bias: 2639.644916, T: 334231677, Avg. loss: 50212255.978076\n",
      "Total training time: 115.26 seconds.\n",
      "-- Epoch 838\n",
      "Norm: 23894.87, NNZs: 41, Bias: 2639.674347, T: 334630998, Avg. loss: 50157177.867313\n",
      "Total training time: 115.39 seconds.\n",
      "-- Epoch 839\n",
      "Norm: 23881.47, NNZs: 41, Bias: 2639.698636, T: 335030319, Avg. loss: 50102286.895063\n",
      "Total training time: 115.52 seconds.\n",
      "-- Epoch 840\n",
      "Norm: 23867.81, NNZs: 41, Bias: 2639.708177, T: 335429640, Avg. loss: 50047317.082431\n",
      "Total training time: 115.63 seconds.\n",
      "-- Epoch 841\n",
      "Norm: 23854.05, NNZs: 41, Bias: 2639.766685, T: 335828961, Avg. loss: 49992615.477182\n",
      "Total training time: 115.74 seconds.\n",
      "-- Epoch 842\n",
      "Norm: 23841.84, NNZs: 41, Bias: 2639.813225, T: 336228282, Avg. loss: 49938129.472994\n",
      "Total training time: 115.88 seconds.\n",
      "-- Epoch 843\n",
      "       123           0.7241           13.38m\n",
      "Norm: 23828.19, NNZs: 41, Bias: 2639.852241, T: 336627603, Avg. loss: 49883785.768876\n",
      "Total training time: 116.00 seconds.\n",
      "-- Epoch 844\n",
      "Norm: 23814.06, NNZs: 41, Bias: 2639.898555, T: 337026924, Avg. loss: 49829469.405126\n",
      "Total training time: 116.13 seconds.\n",
      "-- Epoch 845\n",
      "Norm: 23801.76, NNZs: 41, Bias: 2639.961308, T: 337426245, Avg. loss: 49775259.046800\n",
      "Total training time: 116.24 seconds.\n",
      "-- Epoch 846\n",
      "Norm: 23786.86, NNZs: 41, Bias: 2639.974460, T: 337825566, Avg. loss: 49720976.338117\n",
      "Total training time: 116.36 seconds.\n",
      "-- Epoch 847\n",
      "Norm: 23773.79, NNZs: 41, Bias: 2640.007605, T: 338224887, Avg. loss: 49666942.212130\n",
      "Total training time: 116.47 seconds.\n",
      "-- Epoch 848\n",
      "Norm: 23758.79, NNZs: 41, Bias: 2640.047907, T: 338624208, Avg. loss: 49612925.493993\n",
      "Total training time: 116.61 seconds.\n",
      "-- Epoch 849\n",
      "Norm: 23746.91, NNZs: 41, Bias: 2640.126436, T: 339023529, Avg. loss: 49559323.504287\n",
      "Total training time: 116.74 seconds.\n",
      "-- Epoch 850\n",
      "       124           0.7219           13.36m\n",
      "Norm: 23732.89, NNZs: 41, Bias: 2640.161350, T: 339422850, Avg. loss: 49505648.174459\n",
      "Total training time: 116.87 seconds.\n",
      "-- Epoch 851\n",
      "Norm: 23719.89, NNZs: 41, Bias: 2640.190085, T: 339822171, Avg. loss: 49452078.498032\n",
      "Total training time: 116.98 seconds.\n",
      "-- Epoch 852\n",
      "Norm: 23706.42, NNZs: 41, Bias: 2640.206082, T: 340221492, Avg. loss: 49398607.748305\n",
      "Total training time: 117.12 seconds.\n",
      "-- Epoch 853\n",
      "Norm: 23693.83, NNZs: 41, Bias: 2640.258136, T: 340620813, Avg. loss: 49345389.131428\n",
      "Total training time: 117.25 seconds.\n",
      "-- Epoch 854\n",
      "Norm: 23682.04, NNZs: 41, Bias: 2640.300972, T: 341020134, Avg. loss: 49292313.861803\n",
      "Total training time: 117.38 seconds.\n",
      "-- Epoch 855\n",
      "Norm: 23669.49, NNZs: 41, Bias: 2640.353364, T: 341419455, Avg. loss: 49239381.234018\n",
      "Total training time: 117.49 seconds.\n",
      "-- Epoch 856\n",
      "       125           0.7200           13.33m\n",
      "Norm: 23655.88, NNZs: 41, Bias: 2640.380520, T: 341818776, Avg. loss: 49186396.331319\n",
      "Total training time: 117.62 seconds.\n",
      "-- Epoch 857\n",
      "Norm: 23642.57, NNZs: 41, Bias: 2640.401342, T: 342218097, Avg. loss: 49133494.724527\n",
      "Total training time: 117.75 seconds.\n",
      "-- Epoch 858\n",
      "Norm: 23629.62, NNZs: 41, Bias: 2640.426695, T: 342617418, Avg. loss: 49080820.828655\n",
      "Total training time: 117.88 seconds.\n",
      "-- Epoch 859\n",
      "Norm: 23615.83, NNZs: 41, Bias: 2640.450862, T: 343016739, Avg. loss: 49028209.890097\n",
      "Total training time: 117.99 seconds.\n",
      "-- Epoch 860\n",
      "Norm: 23602.64, NNZs: 41, Bias: 2640.506235, T: 343416060, Avg. loss: 48975813.260390\n",
      "Total training time: 118.13 seconds.\n",
      "-- Epoch 861\n",
      "Norm: 23590.63, NNZs: 41, Bias: 2640.543367, T: 343815381, Avg. loss: 48923506.489134\n",
      "Total training time: 118.25 seconds.\n",
      "-- Epoch 862\n",
      "       126           0.7180           13.30m\n",
      "Norm: 23576.45, NNZs: 41, Bias: 2640.577187, T: 344214702, Avg. loss: 48871175.718225\n",
      "Total training time: 118.38 seconds.\n",
      "-- Epoch 863\n",
      "Norm: 23566.51, NNZs: 41, Bias: 2640.648395, T: 344614023, Avg. loss: 48819278.954770\n",
      "Total training time: 118.49 seconds.\n",
      "-- Epoch 864\n",
      "Norm: 23552.45, NNZs: 41, Bias: 2640.669625, T: 345013344, Avg. loss: 48767286.150135\n",
      "Total training time: 118.64 seconds.\n",
      "-- Epoch 865\n",
      "Norm: 23539.12, NNZs: 41, Bias: 2640.720733, T: 345412665, Avg. loss: 48715491.866139\n",
      "Total training time: 118.77 seconds.\n",
      "-- Epoch 866\n",
      "Norm: 23527.27, NNZs: 41, Bias: 2640.772931, T: 345811986, Avg. loss: 48663879.102571\n",
      "Total training time: 118.88 seconds.\n",
      "-- Epoch 867\n",
      "Norm: 23514.96, NNZs: 41, Bias: 2640.830068, T: 346211307, Avg. loss: 48612420.214995\n",
      "Total training time: 119.03 seconds.\n",
      "-- Epoch 868\n",
      "Norm: 23503.72, NNZs: 41, Bias: 2640.882367, T: 346610628, Avg. loss: 48561037.312281\n",
      "Total training time: 119.16 seconds.\n",
      "-- Epoch 869\n",
      "       127           0.7172           13.28m\n",
      "Norm: 23490.78, NNZs: 41, Bias: 2640.920994, T: 347009949, Avg. loss: 48509575.502770\n",
      "Total training time: 119.28 seconds.\n",
      "-- Epoch 870\n",
      "Norm: 23477.34, NNZs: 41, Bias: 2640.954980, T: 347409270, Avg. loss: 48458208.369772\n",
      "Total training time: 119.38 seconds.\n",
      "-- Epoch 871\n",
      "Norm: 23462.94, NNZs: 41, Bias: 2640.923094, T: 347808591, Avg. loss: 48406896.050171\n",
      "Total training time: 119.52 seconds.\n",
      "-- Epoch 872\n",
      "Norm: 23448.97, NNZs: 41, Bias: 2640.942168, T: 348207912, Avg. loss: 48355711.609697\n",
      "Total training time: 119.65 seconds.\n",
      "-- Epoch 873\n",
      "Norm: 23436.76, NNZs: 41, Bias: 2640.981312, T: 348607233, Avg. loss: 48304691.703495\n",
      "Total training time: 119.78 seconds.\n",
      "-- Epoch 874\n",
      "Norm: 23424.90, NNZs: 41, Bias: 2641.020777, T: 349006554, Avg. loss: 48253873.757894\n",
      "Total training time: 119.89 seconds.\n",
      "-- Epoch 875\n",
      "       128           0.7161           13.24m\n",
      "Norm: 23412.01, NNZs: 41, Bias: 2641.070053, T: 349405875, Avg. loss: 48203175.985446\n",
      "Total training time: 120.03 seconds.\n",
      "-- Epoch 876\n",
      "Norm: 23399.45, NNZs: 41, Bias: 2641.109677, T: 349805196, Avg. loss: 48152513.893653\n",
      "Total training time: 120.16 seconds.\n",
      "-- Epoch 877\n",
      "Norm: 23387.55, NNZs: 41, Bias: 2641.134834, T: 350204517, Avg. loss: 48102018.702958\n",
      "Total training time: 120.28 seconds.\n",
      "-- Epoch 878\n",
      "Norm: 23375.32, NNZs: 41, Bias: 2641.188363, T: 350603838, Avg. loss: 48051623.199112\n",
      "Total training time: 120.41 seconds.\n",
      "-- Epoch 879\n",
      "Norm: 23362.66, NNZs: 41, Bias: 2641.222233, T: 351003159, Avg. loss: 48001354.380145\n",
      "Total training time: 120.51 seconds.\n",
      "-- Epoch 880\n",
      "Norm: 23350.49, NNZs: 41, Bias: 2641.292158, T: 351402480, Avg. loss: 47951226.758099\n",
      "Total training time: 120.61 seconds.\n",
      "-- Epoch 881\n",
      "Norm: 23338.90, NNZs: 41, Bias: 2641.352123, T: 351801801, Avg. loss: 47901282.782821\n",
      "Total training time: 120.71 seconds.\n",
      "-- Epoch 882\n",
      "Norm: 23326.87, NNZs: 41, Bias: 2641.362341, T: 352201122, Avg. loss: 47851332.420640\n",
      "Total training time: 120.82 seconds.\n",
      "-- Epoch 883\n",
      "       129           0.7155           13.23m\n",
      "Norm: 23315.22, NNZs: 41, Bias: 2641.411262, T: 352600443, Avg. loss: 47801499.960989\n",
      "Total training time: 120.92 seconds.\n",
      "-- Epoch 884\n",
      "Norm: 23302.11, NNZs: 41, Bias: 2641.430861, T: 352999764, Avg. loss: 47751730.401107\n",
      "Total training time: 121.07 seconds.\n",
      "-- Epoch 885\n",
      "Norm: 23290.02, NNZs: 41, Bias: 2641.491414, T: 353399085, Avg. loss: 47702158.217149\n",
      "Total training time: 121.18 seconds.\n",
      "-- Epoch 886\n",
      "Norm: 23277.98, NNZs: 41, Bias: 2641.512791, T: 353798406, Avg. loss: 47652593.400673\n",
      "Total training time: 121.33 seconds.\n",
      "-- Epoch 887\n",
      "Norm: 23265.88, NNZs: 41, Bias: 2641.545448, T: 354197727, Avg. loss: 47603198.901535\n",
      "Total training time: 121.48 seconds.\n",
      "-- Epoch 888\n",
      "Norm: 23254.79, NNZs: 41, Bias: 2641.610900, T: 354597048, Avg. loss: 47553977.214880\n",
      "Total training time: 121.60 seconds.\n",
      "-- Epoch 889\n",
      "Norm: 23242.22, NNZs: 41, Bias: 2641.631208, T: 354996369, Avg. loss: 47504802.898975\n",
      "Total training time: 121.72 seconds.\n",
      "-- Epoch 890\n",
      "       130           0.7136           13.21m\n",
      "Norm: 23230.91, NNZs: 41, Bias: 2641.668378, T: 355395690, Avg. loss: 47455768.004552\n",
      "Total training time: 121.85 seconds.\n",
      "-- Epoch 891\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "Norm: 23233.75, NNZs: 41, Bias: 2641.674119, T: 355795011, Avg. loss: 47406655.934829\n",
      "Total training time: 121.96 seconds.\n",
      "-- Epoch 892\n",
      "Norm: 23205.23, NNZs: 41, Bias: 2641.687835, T: 356194332, Avg. loss: 47357812.720737\n",
      "Total training time: 122.06 seconds.\n",
      "-- Epoch 893\n",
      "Norm: 23193.70, NNZs: 41, Bias: 2641.764916, T: 356593653, Avg. loss: 47309101.052057\n",
      "Total training time: 122.15 seconds.\n",
      "-- Epoch 894\n",
      "Norm: 23182.01, NNZs: 41, Bias: 2641.800027, T: 356992974, Avg. loss: 47260436.315991\n",
      "Total training time: 122.24 seconds.\n",
      "-- Epoch 895\n",
      "Norm: 23170.87, NNZs: 41, Bias: 2641.858600, T: 357392295, Avg. loss: 47211970.476446\n",
      "Total training time: 122.34 seconds.\n",
      "-- Epoch 896\n",
      "Norm: 23157.23, NNZs: 41, Bias: 2641.854231, T: 357791616, Avg. loss: 47163357.621083\n",
      "Total training time: 122.49 seconds.\n",
      "-- Epoch 897\n",
      "Norm: 23145.03, NNZs: 41, Bias: 2641.890096, T: 358190937, Avg. loss: 47115010.218291\n",
      "Total training time: 122.62 seconds.\n",
      "-- Epoch 898\n",
      "Norm: 23133.90, NNZs: 41, Bias: 2641.953383, T: 358590258, Avg. loss: 47066824.703204\n",
      "Total training time: 122.73 seconds.\n",
      "-- Epoch 899\n",
      "       131           0.7129           13.21m\n",
      "Norm: 23122.43, NNZs: 41, Bias: 2641.967548, T: 358989579, Avg. loss: 47018608.882972\n",
      "Total training time: 122.83 seconds.\n",
      "-- Epoch 900\n",
      "Norm: 23110.77, NNZs: 41, Bias: 2642.004866, T: 359388900, Avg. loss: 46970591.741293\n",
      "Total training time: 122.99 seconds.\n",
      "-- Epoch 901\n",
      "Norm: 23099.38, NNZs: 41, Bias: 2642.073727, T: 359788221, Avg. loss: 46922690.796084\n",
      "Total training time: 123.13 seconds.\n",
      "-- Epoch 902\n",
      "Norm: 23088.02, NNZs: 41, Bias: 2642.083171, T: 360187542, Avg. loss: 46874867.918876\n",
      "Total training time: 123.25 seconds.\n",
      "-- Epoch 903\n",
      "Norm: 23076.70, NNZs: 41, Bias: 2642.115044, T: 360586863, Avg. loss: 46827150.641403\n",
      "Total training time: 123.37 seconds.\n",
      "-- Epoch 904\n",
      "Norm: 23065.35, NNZs: 41, Bias: 2642.161527, T: 360986184, Avg. loss: 46779481.007548\n",
      "Total training time: 123.51 seconds.\n",
      "-- Epoch 905\n",
      "       132           0.7124           13.17m\n",
      "Norm: 23055.35, NNZs: 41, Bias: 2642.241932, T: 361385505, Avg. loss: 46732108.374949\n",
      "Total training time: 123.64 seconds.\n",
      "-- Epoch 906\n",
      "Norm: 23042.58, NNZs: 41, Bias: 2642.260790, T: 361784826, Avg. loss: 46684649.764334\n",
      "Total training time: 123.75 seconds.\n",
      "-- Epoch 907\n",
      "Norm: 23029.36, NNZs: 41, Bias: 2642.326329, T: 362184147, Avg. loss: 46637260.700891\n",
      "Total training time: 123.89 seconds.\n",
      "-- Epoch 908\n",
      "Norm: 23017.59, NNZs: 41, Bias: 2642.353721, T: 362583468, Avg. loss: 46589985.728403\n",
      "Total training time: 124.02 seconds.\n",
      "-- Epoch 909\n",
      "Norm: 23004.61, NNZs: 41, Bias: 2642.381608, T: 362982789, Avg. loss: 46542749.132850\n",
      "Total training time: 124.14 seconds.\n",
      "-- Epoch 910\n",
      "Norm: 22993.02, NNZs: 41, Bias: 2642.402086, T: 363382110, Avg. loss: 46495616.424240\n",
      "Total training time: 124.25 seconds.\n",
      "-- Epoch 911\n",
      "       133           0.7121           13.14m\n",
      "Norm: 22981.41, NNZs: 41, Bias: 2642.446300, T: 363781431, Avg. loss: 46448757.499734\n",
      "Total training time: 124.40 seconds.\n",
      "-- Epoch 912\n",
      "Norm: 22968.55, NNZs: 41, Bias: 2642.475300, T: 364180752, Avg. loss: 46401799.255770\n",
      "Total training time: 124.53 seconds.\n",
      "-- Epoch 913\n",
      "Norm: 22956.84, NNZs: 41, Bias: 2642.547201, T: 364580073, Avg. loss: 46355129.060371\n",
      "Total training time: 124.66 seconds.\n",
      "-- Epoch 914\n",
      "Norm: 22944.80, NNZs: 41, Bias: 2642.561027, T: 364979394, Avg. loss: 46308477.470312\n",
      "Total training time: 124.79 seconds.\n",
      "-- Epoch 915\n",
      "Norm: 22933.19, NNZs: 41, Bias: 2642.600579, T: 365378715, Avg. loss: 46261908.324211\n",
      "Total training time: 124.90 seconds.\n",
      "-- Epoch 916\n",
      "       134           0.7118           13.10m\n",
      "Norm: 22921.84, NNZs: 41, Bias: 2642.628581, T: 365778036, Avg. loss: 46215455.272715\n",
      "Total training time: 125.05 seconds.\n",
      "-- Epoch 917\n",
      "Norm: 22910.19, NNZs: 41, Bias: 2642.655473, T: 366177357, Avg. loss: 46169165.638846\n",
      "Total training time: 125.17 seconds.\n",
      "-- Epoch 918\n",
      "Norm: 22897.91, NNZs: 41, Bias: 2642.698918, T: 366576678, Avg. loss: 46122904.547147\n",
      "Total training time: 125.28 seconds.\n",
      "-- Epoch 919\n",
      "Norm: 22887.36, NNZs: 41, Bias: 2642.696849, T: 366975999, Avg. loss: 46076634.618649\n",
      "Total training time: 125.38 seconds.\n",
      "-- Epoch 920\n",
      "Norm: 22875.60, NNZs: 41, Bias: 2642.709492, T: 367375320, Avg. loss: 46030515.668808\n",
      "Total training time: 125.48 seconds.\n",
      "-- Epoch 921\n",
      "Norm: 22863.89, NNZs: 41, Bias: 2642.743435, T: 367774641, Avg. loss: 45984520.413910\n",
      "Total training time: 125.63 seconds.\n",
      "-- Epoch 922\n",
      "Norm: 22851.02, NNZs: 41, Bias: 2642.749083, T: 368173962, Avg. loss: 45938635.306712\n",
      "Total training time: 125.76 seconds.\n",
      "-- Epoch 923\n",
      "       135           0.7114           13.08m\n",
      "Norm: 22839.63, NNZs: 41, Bias: 2642.789040, T: 368573283, Avg. loss: 45892888.492046\n",
      "Total training time: 125.89 seconds.\n",
      "-- Epoch 924\n",
      "Norm: 22827.33, NNZs: 41, Bias: 2642.798031, T: 368972604, Avg. loss: 45847183.429314\n",
      "Total training time: 126.02 seconds.\n",
      "-- Epoch 925\n",
      "Norm: 22814.71, NNZs: 41, Bias: 2642.837155, T: 369371925, Avg. loss: 45801524.733805\n",
      "Total training time: 126.13 seconds.\n",
      "-- Epoch 926\n",
      "Norm: 22803.54, NNZs: 41, Bias: 2642.872428, T: 369771246, Avg. loss: 45756000.445433\n",
      "Total training time: 126.26 seconds.\n",
      "-- Epoch 927\n",
      "Norm: 22792.84, NNZs: 41, Bias: 2642.929188, T: 370170567, Avg. loss: 45710701.214275\n",
      "Total training time: 126.37 seconds.\n",
      "-- Epoch 928\n",
      "Norm: 22781.51, NNZs: 41, Bias: 2642.966017, T: 370569888, Avg. loss: 45665369.319956\n",
      "       136           0.7100           13.04m\n",
      "Total training time: 126.52 seconds.\n",
      "-- Epoch 929\n",
      "Norm: 22769.91, NNZs: 41, Bias: 2642.988997, T: 370969209, Avg. loss: 45620186.513861\n",
      "Total training time: 126.67 seconds.\n",
      "-- Epoch 930\n",
      "Norm: 22759.34, NNZs: 41, Bias: 2643.005810, T: 371368530, Avg. loss: 45575017.627181\n",
      "Total training time: 126.80 seconds.\n",
      "-- Epoch 931\n",
      "Norm: 22747.84, NNZs: 41, Bias: 2643.041537, T: 371767851, Avg. loss: 45529951.684212\n",
      "Total training time: 126.92 seconds.\n",
      "-- Epoch 932\n",
      "Norm: 22735.90, NNZs: 41, Bias: 2643.101218, T: 372167172, Avg. loss: 45485007.203814\n",
      "Total training time: 127.05 seconds.\n",
      "-- Epoch 933\n",
      "Norm: 22723.82, NNZs: 41, Bias: 2643.116085, T: 372566493, Avg. loss: 45440076.202237\n",
      "Total training time: 127.16 seconds.\n",
      "-- Epoch 934\n",
      "Norm: 22712.12, NNZs: 41, Bias: 2643.144946, T: 372965814, Avg. loss: 45395259.639147\n",
      "Total training time: 127.27 seconds.\n",
      "-- Epoch 935\n",
      "Norm: 22701.06, NNZs: 41, Bias: 2643.171852, T: 373365135, Avg. loss: 45350595.684066\n",
      "Total training time: 127.41 seconds.\n",
      "-- Epoch 936\n",
      "       137           0.7096           13.03m\n",
      "Norm: 22689.60, NNZs: 41, Bias: 2643.232522, T: 373764456, Avg. loss: 45306064.755518\n",
      "Total training time: 127.53 seconds.\n",
      "-- Epoch 937\n",
      "Norm: 22679.13, NNZs: 41, Bias: 2643.232840, T: 374163777, Avg. loss: 45261631.427496\n",
      "Total training time: 127.66 seconds.\n",
      "-- Epoch 938\n",
      "Norm: 22667.43, NNZs: 41, Bias: 2643.285200, T: 374563098, Avg. loss: 45217210.969294\n",
      "Total training time: 127.77 seconds.\n",
      "-- Epoch 939\n",
      "Norm: 22654.92, NNZs: 41, Bias: 2643.303551, T: 374962419, Avg. loss: 45172751.943644\n",
      "Total training time: 127.92 seconds.\n",
      "-- Epoch 940\n",
      "Norm: 22644.79, NNZs: 41, Bias: 2643.342369, T: 375361740, Avg. loss: 45128510.121151\n",
      "Total training time: 128.07 seconds.\n",
      "-- Epoch 941\n",
      "Norm: 22633.25, NNZs: 41, Bias: 2643.378795, T: 375761061, Avg. loss: 45084403.623334\n",
      "Total training time: 128.19 seconds.\n",
      "-- Epoch 942\n",
      "Norm: 22621.95, NNZs: 41, Bias: 2643.434006, T: 376160382, Avg. loss: 45040328.498107\n",
      "Total training time: 128.32 seconds.\n",
      "-- Epoch 943\n",
      "       138           0.7078           13.01m\n",
      "Norm: 22611.11, NNZs: 41, Bias: 2643.472799, T: 376559703, Avg. loss: 44996375.551882\n",
      "Total training time: 128.43 seconds.\n",
      "-- Epoch 944\n",
      "Norm: 22601.07, NNZs: 41, Bias: 2643.529284, T: 376959024, Avg. loss: 44952652.047262\n",
      "Total training time: 128.58 seconds.\n",
      "-- Epoch 945\n",
      "Norm: 22588.60, NNZs: 41, Bias: 2643.544143, T: 377358345, Avg. loss: 44908795.073078\n",
      "Total training time: 128.70 seconds.\n",
      "-- Epoch 946\n",
      "Norm: 22578.17, NNZs: 41, Bias: 2643.554941, T: 377757666, Avg. loss: 44865132.383040\n",
      "Total training time: 128.82 seconds.\n",
      "-- Epoch 947\n",
      "Norm: 22569.30, NNZs: 41, Bias: 2643.597791, T: 378156987, Avg. loss: 44821561.256678\n",
      "Total training time: 128.95 seconds.\n",
      "-- Epoch 948\n",
      "Norm: 22557.23, NNZs: 41, Bias: 2643.639478, T: 378556308, Avg. loss: 44778102.412506\n",
      "Total training time: 129.08 seconds.\n",
      "-- Epoch 949\n",
      "Norm: 22546.11, NNZs: 41, Bias: 2643.666197, T: 378955629, Avg. loss: 44734616.269941\n",
      "Total training time: 129.19 seconds.\n",
      "-- Epoch 950\n",
      "       139           0.7072           12.99m\n",
      "Norm: 22534.50, NNZs: 41, Bias: 2643.673368, T: 379354950, Avg. loss: 44691160.046866\n",
      "Total training time: 129.30 seconds.\n",
      "-- Epoch 951\n",
      "Norm: 22524.54, NNZs: 41, Bias: 2643.720272, T: 379754271, Avg. loss: 44647904.762072\n",
      "Total training time: 129.40 seconds.\n",
      "-- Epoch 952\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "Norm: 22513.52, NNZs: 41, Bias: 2643.742588, T: 380153592, Avg. loss: 44604709.772749\n",
      "Total training time: 129.51 seconds.\n",
      "-- Epoch 953\n",
      "Norm: 22502.17, NNZs: 41, Bias: 2643.740584, T: 380552913, Avg. loss: 44561525.494309\n",
      "Total training time: 129.61 seconds.\n",
      "-- Epoch 954\n",
      "Norm: 22491.44, NNZs: 41, Bias: 2643.776724, T: 380952234, Avg. loss: 44518507.260650\n",
      "Total training time: 129.76 seconds.\n",
      "-- Epoch 955\n",
      "Norm: 22480.44, NNZs: 41, Bias: 2643.810102, T: 381351555, Avg. loss: 44475506.164445\n",
      "Total training time: 129.89 seconds.\n",
      "-- Epoch 956\n",
      "       140           0.7059           12.96m\n",
      "Norm: 22468.69, NNZs: 41, Bias: 2643.826422, T: 381750876, Avg. loss: 44432646.130358\n",
      "Total training time: 130.00 seconds.\n",
      "-- Epoch 957\n",
      "Norm: 22459.15, NNZs: 41, Bias: 2643.878568, T: 382150197, Avg. loss: 44389922.583285\n",
      "Total training time: 130.15 seconds.\n",
      "-- Epoch 958\n",
      "Norm: 22446.91, NNZs: 41, Bias: 2643.871500, T: 382549518, Avg. loss: 44347242.288377\n",
      "Total training time: 130.27 seconds.\n",
      "-- Epoch 959\n",
      "Norm: 22435.88, NNZs: 41, Bias: 2643.891881, T: 382948839, Avg. loss: 44304632.199682\n",
      "Total training time: 130.40 seconds.\n",
      "-- Epoch 960\n",
      "Norm: 22424.69, NNZs: 41, Bias: 2643.924346, T: 383348160, Avg. loss: 44262186.528601\n",
      "Total training time: 130.51 seconds.\n",
      "-- Epoch 961\n",
      "Norm: 22413.79, NNZs: 41, Bias: 2643.972220, T: 383747481, Avg. loss: 44219811.603135\n",
      "Total training time: 130.66 seconds.\n",
      "-- Epoch 962\n",
      "Norm: 22403.60, NNZs: 41, Bias: 2643.997843, T: 384146802, Avg. loss: 44177412.708762\n",
      "Total training time: 130.79 seconds.\n",
      "-- Epoch 963\n",
      "       141           0.7044           12.94m\n",
      "Norm: 22392.53, NNZs: 41, Bias: 2644.055688, T: 384546123, Avg. loss: 44135268.159855\n",
      "Total training time: 130.91 seconds.\n",
      "-- Epoch 964\n",
      "Norm: 22380.95, NNZs: 41, Bias: 2644.070972, T: 384945444, Avg. loss: 44093111.499248\n",
      "Total training time: 131.04 seconds.\n",
      "-- Epoch 965\n",
      "Norm: 22370.33, NNZs: 41, Bias: 2644.129977, T: 385344765, Avg. loss: 44051140.872809\n",
      "Total training time: 131.17 seconds.\n",
      "-- Epoch 966\n",
      "Norm: 22359.27, NNZs: 41, Bias: 2644.169092, T: 385744086, Avg. loss: 44009130.554182\n",
      "Total training time: 131.30 seconds.\n",
      "-- Epoch 967\n",
      "Norm: 22349.92, NNZs: 41, Bias: 2644.220392, T: 386143407, Avg. loss: 43967277.438481\n",
      "Total training time: 131.41 seconds.\n",
      "-- Epoch 968\n",
      "Norm: 22340.15, NNZs: 41, Bias: 2644.251871, T: 386542728, Avg. loss: 43925510.582878\n",
      "Total training time: 131.54 seconds.\n",
      "-- Epoch 969\n",
      "       142           0.7029           12.92m\n",
      "Norm: 22329.11, NNZs: 41, Bias: 2644.275963, T: 386942049, Avg. loss: 43883745.638841\n",
      "Total training time: 131.69 seconds.\n",
      "-- Epoch 970\n",
      "Norm: 22317.94, NNZs: 41, Bias: 2644.292089, T: 387341370, Avg. loss: 43842081.786752\n",
      "Total training time: 131.81 seconds.\n",
      "-- Epoch 971\n",
      "Norm: 22307.09, NNZs: 41, Bias: 2644.300558, T: 387740691, Avg. loss: 43800535.192168\n",
      "Total training time: 131.93 seconds.\n",
      "-- Epoch 972\n",
      "Norm: 22296.66, NNZs: 41, Bias: 2644.335716, T: 388140012, Avg. loss: 43758976.851205\n",
      "Total training time: 132.06 seconds.\n",
      "-- Epoch 973\n",
      "Norm: 22286.63, NNZs: 41, Bias: 2644.379797, T: 388539333, Avg. loss: 43717620.905775\n",
      "Total training time: 132.19 seconds.\n",
      "-- Epoch 974\n",
      "Norm: 22276.52, NNZs: 41, Bias: 2644.434533, T: 388938654, Avg. loss: 43676327.322998\n",
      "Total training time: 132.29 seconds.\n",
      "-- Epoch 975\n",
      "       143           0.7022           12.88m\n",
      "Norm: 22264.83, NNZs: 41, Bias: 2644.457044, T: 389337975, Avg. loss: 43634997.986690\n",
      "Total training time: 132.44 seconds.\n",
      "-- Epoch 976\n",
      "Norm: 22252.92, NNZs: 41, Bias: 2644.474404, T: 389737296, Avg. loss: 43593759.196729\n",
      "Total training time: 132.56 seconds.\n",
      "-- Epoch 977\n",
      "Norm: 22242.78, NNZs: 41, Bias: 2644.519026, T: 390136617, Avg. loss: 43552742.977390\n",
      "Total training time: 132.69 seconds.\n",
      "-- Epoch 978\n",
      "Norm: 22233.19, NNZs: 41, Bias: 2644.554158, T: 390535938, Avg. loss: 43511663.603903\n",
      "Total training time: 132.84 seconds.\n",
      "-- Epoch 979\n",
      "Norm: 22221.43, NNZs: 41, Bias: 2644.566648, T: 390935259, Avg. loss: 43470677.274128\n",
      "Total training time: 132.96 seconds.\n",
      "-- Epoch 980\n",
      "Norm: 22210.66, NNZs: 41, Bias: 2644.602441, T: 391334580, Avg. loss: 43429848.709684\n",
      "Total training time: 133.09 seconds.\n",
      "-- Epoch 981\n",
      "       144           0.7005           12.86m\n",
      "Norm: 22199.65, NNZs: 41, Bias: 2644.658680, T: 391733901, Avg. loss: 43389033.241865\n",
      "Total training time: 133.20 seconds.\n",
      "-- Epoch 982\n",
      "Norm: 22189.17, NNZs: 41, Bias: 2644.706453, T: 392133222, Avg. loss: 43348335.377344\n",
      "Total training time: 133.35 seconds.\n",
      "-- Epoch 983\n",
      "Norm: 22179.26, NNZs: 41, Bias: 2644.753660, T: 392532543, Avg. loss: 43307721.226472\n",
      "Total training time: 133.48 seconds.\n",
      "-- Epoch 984\n",
      "Norm: 22169.27, NNZs: 41, Bias: 2644.804377, T: 392931864, Avg. loss: 43267255.584133\n",
      "Total training time: 133.59 seconds.\n",
      "-- Epoch 985\n",
      "Norm: 22158.94, NNZs: 41, Bias: 2644.848734, T: 393331185, Avg. loss: 43226733.733786\n",
      "Total training time: 133.72 seconds.\n",
      "-- Epoch 986\n",
      "Norm: 22148.30, NNZs: 41, Bias: 2644.882888, T: 393730506, Avg. loss: 43186382.746736\n",
      "Total training time: 133.83 seconds.\n",
      "-- Epoch 987\n",
      "       145           0.6991           12.83m\n",
      "Norm: 22137.71, NNZs: 41, Bias: 2644.894254, T: 394129827, Avg. loss: 43145995.919512\n",
      "Total training time: 133.98 seconds.\n",
      "-- Epoch 988\n",
      "Norm: 22127.59, NNZs: 41, Bias: 2644.935535, T: 394529148, Avg. loss: 43105804.395999\n",
      "Total training time: 134.10 seconds.\n",
      "-- Epoch 989\n",
      "Norm: 22117.21, NNZs: 41, Bias: 2644.978876, T: 394928469, Avg. loss: 43065704.600323\n",
      "Total training time: 134.23 seconds.\n",
      "-- Epoch 990\n",
      "Norm: 22106.62, NNZs: 41, Bias: 2645.003597, T: 395327790, Avg. loss: 43025627.760617\n",
      "Total training time: 134.34 seconds.\n",
      "-- Epoch 991\n",
      "Norm: 22096.64, NNZs: 41, Bias: 2645.049516, T: 395727111, Avg. loss: 42985686.348504\n",
      "Total training time: 134.48 seconds.\n",
      "-- Epoch 992\n",
      "Norm: 22086.69, NNZs: 41, Bias: 2645.070733, T: 396126432, Avg. loss: 42945737.553149\n",
      "Total training time: 134.62 seconds.\n",
      "-- Epoch 993\n",
      "       146           0.6979           12.81m\n",
      "Norm: 22076.17, NNZs: 41, Bias: 2645.079420, T: 396525753, Avg. loss: 42905864.220237\n",
      "Total training time: 134.75 seconds.\n",
      "-- Epoch 994\n",
      "Norm: 22065.61, NNZs: 41, Bias: 2645.126097, T: 396925074, Avg. loss: 42866154.738782\n",
      "Total training time: 134.85 seconds.\n",
      "-- Epoch 995\n",
      "Norm: 22055.53, NNZs: 41, Bias: 2645.167192, T: 397324395, Avg. loss: 42826507.423072\n",
      "Total training time: 134.94 seconds.\n",
      "-- Epoch 996\n",
      "Norm: 22045.32, NNZs: 41, Bias: 2645.201607, T: 397723716, Avg. loss: 42786861.498162\n",
      "Total training time: 135.03 seconds.\n",
      "-- Epoch 997\n",
      "Norm: 22034.21, NNZs: 41, Bias: 2645.220608, T: 398123037, Avg. loss: 42747325.960915\n",
      "Total training time: 135.11 seconds.\n",
      "-- Epoch 998\n",
      "Norm: 22022.89, NNZs: 41, Bias: 2645.221620, T: 398522358, Avg. loss: 42707791.113068\n",
      "Total training time: 135.20 seconds.\n",
      "-- Epoch 999\n",
      "Norm: 22011.62, NNZs: 41, Bias: 2645.238582, T: 398921679, Avg. loss: 42668361.295772\n",
      "Total training time: 135.29 seconds.\n",
      "-- Epoch 1000\n",
      "Norm: 22002.35, NNZs: 41, Bias: 2645.292202, T: 399321000, Avg. loss: 42629152.445381\n",
      "Total training time: 135.38 seconds.\n",
      "       147           0.6975           12.80m\n",
      "       148           0.6970           12.78m\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "       149           0.6961           12.75m\n",
      "       150           0.6953           12.71m\n",
      "       151           0.6951           12.68m\n",
      "       152           0.6948           12.65m\n",
      "       153           0.6933           12.63m\n",
      "       154           0.6921           12.61m\n",
      "       155           0.6912           12.58m\n",
      "       156           0.6902           12.55m\n",
      "       157           0.6889           12.53m\n",
      "       158           0.6884           12.51m\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "       159           0.6880           12.47m\n",
      "       160           0.6877           12.44m\n",
      "       161           0.6862           12.42m\n",
      "       162           0.6860           12.38m\n",
      "       163           0.6847           12.36m\n",
      "       164           0.6843           12.34m\n",
      "       165           0.6840           12.31m\n",
      "       166           0.6837           12.28m\n",
      "       167           0.6820           12.27m\n",
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "       168           0.6816           12.24m\n",
      "       169           0.6799           12.22m\n",
      "       170           0.6789           12.20m\n",
      "       171           0.6781           12.17m\n",
      "       172           0.6777           12.15m\n",
      "       173           0.6774           12.12m\n",
      "       174           0.6758           12.10m\n",
      "       175           0.6748           12.08m\n",
      "       176           0.6745           12.05m\n",
      "       177           0.6739           12.03m\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "       178           0.6733           12.01m\n",
      "       179           0.6727           11.97m\n",
      "       180           0.6724           11.95m\n",
      "       181           0.6722           11.93m\n",
      "       182           0.6720           11.91m\n",
      "       183           0.6704           11.89m\n",
      "       184           0.6696           11.86m\n",
      "       185           0.6692           11.85m\n",
      "       186           0.6690           11.82m\n",
      "       187           0.6687           11.79m\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "       188           0.6675           11.77m\n",
      "       189           0.6671           11.76m\n",
      "       190           0.6670           11.73m\n",
      "       191           0.6665           11.71m\n",
      "       192           0.6662           11.69m\n",
      "       193           0.6660           11.67m\n",
      "       194           0.6656           11.65m\n",
      "       195           0.6653           11.63m\n",
      "       196           0.6650           11.61m\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "       197           0.6639           11.59m\n",
      "       198           0.6634           11.56m\n",
      "       199           0.6621           11.55m\n",
      "       200           0.6617           11.53m\n",
      "       201           0.6615           11.50m\n",
      "       202           0.6610           11.49m\n",
      "       203           0.6608           11.47m\n",
      "       204           0.6605           11.44m\n",
      "       205           0.6601           11.41m\n",
      "       206           0.6596           11.38m\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "       207           0.6594           11.36m\n",
      "       208           0.6583           11.35m\n",
      "       209           0.6576           11.33m\n",
      "       210           0.6572           11.31m\n",
      "       211           0.6557           11.28m\n",
      "       212           0.6549           11.27m\n",
      "       213           0.6542           11.25m\n",
      "       214           0.6534           11.23m\n",
      "       215           0.6532           11.21m\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "       216           0.6529           11.19m\n",
      "       217           0.6527           11.17m\n",
      "       218           0.6516           11.15m\n",
      "       219           0.6513           11.13m\n",
      "       220           0.6505           11.12m\n",
      "       221           0.6501           11.10m\n",
      "       222           0.6498           11.08m\n",
      "       223           0.6487           11.06m\n",
      "       224           0.6483           11.05m\n",
      "       225           0.6477           11.03m\n",
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "       226           0.6471           11.00m\n",
      "       227           0.6469           10.99m\n",
      "       228           0.6467           10.96m\n",
      "       229           0.6464           10.95m\n",
      "       230           0.6460           10.92m\n",
      "       231           0.6457           10.90m\n",
      "       232           0.6445           10.88m\n",
      "       233           0.6440           10.86m\n",
      "       234           0.6439           10.84m\n",
      "       235           0.6437           10.82m\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "       236           0.6435           10.80m\n",
      "       237           0.6431           10.78m\n",
      "       238           0.6430           10.76m\n",
      "       239           0.6428           10.74m\n",
      "       240           0.6419           10.72m\n",
      "       241           0.6415           10.70m\n",
      "       242           0.6407           10.68m\n",
      "       243           0.6403           10.67m\n",
      "       244           0.6396           10.65m\n",
      "       245           0.6394           10.62m\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "       246           0.6391           10.61m\n",
      "       247           0.6389           10.58m\n",
      "       248           0.6386           10.56m\n",
      "       249           0.6376           10.54m\n",
      "       250           0.6368           10.53m\n",
      "       251           0.6366           10.51m\n",
      "       252           0.6365           10.49m\n",
      "       253           0.6363           10.47m\n",
      "       254           0.6362           10.45m\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "       255           0.6360           10.44m\n",
      "       256           0.6357           10.41m\n",
      "       257           0.6352           10.40m\n",
      "       258           0.6351           10.38m\n",
      "       259           0.6343           10.37m\n",
      "       260           0.6332           10.35m\n",
      "       261           0.6328           10.33m\n",
      "       262           0.6326           10.31m\n",
      "       263           0.6320           10.29m\n",
      "       264           0.6314           10.28m\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "       265           0.6309           10.26m\n",
      "       266           0.6307           10.24m\n",
      "       267           0.6298           10.22m\n",
      "       268           0.6294           10.21m\n",
      "       269           0.6288           10.19m\n",
      "       270           0.6286           10.18m\n",
      "       271           0.6282           10.16m\n",
      "       272           0.6280           10.14m\n",
      "       273           0.6275           10.12m\n",
      "       274           0.6271           10.10m\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "       275           0.6264           10.09m\n",
      "       276           0.6262           10.08m\n",
      "       277           0.6257           10.06m\n",
      "       278           0.6250           10.04m\n",
      "       279           0.6243           10.02m\n",
      "       280           0.6241           10.00m\n",
      "       281           0.6240            9.98m\n",
      "       282           0.6238            9.96m\n",
      "       283           0.6237            9.95m\n",
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "       284           0.6236            9.93m\n",
      "       285           0.6234            9.91m\n",
      "       286           0.6232            9.90m\n",
      "       287           0.6230            9.88m\n",
      "       288           0.6226            9.86m\n",
      "       289           0.6219            9.85m\n",
      "       290           0.6212            9.83m\n",
      "       291           0.6207            9.81m\n",
      "       292           0.6201            9.80m\n",
      "       293           0.6199            9.78m\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "       294           0.6198            9.76m\n",
      "       295           0.6197            9.74m\n",
      "       296           0.6196            9.72m\n",
      "       297           0.6191            9.71m\n",
      "       298           0.6190            9.69m\n",
      "       299           0.6183            9.67m\n",
      "       300           0.6180            9.66m\n",
      "       301           0.6177            9.64m\n",
      "       302           0.6174            9.62m\n",
      "       303           0.6171            9.60m\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "       304           0.6166            9.59m\n",
      "       305           0.6164            9.58m\n",
      "       306           0.6161            9.56m\n",
      "       307           0.6159            9.54m\n",
      "       308           0.6157            9.52m\n",
      "       309           0.6151            9.51m\n",
      "       310           0.6147            9.49m\n",
      "       311           0.6142            9.48m\n",
      "       312           0.6141            9.46m\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "       313           0.6139            9.45m\n",
      "       314           0.6134            9.43m\n",
      "       315           0.6130            9.42m\n",
      "       316           0.6128            9.40m\n",
      "       317           0.6122            9.38m\n",
      "       318           0.6120            9.36m\n",
      "       319           0.6117            9.34m\n",
      "       320           0.6116            9.33m\n",
      "       321           0.6115            9.31m\n",
      "       322           0.6114            9.29m\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "       323           0.6113            9.28m\n",
      "       324           0.6112            9.26m\n",
      "       325           0.6107            9.25m\n",
      "       326           0.6106            9.23m\n",
      "       327           0.6105            9.22m\n",
      "       328           0.6102            9.20m\n",
      "       329           0.6098            9.18m\n",
      "       330           0.6095            9.17m\n",
      "       331           0.6090            9.15m\n",
      "       332           0.6087            9.13m\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "       333           0.6085            9.11m\n",
      "       334           0.6080            9.10m\n",
      "       335           0.6076            9.08m\n",
      "       336           0.6074            9.07m\n",
      "       337           0.6070            9.05m\n",
      "       338           0.6066            9.04m\n",
      "       339           0.6065            9.02m\n",
      "       340           0.6057            9.00m\n",
      "       341           0.6056            8.98m\n",
      "       342           0.6055            8.97m\n",
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "       343           0.6052            8.95m\n",
      "       344           0.6050            8.94m\n",
      "       345           0.6044            8.92m\n",
      "       346           0.6038            8.91m\n",
      "       347           0.6036            8.89m\n",
      "       348           0.6034            8.88m\n",
      "       349           0.6034            8.86m\n",
      "       350           0.6033            8.84m\n",
      "       351           0.6030            8.83m\n",
      "       352           0.6026            8.81m\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "       353           0.6023            8.79m\n",
      "       354           0.6018            8.78m\n",
      "       355           0.6017            8.76m\n",
      "       356           0.6014            8.74m\n",
      "       357           0.6013            8.73m\n",
      "       358           0.6012            8.71m\n",
      "       359           0.6006            8.69m\n",
      "       360           0.6003            8.68m\n",
      "       361           0.6000            8.66m\n",
      "       362           0.5997            8.65m\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "       363           0.5994            8.63m\n",
      "       364           0.5988            8.62m\n",
      "       365           0.5983            8.60m\n",
      "       366           0.5980            8.59m\n",
      "       367           0.5976            8.57m\n",
      "       368           0.5970            8.55m\n",
      "       369           0.5967            8.54m\n",
      "       370           0.5965            8.53m\n",
      "       371           0.5963            8.51m\n",
      "       372           0.5959            8.50m\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "       373           0.5956            8.49m\n",
      "       374           0.5952            8.47m\n",
      "       375           0.5951            8.45m\n",
      "       376           0.5948            8.44m\n",
      "       377           0.5947            8.42m\n",
      "       378           0.5946            8.40m\n",
      "       379           0.5945            8.39m\n",
      "       380           0.5944            8.38m\n",
      "       381           0.5942            8.36m\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "       382           0.5938            8.35m\n",
      "       383           0.5937            8.33m\n",
      "       384           0.5935            8.32m\n",
      "       385           0.5934            8.30m\n",
      "       386           0.5932            8.28m\n",
      "       387           0.5931            8.26m\n",
      "       388           0.5926            8.25m\n",
      "       389           0.5922            8.24m\n",
      "       390           0.5919            8.22m\n",
      "       391           0.5917            8.20m\n",
      "       392           0.5916            8.19m\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "       393           0.5915            8.17m\n",
      "       394           0.5910            8.16m\n",
      "       395           0.5909            8.14m\n",
      "       396           0.5905            8.13m\n",
      "       397           0.5903            8.12m\n",
      "       398           0.5900            8.10m\n",
      "       399           0.5896            8.09m\n",
      "       400           0.5895            8.07m\n",
      "       401           0.5892            8.06m\n",
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "       402           0.5887            8.04m\n",
      "       403           0.5886            8.03m\n",
      "       404           0.5882            8.01m\n",
      "       405           0.5879            8.00m\n",
      "       406           0.5878            7.98m\n",
      "       407           0.5877            7.97m\n",
      "       408           0.5874            7.96m\n",
      "       409           0.5869            7.94m\n",
      "       410           0.5866            7.93m\n",
      "       411           0.5862            7.92m\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "       412           0.5861            7.90m\n",
      "       413           0.5857            7.89m\n",
      "       414           0.5855            7.87m\n",
      "       415           0.5854            7.86m\n",
      "       416           0.5852            7.85m\n",
      "       417           0.5848            7.83m\n",
      "       418           0.5846            7.82m\n",
      "       419           0.5845            7.81m\n",
      "       420           0.5843            7.79m\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "       421           0.5838            7.78m\n",
      "       422           0.5838            7.76m\n",
      "       423           0.5836            7.75m\n",
      "       424           0.5835            7.73m\n",
      "       425           0.5834            7.72m\n",
      "       426           0.5833            7.70m\n",
      "       427           0.5832            7.69m\n",
      "       428           0.5831            7.67m\n",
      "       429           0.5829            7.65m\n",
      "       430           0.5828            7.64m\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "       431           0.5825            7.63m\n",
      "       432           0.5825            7.61m\n",
      "       433           0.5824            7.60m\n",
      "       434           0.5822            7.58m\n",
      "       435           0.5822            7.57m\n",
      "       436           0.5821            7.55m\n",
      "       437           0.5819            7.53m\n",
      "       438           0.5816            7.52m\n",
      "       439           0.5815            7.50m\n",
      "       440           0.5812            7.49m\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "       441           0.5811            7.47m\n",
      "       442           0.5809            7.46m\n",
      "       443           0.5809            7.44m\n",
      "       444           0.5808            7.43m\n",
      "       445           0.5803            7.41m\n",
      "       446           0.5799            7.40m\n",
      "       447           0.5798            7.38m\n",
      "       448           0.5797            7.37m\n",
      "       449           0.5796            7.36m\n",
      "       450           0.5796            7.34m\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "       451           0.5795            7.33m\n",
      "       452           0.5793            7.31m\n",
      "       453           0.5790            7.30m\n",
      "       454           0.5788            7.28m\n",
      "       455           0.5788            7.26m\n",
      "       456           0.5787            7.25m\n",
      "       457           0.5787            7.23m\n",
      "       458           0.5786            7.22m\n",
      "       459           0.5784            7.20m\n",
      "       460           0.5781            7.19m\n",
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "       461           0.5781            7.17m\n",
      "       462           0.5780            7.16m\n",
      "       463           0.5779            7.14m\n",
      "       464           0.5778            7.12m\n",
      "       465           0.5777            7.11m\n",
      "       466           0.5777            7.09m\n",
      "       467           0.5772            7.08m\n",
      "       468           0.5771            7.07m\n",
      "       469           0.5769            7.05m\n",
      "       470           0.5765            7.04m\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "       471           0.5759            7.03m\n",
      "       472           0.5757            7.02m\n",
      "       473           0.5753            7.00m\n",
      "       474           0.5751            6.99m\n",
      "       475           0.5749            6.98m\n",
      "       476           0.5747            6.96m\n",
      "       477           0.5745            6.95m\n",
      "       478           0.5741            6.93m\n",
      "       479           0.5738            6.92m\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "       480           0.5736            6.91m\n",
      "       481           0.5732            6.89m\n",
      "       482           0.5730            6.88m\n",
      "       483           0.5727            6.87m\n",
      "       484           0.5724            6.85m\n",
      "       485           0.5720            6.84m\n",
      "       486           0.5719            6.83m\n",
      "       487           0.5717            6.81m\n",
      "       488           0.5714            6.80m\n",
      "       489           0.5712            6.78m\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "       490           0.5711            6.77m\n",
      "       491           0.5708            6.76m\n",
      "       492           0.5706            6.74m\n",
      "       493           0.5704            6.73m\n",
      "       494           0.5700            6.72m\n",
      "       495           0.5699            6.70m\n",
      "       496           0.5697            6.69m\n",
      "       497           0.5694            6.67m\n",
      "       498           0.5690            6.66m\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "       499           0.5686            6.65m\n",
      "       500           0.5684            6.63m\n",
      "       501           0.5680            6.62m\n",
      "       502           0.5680            6.60m\n",
      "       503           0.5679            6.59m\n",
      "       504           0.5676            6.57m\n",
      "       505           0.5675            6.56m\n",
      "       506           0.5674            6.55m\n",
      "       507           0.5673            6.53m\n",
      "       508           0.5671            6.52m\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "       509           0.5671            6.50m\n",
      "       510           0.5670            6.49m\n",
      "       511           0.5668            6.47m\n",
      "       512           0.5667            6.46m\n",
      "       513           0.5663            6.44m\n",
      "       514           0.5662            6.43m\n",
      "       515           0.5661            6.42m\n",
      "       516           0.5658            6.40m\n",
      "       517           0.5657            6.39m\n",
      "       518           0.5654            6.38m\n",
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "       519           0.5651            6.36m\n",
      "       520           0.5649            6.35m\n",
      "       521           0.5647            6.34m\n",
      "       522           0.5645            6.32m\n",
      "       523           0.5644            6.31m\n",
      "       524           0.5641            6.30m\n",
      "       525           0.5638            6.28m\n",
      "       526           0.5636            6.27m\n",
      "       527           0.5635            6.26m\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "       528           0.5632            6.24m\n",
      "       529           0.5630            6.23m\n",
      "       530           0.5627            6.22m\n",
      "       531           0.5625            6.20m\n",
      "       532           0.5623            6.19m\n",
      "       533           0.5620            6.18m\n",
      "       534           0.5619            6.16m\n",
      "       535           0.5615            6.15m\n",
      "       536           0.5612            6.14m\n",
      "       537           0.5610            6.12m\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "       538           0.5609            6.11m\n",
      "       539           0.5608            6.09m\n",
      "       540           0.5607            6.08m\n",
      "       541           0.5606            6.06m\n",
      "       542           0.5604            6.05m\n",
      "       543           0.5601            6.03m\n",
      "       544           0.5599            6.02m\n",
      "       545           0.5598            6.01m\n",
      "       546           0.5595            6.00m\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "       547           0.5591            5.98m\n",
      "       548           0.5588            5.97m\n",
      "       549           0.5585            5.96m\n",
      "       550           0.5583            5.94m\n",
      "       551           0.5582            5.93m\n",
      "       552           0.5582            5.92m\n",
      "       553           0.5581            5.90m\n",
      "       554           0.5580            5.89m\n",
      "       555           0.5578            5.87m\n",
      "       556           0.5575            5.86m\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "       557           0.5574            5.85m\n",
      "       558           0.5572            5.83m\n",
      "       559           0.5571            5.82m\n",
      "       560           0.5569            5.81m\n",
      "       561           0.5568            5.79m\n",
      "       562           0.5568            5.78m\n",
      "       563           0.5566            5.76m\n",
      "       564           0.5564            5.75m\n",
      "       565           0.5563            5.73m\n",
      "       566           0.5562            5.72m\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "       567           0.5561            5.71m\n",
      "       568           0.5559            5.69m\n",
      "       569           0.5557            5.68m\n",
      "       570           0.5555            5.67m\n",
      "       571           0.5555            5.65m\n",
      "       572           0.5552            5.64m\n",
      "       573           0.5551            5.63m\n",
      "       574           0.5550            5.61m\n",
      "       575           0.5549            5.60m\n",
      "       576           0.5549            5.58m\n",
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "       577           0.5548            5.57m\n",
      "       578           0.5547            5.55m\n",
      "       579           0.5545            5.54m\n",
      "       580           0.5545            5.53m\n",
      "       581           0.5542            5.51m\n",
      "       582           0.5541            5.50m\n",
      "       583           0.5541            5.48m\n",
      "       584           0.5538            5.47m\n",
      "       585           0.5537            5.46m\n",
      "       586           0.5534            5.44m\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "       587           0.5534            5.43m\n",
      "       588           0.5534            5.42m\n",
      "       589           0.5533            5.40m\n",
      "       590           0.5530            5.39m\n",
      "       591           0.5528            5.37m\n",
      "       592           0.5526            5.36m\n",
      "       593           0.5525            5.35m\n",
      "       594           0.5523            5.33m\n",
      "       595           0.5522            5.32m\n",
      "       596           0.5522            5.31m\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "       597           0.5520            5.29m\n",
      "       598           0.5519            5.28m\n",
      "       599           0.5518            5.27m\n",
      "       600           0.5518            5.25m\n",
      "       601           0.5516            5.24m\n",
      "       602           0.5513            5.23m\n",
      "       603           0.5512            5.21m\n",
      "       604           0.5510            5.20m\n",
      "       605           0.5509            5.19m\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "       606           0.5509            5.17m\n",
      "       607           0.5508            5.16m\n",
      "       608           0.5508            5.15m\n",
      "       609           0.5507            5.13m\n",
      "       610           0.5505            5.12m\n",
      "       611           0.5504            5.10m\n",
      "       612           0.5504            5.09m\n",
      "       613           0.5503            5.07m\n",
      "       614           0.5501            5.06m\n",
      "       615           0.5499            5.05m\n",
      "       616           0.5498            5.03m\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "       617           0.5498            5.02m\n",
      "       618           0.5495            5.00m\n",
      "       619           0.5494            4.99m\n",
      "       620           0.5493            4.98m\n",
      "       621           0.5492            4.96m\n",
      "       622           0.5491            4.95m\n",
      "       623           0.5489            4.94m\n",
      "       624           0.5487            4.92m\n",
      "       625           0.5486            4.91m\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "       626           0.5486            4.90m\n",
      "       627           0.5485            4.88m\n",
      "       628           0.5485            4.87m\n",
      "       629           0.5482            4.86m\n",
      "       630           0.5481            4.84m\n",
      "       631           0.5478            4.83m\n",
      "       632           0.5477            4.82m\n",
      "       633           0.5476            4.80m\n",
      "       634           0.5474            4.79m\n",
      "       635           0.5472            4.78m\n",
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "       636           0.5472            4.76m\n",
      "       637           0.5470            4.75m\n",
      "       638           0.5469            4.74m\n",
      "       639           0.5467            4.72m\n",
      "       640           0.5466            4.71m\n",
      "       641           0.5464            4.70m\n",
      "       642           0.5462            4.68m\n",
      "       643           0.5461            4.67m\n",
      "       644           0.5460            4.66m\n",
      "       645           0.5459            4.64m\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "       646           0.5457            4.63m\n",
      "       647           0.5456            4.62m\n",
      "       648           0.5454            4.60m\n",
      "       649           0.5452            4.59m\n",
      "       650           0.5452            4.57m\n",
      "       651           0.5452            4.56m\n",
      "       652           0.5450            4.55m\n",
      "       653           0.5448            4.53m\n",
      "       654           0.5448            4.52m\n",
      "       655           0.5446            4.51m\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "       656           0.5444            4.49m\n",
      "       657           0.5442            4.48m\n",
      "       658           0.5442            4.47m\n",
      "       659           0.5442            4.45m\n",
      "       660           0.5441            4.44m\n",
      "       661           0.5441            4.43m\n",
      "       662           0.5439            4.41m\n",
      "       663           0.5438            4.40m\n",
      "       664           0.5438            4.38m\n",
      "       665           0.5437            4.37m\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "       666           0.5437            4.36m\n",
      "       667           0.5436            4.34m\n",
      "       668           0.5435            4.33m\n",
      "       669           0.5434            4.31m\n",
      "       670           0.5431            4.30m\n",
      "       671           0.5429            4.29m\n",
      "       672           0.5427            4.28m\n",
      "       673           0.5426            4.26m\n",
      "       674           0.5424            4.25m\n",
      "       675           0.5422            4.24m\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "       676           0.5421            4.22m\n",
      "       677           0.5421            4.21m\n",
      "       678           0.5420            4.19m\n",
      "       679           0.5420            4.18m\n",
      "       680           0.5419            4.17m\n",
      "       681           0.5419            4.15m\n",
      "       682           0.5419            4.14m\n",
      "       683           0.5418            4.13m\n",
      "       684           0.5418            4.11m\n",
      "       685           0.5417            4.10m\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "       686           0.5416            4.09m\n",
      "       687           0.5415            4.07m\n",
      "       688           0.5414            4.06m\n",
      "       689           0.5414            4.04m\n",
      "       690           0.5412            4.03m\n",
      "       691           0.5411            4.02m\n",
      "       692           0.5410            4.00m\n",
      "       693           0.5410            3.99m\n",
      "       694           0.5408            3.98m\n",
      "       695           0.5407            3.97m\n",
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "       696           0.5406            3.95m\n",
      "       697           0.5404            3.94m\n",
      "       698           0.5403            3.93m\n",
      "       699           0.5403            3.91m\n",
      "       700           0.5402            3.90m\n",
      "       701           0.5402            3.89m\n",
      "       702           0.5400            3.87m\n",
      "       703           0.5400            3.86m\n",
      "       704           0.5399            3.85m\n",
      "       705           0.5399            3.83m\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "       706           0.5397            3.82m\n",
      "       707           0.5395            3.81m\n",
      "       708           0.5394            3.79m\n",
      "       709           0.5393            3.78m\n",
      "       710           0.5392            3.77m\n",
      "       711           0.5391            3.75m\n",
      "       712           0.5390            3.74m\n",
      "       713           0.5389            3.73m\n",
      "       714           0.5389            3.71m\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "       715           0.5388            3.70m\n",
      "       716           0.5387            3.69m\n",
      "       717           0.5385            3.68m\n",
      "       718           0.5384            3.66m\n",
      "       719           0.5383            3.65m\n",
      "       720           0.5382            3.64m\n",
      "       721           0.5381            3.62m\n",
      "       722           0.5380            3.61m\n",
      "       723           0.5379            3.60m\n",
      "       724           0.5378            3.58m\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "       725           0.5378            3.57m\n",
      "       726           0.5377            3.56m\n",
      "       727           0.5377            3.54m\n",
      "       728           0.5377            3.53m\n",
      "       729           0.5376            3.51m\n",
      "       730           0.5375            3.50m\n",
      "       731           0.5373            3.49m\n",
      "       732           0.5373            3.47m\n",
      "       733           0.5373            3.46m\n",
      "       734           0.5373            3.45m\n",
      "       735           0.5370            3.43m\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "       736           0.5370            3.42m\n",
      "       737           0.5369            3.41m\n",
      "       738           0.5369            3.39m\n",
      "       739           0.5367            3.38m\n",
      "       740           0.5366            3.37m\n",
      "       741           0.5364            3.36m\n",
      "       742           0.5363            3.34m\n",
      "       743           0.5362            3.33m\n",
      "       744           0.5360            3.32m\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "       745           0.5360            3.30m\n",
      "       746           0.5358            3.29m\n",
      "       747           0.5357            3.28m\n",
      "       748           0.5357            3.26m\n",
      "       749           0.5356            3.25m\n",
      "       750           0.5354            3.24m\n",
      "       751           0.5354            3.22m\n",
      "       752           0.5352            3.21m\n",
      "       753           0.5351            3.20m\n",
      "       754           0.5349            3.18m\n",
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "       755           0.5349            3.17m\n",
      "       756           0.5348            3.16m\n",
      "       757           0.5348            3.14m\n",
      "       758           0.5346            3.13m\n",
      "       759           0.5345            3.12m\n",
      "       760           0.5344            3.11m\n",
      "       761           0.5344            3.09m\n",
      "       762           0.5344            3.08m\n",
      "       763           0.5343            3.07m\n",
      "       764           0.5343            3.05m\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "       765           0.5341            3.04m\n",
      "       766           0.5340            3.03m\n",
      "       767           0.5339            3.01m\n",
      "       768           0.5336            3.00m\n",
      "       769           0.5335            2.99m\n",
      "       770           0.5335            2.98m\n",
      "       771           0.5335            2.96m\n",
      "       772           0.5334            2.95m\n",
      "       773           0.5334            2.94m\n",
      "       774           0.5334            2.92m\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "       775           0.5333            2.91m\n",
      "       776           0.5332            2.90m\n",
      "       777           0.5331            2.88m\n",
      "       778           0.5330            2.87m\n",
      "       779           0.5329            2.86m\n",
      "       780           0.5328            2.84m\n",
      "       781           0.5326            2.83m\n",
      "       782           0.5325            2.82m\n",
      "       783           0.5325            2.80m\n",
      "       784           0.5324            2.79m\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "       785           0.5323            2.78m\n",
      "       786           0.5322            2.76m\n",
      "       787           0.5322            2.75m\n",
      "       788           0.5321            2.74m\n",
      "       789           0.5321            2.72m\n",
      "       790           0.5320            2.71m\n",
      "       791           0.5319            2.70m\n",
      "       792           0.5318            2.68m\n",
      "       793           0.5317            2.67m\n",
      "       794           0.5317            2.66m\n",
      "       795           0.5316            2.64m\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "       796           0.5314            2.63m\n",
      "       797           0.5314            2.62m\n",
      "       798           0.5312            2.61m\n",
      "       799           0.5311            2.59m\n",
      "       800           0.5311            2.58m\n",
      "       801           0.5310            2.57m\n",
      "       802           0.5310            2.55m\n",
      "       803           0.5310            2.54m\n",
      "       804           0.5309            2.53m\n",
      "       805           0.5307            2.51m\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "       806           0.5306            2.50m\n",
      "       807           0.5304            2.49m\n",
      "       808           0.5301            2.47m\n",
      "       809           0.5300            2.46m\n",
      "       810           0.5300            2.45m\n",
      "       811           0.5299            2.43m\n",
      "       812           0.5298            2.42m\n",
      "       813           0.5297            2.41m\n",
      "       814           0.5297            2.39m\n",
      "       815           0.5296            2.38m\n",
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "       816           0.5295            2.37m\n",
      "       817           0.5294            2.36m\n",
      "       818           0.5293            2.34m\n",
      "       819           0.5293            2.33m\n",
      "       820           0.5292            2.32m\n",
      "       821           0.5292            2.30m\n",
      "       822           0.5291            2.29m\n",
      "       823           0.5291            2.28m\n",
      "       824           0.5291            2.26m\n",
      "       825           0.5291            2.25m\n",
      "       826           0.5289            2.24m\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "       827           0.5289            2.22m\n",
      "       828           0.5289            2.21m\n",
      "       829           0.5288            2.20m\n",
      "       830           0.5287            2.18m\n",
      "       831           0.5286            2.17m\n",
      "       832           0.5285            2.16m\n",
      "       833           0.5285            2.15m\n",
      "       834           0.5285            2.13m\n",
      "       835           0.5285            2.12m\n",
      "       836           0.5284            2.11m\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "       837           0.5282            2.09m\n",
      "       838           0.5281            2.08m\n",
      "       839           0.5279            2.07m\n",
      "       840           0.5279            2.05m\n",
      "       841           0.5279            2.04m\n",
      "       842           0.5278            2.03m\n",
      "       843           0.5278            2.02m\n",
      "       844           0.5277            2.00m\n",
      "       845           0.5276            1.99m\n",
      "       846           0.5274            1.98m\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "       847           0.5272            1.96m\n",
      "       848           0.5272            1.95m\n",
      "       849           0.5270            1.94m\n",
      "       850           0.5269            1.93m\n",
      "       851           0.5267            1.91m\n",
      "       852           0.5265            1.90m\n",
      "       853           0.5265            1.89m\n",
      "       854           0.5264            1.87m\n",
      "       855           0.5263            1.86m\n",
      "       856           0.5262            1.85m\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "       857           0.5261            1.83m\n",
      "       858           0.5260            1.82m\n",
      "       859           0.5258            1.81m\n",
      "       860           0.5257            1.80m\n",
      "       861           0.5256            1.78m\n",
      "       862           0.5255            1.77m\n",
      "       863           0.5255            1.76m\n",
      "       864           0.5255            1.75m\n",
      "       865           0.5254            1.73m\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "       866           0.5253            1.72m\n",
      "       867           0.5253            1.71m\n",
      "       868           0.5252            1.69m\n",
      "       869           0.5251            1.68m\n",
      "       870           0.5250            1.67m\n",
      "       871           0.5250            1.65m\n",
      "       872           0.5250            1.64m\n",
      "       873           0.5250            1.63m\n",
      "       874           0.5249            1.61m\n",
      "       875           0.5248            1.60m\n",
      "       876           0.5248            1.59m\n",
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "       877           0.5247            1.58m\n",
      "       878           0.5247            1.56m\n",
      "       879           0.5246            1.55m\n",
      "       880           0.5245            1.54m\n",
      "       881           0.5244            1.52m\n",
      "       882           0.5243            1.51m\n",
      "       883           0.5242            1.50m\n",
      "       884           0.5241            1.48m\n",
      "       885           0.5240            1.47m\n",
      "       886           0.5239            1.46m\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "       887           0.5239            1.45m\n",
      "       888           0.5239            1.43m\n",
      "       889           0.5238            1.42m\n",
      "       890           0.5237            1.41m\n",
      "       891           0.5237            1.39m\n",
      "       892           0.5236            1.38m\n",
      "       893           0.5236            1.37m\n",
      "       894           0.5236            1.36m\n",
      "       895           0.5236            1.34m\n",
      "       896           0.5235            1.33m\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "       897           0.5235            1.32m\n",
      "       898           0.5235            1.30m\n",
      "       899           0.5234            1.29m\n",
      "       900           0.5234            1.28m\n",
      "       901           0.5233            1.27m\n",
      "       902           0.5233            1.25m\n",
      "       903           0.5232            1.24m\n",
      "       904           0.5231            1.23m\n",
      "       905           0.5230            1.21m\n",
      "       906           0.5229            1.20m\n",
      "       907           0.5227            1.19m\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "       908           0.5226            1.18m\n",
      "       909           0.5225            1.16m\n",
      "       910           0.5223            1.15m\n",
      "       911           0.5223            1.14m\n",
      "       912           0.5221            1.12m\n",
      "       913           0.5221            1.11m\n",
      "       914           0.5220            1.10m\n",
      "       915           0.5219            1.09m\n",
      "       916           0.5218            1.07m\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "       917           0.5217            1.06m\n",
      "       918           0.5216            1.05m\n",
      "       919           0.5215            1.03m\n",
      "       920           0.5215            1.02m\n",
      "       921           0.5213            1.01m\n",
      "       922           0.5211           59.77s\n",
      "       923           0.5211           59.01s\n",
      "       924           0.5210           58.24s\n",
      "       925           0.5210           57.46s\n",
      "       926           0.5208           56.70s\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "       927           0.5207           55.93s\n",
      "       928           0.5205           55.17s\n",
      "       929           0.5205           54.40s\n",
      "       930           0.5204           53.63s\n",
      "       931           0.5203           52.86s\n",
      "       932           0.5202           52.10s\n",
      "       933           0.5201           51.34s\n",
      "       934           0.5200           50.57s\n",
      "       935           0.5199           49.81s\n",
      "       936           0.5198           49.03s\n",
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "       937           0.5198           48.26s\n",
      "       938           0.5196           47.49s\n",
      "       939           0.5195           46.73s\n",
      "       940           0.5194           45.96s\n",
      "       941           0.5194           45.20s\n",
      "       942           0.5194           44.42s\n",
      "       943           0.5193           43.65s\n",
      "       944           0.5193           42.89s\n",
      "       945           0.5192           42.12s\n",
      "       946           0.5192           41.35s\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "       947           0.5192           40.58s\n",
      "       948           0.5192           39.81s\n",
      "       949           0.5191           39.03s\n",
      "       950           0.5191           38.26s\n",
      "       951           0.5189           37.50s\n",
      "       952           0.5188           36.74s\n",
      "       953           0.5187           35.97s\n",
      "       954           0.5187           35.20s\n",
      "       955           0.5186           34.43s\n",
      "       956           0.5185           33.67s\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "       957           0.5184           32.90s\n",
      "       958           0.5183           32.13s\n",
      "       959           0.5182           31.37s\n",
      "       960           0.5182           30.60s\n",
      "       961           0.5181           29.83s\n",
      "       962           0.5181           29.06s\n",
      "       963           0.5181           28.29s\n",
      "       964           0.5180           27.52s\n",
      "       965           0.5180           26.75s\n",
      "       966           0.5179           25.98s\n",
      "       967           0.5179           25.22s\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "       968           0.5179           24.45s\n",
      "       969           0.5178           23.69s\n",
      "       970           0.5178           22.93s\n",
      "       971           0.5177           22.16s\n",
      "       972           0.5176           21.40s\n",
      "       973           0.5176           20.63s\n",
      "       974           0.5175           19.86s\n",
      "       975           0.5175           19.09s\n",
      "       976           0.5174           18.33s\n",
      "       977           0.5174           17.57s\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "       978           0.5173           16.80s\n",
      "       979           0.5173           16.04s\n",
      "       980           0.5172           15.27s\n",
      "       981           0.5172           14.50s\n",
      "       982           0.5171           13.74s\n",
      "       983           0.5170           12.97s\n",
      "       984           0.5170           12.21s\n",
      "       985           0.5170           11.45s\n",
      "       986           0.5170           10.68s\n",
      "       987           0.5170            9.92s\n",
      "       988           0.5169            9.15s\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "       989           0.5169            8.39s\n",
      "       990           0.5169            7.62s\n",
      "       991           0.5168            6.86s\n",
      "       992           0.5167            6.10s\n",
      "       993           0.5167            5.33s\n",
      "       994           0.5167            4.57s\n",
      "       995           0.5166            3.81s\n",
      "       996           0.5165            3.05s\n",
      "       997           0.5165            2.29s\n",
      "       998           0.5164            1.52s\n",
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "       999           0.5163            0.76s\n",
      "      1000           0.5163            0.00s\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "Building estimator 1 of 6 for this parallel run (total 200)...\n",
      "Building estimator 2 of 6 for this parallel run (total 200)...\n",
      "Building estimator 3 of 6 for this parallel run (total 200)...\n",
      "Building estimator 4 of 6 for this parallel run (total 200)...\n",
      "Building estimator 5 of 6 for this parallel run (total 200)...\n",
      "Building estimator 6 of 6 for this parallel run (total 200)...\n",
      "Building estimator 1 of 5 for this parallel run (total 200)...\n",
      "Building estimator 2 of 5 for this parallel run (total 200)...\n",
      "Building estimator 3 of 5 for this parallel run (total 200)...\n",
      "Building estimator 4 of 5 for this parallel run (total 200)...\n",
      "Building estimator 5 of 5 for this parallel run (total 200)...\n",
      "Building estimator 1 of 5 for this parallel run (total 200)...\n",
      "Building estimator 2 of 5 for this parallel run (total 200)...\n",
      "Building estimator 3 of 5 for this parallel run (total 200)...\n",
      "Building estimator 4 of 5 for this parallel run (total 200)...\n",
      "Building estimator 5 of 5 for this parallel run (total 200)...\n",
      "Building estimator 1 of 5 for this parallel run (total 200)...\n",
      "Building estimator 2 of 5 for this parallel run (total 200)...\n",
      "Building estimator 3 of 5 for this parallel run (total 200)...\n",
      "Building estimator 4 of 5 for this parallel run (total 200)...\n",
      "Building estimator 5 of 5 for this parallel run (total 200)...\n",
      "Building estimator 1 of 5 for this parallel run (total 200)...\n",
      "Building estimator 2 of 5 for this parallel run (total 200)...\n",
      "Building estimator 3 of 5 for this parallel run (total 200)...\n",
      "Building estimator 4 of 5 for this parallel run (total 200)...\n",
      "Building estimator 5 of 5 for this parallel run (total 200)...\n",
      "Building estimator 1 of 5 for this parallel run (total 200)...\n",
      "Building estimator 2 of 5 for this parallel run (total 200)...\n",
      "Building estimator 3 of 5 for this parallel run (total 200)...\n",
      "Building estimator 4 of 5 for this parallel run (total 200)...\n",
      "Building estimator 5 of 5 for this parallel run (total 200)...\n",
      "Building estimator 1 of 5 for this parallel run (total 200)...\n",
      "Building estimator 2 of 5 for this parallel run (total 200)...\n",
      "Building estimator 3 of 5 for this parallel run (total 200)...\n",
      "Building estimator 4 of 5 for this parallel run (total 200)...\n",
      "Building estimator 5 of 5 for this parallel run (total 200)...\n",
      "Building estimator 1 of 5 for this parallel run (total 200)...\n",
      "Building estimator 2 of 5 for this parallel run (total 200)...\n",
      "Building estimator 3 of 5 for this parallel run (total 200)...\n",
      "Building estimator 4 of 5 for this parallel run (total 200)...\n",
      "Building estimator 5 of 5 for this parallel run (total 200)...\n",
      "Building estimator 1 of 5 for this parallel run (total 200)...\n",
      "Building estimator 2 of 5 for this parallel run (total 200)...\n",
      "Building estimator 3 of 5 for this parallel run (total 200)...\n",
      "Building estimator 4 of 5 for this parallel run (total 200)...\n",
      "Building estimator 5 of 5 for this parallel run (total 200)...\n",
      "Building estimator 1 of 5 for this parallel run (total 200)...\n",
      "Building estimator 2 of 5 for this parallel run (total 200)...\n",
      "Building estimator 3 of 5 for this parallel run (total 200)...\n",
      "Building estimator 4 of 5 for this parallel run (total 200)...\n",
      "Building estimator 5 of 5 for this parallel run (total 200)...\n",
      "Building estimator 1 of 5 for this parallel run (total 200)...\n",
      "Building estimator 2 of 5 for this parallel run (total 200)...\n",
      "Building estimator 3 of 5 for this parallel run (total 200)...\n",
      "Building estimator 4 of 5 for this parallel run (total 200)...\n",
      "Building estimator 5 of 5 for this parallel run (total 200)...\n",
      "Building estimator 1 of 5 for this parallel run (total 200)...\n",
      "Building estimator 2 of 5 for this parallel run (total 200)...\n",
      "Building estimator 3 of 5 for this parallel run (total 200)...\n",
      "Building estimator 4 of 5 for this parallel run (total 200)...\n",
      "Building estimator 5 of 5 for this parallel run (total 200)...\n",
      "Building estimator 1 of 5 for this parallel run (total 200)...\n",
      "Building estimator 2 of 5 for this parallel run (total 200)...\n",
      "Building estimator 3 of 5 for this parallel run (total 200)...\n",
      "Building estimator 4 of 5 for this parallel run (total 200)...\n",
      "Building estimator 5 of 5 for this parallel run (total 200)...\n",
      "Building estimator 1 of 5 for this parallel run (total 200)...\n",
      "Building estimator 2 of 5 for this parallel run (total 200)...\n",
      "Building estimator 3 of 5 for this parallel run (total 200)...\n",
      "Building estimator 4 of 5 for this parallel run (total 200)...\n",
      "Building estimator 5 of 5 for this parallel run (total 200)...\n",
      "Building estimator 1 of 5 for this parallel run (total 200)...\n",
      "Building estimator 2 of 5 for this parallel run (total 200)...\n",
      "Building estimator 3 of 5 for this parallel run (total 200)...\n",
      "Building estimator 4 of 5 for this parallel run (total 200)...\n",
      "Building estimator 5 of 5 for this parallel run (total 200)...\n",
      "Building estimator 1 of 5 for this parallel run (total 200)...\n",
      "Building estimator 2 of 5 for this parallel run (total 200)...\n",
      "Building estimator 3 of 5 for this parallel run (total 200)...\n",
      "Building estimator 4 of 5 for this parallel run (total 200)...\n",
      "Building estimator 5 of 5 for this parallel run (total 200)...\n",
      "Building estimator 1 of 5 for this parallel run (total 200)...\n",
      "Building estimator 2 of 5 for this parallel run (total 200)...\n",
      "Building estimator 3 of 5 for this parallel run (total 200)...\n",
      "Building estimator 4 of 5 for this parallel run (total 200)...\n",
      "Building estimator 5 of 5 for this parallel run (total 200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=36)]: Done  36 out of  36 | elapsed: 24.7min finished\n"
     ]
    },
    {
     "ename": "MaybeEncodingError",
     "evalue": "Error sending result: '[ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n           max_depth=None, max_features='auto', max_leaf_nodes=None,\n           min_impurity_split=1e-07, min_samples_leaf=1,\n           min_samples_split=2, min_weight_fraction_leaf=0.0,\n           n_estimators=200, n_jobs=-1, oob_score=False, random_state=None,\n           verbose=2, warm_start=False)]'. Reason: 'error(\"'i' format requires -2147483648 <= number <= 2147483647\",)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMaybeEncodingError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-34d3e9af9dc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mensemble_vote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/ensemble/voting_classifier.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 delayed(_parallel_fit_estimator)(clone(clf), X, transformed_y,\n\u001b[1;32m    164\u001b[0m                     sample_weight)\n\u001b[0;32m--> 165\u001b[0;31m                     for _, clf in self.estimators)\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    766\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0;31m# check if timeout supported in backend future implementation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'timeout'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMaybeEncodingError\u001b[0m: Error sending result: '[ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n           max_depth=None, max_features='auto', max_leaf_nodes=None,\n           min_impurity_split=1e-07, min_samples_leaf=1,\n           min_samples_split=2, min_weight_fraction_leaf=0.0,\n           n_estimators=200, n_jobs=-1, oob_score=False, random_state=None,\n           verbose=2, warm_start=False)]'. Reason: 'error(\"'i' format requires -2147483648 <= number <= 2147483647\",)'"
     ]
    }
   ],
   "source": [
    "ensemble_vote.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ensemble_vote.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test = pd.read_csv(\"input/test_indessa.csv\")\n",
    "rows = data_test['member_id'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test = data_test.fillna(\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test['last_week_pay'] = data_test['last_week_pay'].str.extract('(\\d+)', expand=False)\n",
    "data_test = data_test.fillna(\"0\")\n",
    "data_test['last_week_pay'] = data_test['last_week_pay'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test['term'] = data_test['term'].str.extract('(\\d+)', expand=False).astype(int)\n",
    "data_test = data_test.fillna(\"0\")\n",
    "data_test['term'] = data_test['term'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract a new feature from term and last week pay\n",
    "\n",
    "data_test.insert(0, 'payment_completion', (data_test['last_week_pay']/(data_test['term']/12*52+1))*100)\n",
    "data_test['payment_completion'] = data_test['payment_completion'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract a new feature from funded_amnt_inv / loan_amnt\n",
    "\n",
    "data_test.insert(0, 'funded_ratio', (data_test['funded_amnt_inv']/data_test['loan_amnt'])*100)\n",
    "data_test['funded_ratio'] = data_test['funded_ratio'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop irrelevant features and text features\n",
    "drop_cols = ['member_id', 'batch_enrolled', 'desc', 'title', 'emp_title']\n",
    "data_test.drop(drop_cols, 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded:  grade\n",
      "Encoded:  sub_grade\n",
      "Encoded:  emp_length\n",
      "Encoded:  home_ownership\n",
      "Encoded:  verification_status\n",
      "Encoded:  pymnt_plan\n",
      "Encoded:  purpose\n",
      "Encoded:  initial_list_status\n",
      "Encoded:  application_type\n",
      "Encoded:  verification_status_joint\n",
      "Encoded:  zip_code\n",
      "Encoded:  addr_state\n"
     ]
    }
   ],
   "source": [
    "# Encode Label for Classifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cat_cols = ['grade', 'sub_grade', 'emp_length', 'home_ownership', 'verification_status', \n",
    "            'pymnt_plan', 'purpose', 'initial_list_status', 'application_type', \n",
    "            'verification_status_joint', 'zip_code', 'addr_state']\n",
    "le = {}\n",
    "\n",
    "for col in cat_cols:\n",
    "    le[col] = LabelEncoder()\n",
    "    data_test[col] = le[col].fit_transform(data_test[col])\n",
    "    le[col].classes_ = np.append(le[col].classes_, 'other')\n",
    "    \n",
    "    print('Encoded: ', col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=36)]: Done   3 out of  36 | elapsed:    2.3s remaining:   25.2s\n",
      "[Parallel(n_jobs=36)]: Done  22 out of  36 | elapsed:    6.9s remaining:    4.4s\n",
      "[Parallel(n_jobs=36)]: Done  36 out of  36 | elapsed:   10.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Bagging Classifier\n",
    "save_pred_bc = bc.predict_proba(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_pred_bc_frame = pd.DataFrame({'member_id': rows, 'loan_status': save_pred_bc[:,1]})\n",
    "save_pred_bc_frame.to_csv('submission_bagging.csv', index=False, columns=['member_id', 'loan_status'], float_format='%g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GradientBoostClassifier\n",
    "save_pred_gbc = gbc.predict_proba(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_pred_gbc_frame = pd.DataFrame({'member_id': rows, 'loan_status': save_pred_gbc[:,1]})\n",
    "save_pred_gbc_frame = save_pred_gbc_frame.round(2)\n",
    "save_pred_gbc_frame.to_csv('submission_gbc.csv', index=False, columns=['member_id', 'loan_status'], float_format='%g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=36)]: Done  50 out of 100 | elapsed:    0.6s remaining:    0.6s\n",
      "[Parallel(n_jobs=36)]: Done  71 out of 100 | elapsed:    0.8s remaining:    0.3s\n",
      "[Parallel(n_jobs=36)]: Done  92 out of 100 | elapsed:    0.9s remaining:    0.1s\n",
      "[Parallel(n_jobs=36)]: Done 100 out of 100 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=36)]: Done   3 out of  36 | elapsed:    2.2s remaining:   24.3s\n",
      "[Parallel(n_jobs=36)]: Done  22 out of  36 | elapsed:    7.0s remaining:    4.4s\n",
      "[Parallel(n_jobs=36)]: Done  36 out of  36 | elapsed:   10.5s finished\n",
      "[Parallel(n_jobs=36)]: Done  90 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=36)]: Done 200 out of 200 | elapsed:    2.4s finished\n"
     ]
    }
   ],
   "source": [
    "# Ensemble Classifier\n",
    "save_pred_test_rf = rf.predict(data_test)\n",
    "save_pred_test_knc = knc.predict(data_test)\n",
    "save_pred_test_sgd = sgd.predict(data_test)\n",
    "save_pred_test_abc = abc.predict(data_test)\n",
    "save_pred_test_bc = bc.predict(data_test)\n",
    "save_pred_test_etc = etc.predict(data_test)\n",
    "save_pred_test_gbc = gbc.predict(data_test)\n",
    "save_pred_test_pac = pac.predict(data_test)\n",
    "save_pred_test_rc = rc.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_prediction_test = pd.DataFrame({\n",
    "    'rf': save_pred_test_rf,\n",
    "    'knc': save_pred_test_knc,\n",
    "    'sgd': save_pred_test_sgd,\n",
    "    'abc': save_pred_test_abc,\n",
    "    'bc': save_pred_test_bc,\n",
    "    'etc': save_pred_test_etc,\n",
    "    'gbc': save_pred_test_gbc,\n",
    "    'pac': save_pred_test_pac,\n",
    "    'rc': save_pred_test_rc,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_pred_en = ensemble.predict_proba(save_prediction_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_pred_en_frame = pd.DataFrame({'member_id': rows, 'loan_status': save_pred_en[:,1]})\n",
    "save_pred_en_frame = save_pred_en_frame.round(2)\n",
    "save_pred_en_frame.to_csv('submission_en.csv', index=False, columns=['member_id', 'loan_status'], float_format='%g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
